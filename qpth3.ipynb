{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L=\\sum_{i=1}^n\\sum_{l=1}^L\\text{ReLU}[u_l(y_i-x_i^T\\beta) +v_l]+\\sum_{i=1}^n\\sum_{h=1}^H\\text{ReHU}_{\\tau_h}[s_h(y_i-x_i^T\\beta )+t_h]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Using device: cpu\n",
      "\n",
      "==== Generating Laplace Noise Data ====\n",
      "[*] True beta: [8.822693  9.15004   3.8286376 9.593057  3.9044821]\n",
      "[*] Splitting data into train/val sets (150/50)\n",
      "\n",
      "--- Laplace Iteration 1/2 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparam Updates:   0%|          | 0/2 [02:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 568\u001b[0m\n\u001b[1;32m    565\u001b[0m VISUALIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;66;03m# 运行实验\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m autoloss_result, data, val_losses_list \u001b[38;5;241m=\u001b[39m run_experiment(total_sample_size\u001b[38;5;241m=\u001b[39mTOTAL_SAMPLE_SIZE,   \n\u001b[1;32m    569\u001b[0m                     feature_dimension\u001b[38;5;241m=\u001b[39mFEATURE_DIMENSION,\n\u001b[1;32m    570\u001b[0m                     L\u001b[38;5;241m=\u001b[39mL, H\u001b[38;5;241m=\u001b[39mH,\n\u001b[1;32m    571\u001b[0m                     lambda_reg\u001b[38;5;241m=\u001b[39mLAMBDA_REG,\n\u001b[1;32m    572\u001b[0m                     num_hyperparam_iterations\u001b[38;5;241m=\u001b[39mNUM_HYPERPARAM_ITERATIONS,\n\u001b[1;32m    573\u001b[0m                     lr\u001b[38;5;241m=\u001b[39mLR,\n\u001b[1;32m    574\u001b[0m                     num_global_updates\u001b[38;5;241m=\u001b[39mNUM_GLOBAL_UPDATES,\n\u001b[1;32m    575\u001b[0m                     num_training_samples\u001b[38;5;241m=\u001b[39mNUM_TRAINING_SAMPLES,\n\u001b[1;32m    576\u001b[0m                     seed\u001b[38;5;241m=\u001b[39mSEED,\n\u001b[1;32m    577\u001b[0m                     distribution\u001b[38;5;241m=\u001b[39mDISTRIBUTION,\n\u001b[1;32m    578\u001b[0m                     scale\u001b[38;5;241m=\u001b[39mSCALE,\n\u001b[1;32m    579\u001b[0m                     device\u001b[38;5;241m=\u001b[39mDEVICE,\n\u001b[1;32m    580\u001b[0m                     loss_type\u001b[38;5;241m=\u001b[39mLOSS_TYPE,\n\u001b[1;32m    581\u001b[0m                     verbose\u001b[38;5;241m=\u001b[39mVERBOSE,\n\u001b[1;32m    582\u001b[0m                     visualize\u001b[38;5;241m=\u001b[39mVISUALIZE)\n\u001b[1;32m    584\u001b[0m \u001b[38;5;66;03m# 保存实验结果\u001b[39;00m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoloss_result.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[23], line 476\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(total_sample_size, feature_dimension, L, H, lambda_reg, num_hyperparam_iterations, lr, num_global_updates, num_training_samples, seed, distribution, scale, device, loss_type, verbose, visualize)\u001b[0m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_global_updates):\n\u001b[1;32m    475\u001b[0m     log_msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistribution\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_global_updates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 476\u001b[0m     U, V, S, T, val_loss_hist \u001b[38;5;241m=\u001b[39m train_hyperparams(\n\u001b[1;32m    477\u001b[0m         X_train, y_train,\n\u001b[1;32m    478\u001b[0m         X_val,   y_val,\n\u001b[1;32m    479\u001b[0m         U, V, S, T, tau,\n\u001b[1;32m    480\u001b[0m         lambda_reg \u001b[38;5;241m=\u001b[39m lambda_reg,\n\u001b[1;32m    481\u001b[0m         lr \u001b[38;5;241m=\u001b[39m lr,\n\u001b[1;32m    482\u001b[0m         num_hyperparam_iterations \u001b[38;5;241m=\u001b[39m num_hyperparam_iterations,\n\u001b[1;32m    483\u001b[0m         loss_type \u001b[38;5;241m=\u001b[39m loss_type\n\u001b[1;32m    484\u001b[0m     )\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;66;03m# Solve for final beta_opt after each round\u001b[39;00m\n\u001b[1;32m    486\u001b[0m     beta_opt \u001b[38;5;241m=\u001b[39m solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
      "Cell \u001b[0;32mIn[23], line 276\u001b[0m, in \u001b[0;36mtrain_hyperparams\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, lr, num_hyperparam_iterations, loss_type)\u001b[0m\n\u001b[1;32m    273\u001b[0m inner_range \u001b[38;5;241m=\u001b[39m trange(num_hyperparam_iterations, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperparam Updates\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m inner_range:\n\u001b[0;32m--> 276\u001b[0m     results \u001b[38;5;241m=\u001b[39m compute_outer_gradients(X_train, y_train,\n\u001b[1;32m    277\u001b[0m                                       X_val,   y_val,\n\u001b[1;32m    278\u001b[0m                                       U, V, S, T, tau,\n\u001b[1;32m    279\u001b[0m                                       lambda_reg,\n\u001b[1;32m    280\u001b[0m                                       loss_type)\n\u001b[1;32m    282\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_outer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    283\u001b[0m     U_grad, V_grad \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[23], line 206\u001b[0m, in \u001b[0;36mcompute_outer_gradients\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, loss_type)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported loss_type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;66;03m# 3) Backprop to get gradients w.r.t. U, V, S, T\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m loss_outer\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    208\u001b[0m U_grad \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m U\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(U)\n\u001b[1;32m    209\u001b[0m V_grad \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(V)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######## VERSION 1.2.1 ########\n",
    "# 1. 完整增加了 Test 部分 和 Theoretical Loss 的绘制\n",
    "# 2. 实现了保存结果到文件的功能并且优化了保存的参数\n",
    "# 3. 增加了一个新的全局参数 VISUALIZE 用于控制是否绘制图像(以便在服务器上运行)\n",
    "\n",
    "######## VERSION 1.2.0 ########\n",
    "# 1. 使用 tqdm 为外层迭代及每轮训练添加进度条。\n",
    "# 2. 记录并保存每次训练迭代的 train_loss 和 val_loss，在训练结束后进行可视化（用 Matplotlib 画图）。\n",
    "# 3. 在训练完成后，保存最终学到的超参数 (U, V, S, T) 以及由它们解出的 beta_opt 到字典中，方便后续使用或持久化。\n",
    "# 4. 可以自主选择 MAE / MSE 作为外层训练的损失函数，通过 loss_type 参数指定。\n",
    "# 5. 优化了代码结构，将核心代码封装为函数，方便调用和复用。\n",
    "# 6. 优化了部分变量名和注释，提高代码可读性。\n",
    "\n",
    "######## TODO ########\n",
    "# --- 重要 ---\n",
    "# [1] ! tau 这个参数似乎处理的有问题. 这个是给定的还是要学习的?\n",
    "# [2] ! 用大规模数据集测试 MAE+Gaussian & MSE+Laplace 的效果\n",
    "# --- 一般 ---\n",
    "# [4] 代码注释和文档整理. [!! 由于改变/增加了部分变量或名称, 需要更新docstring !!]\n",
    "# [6] 中间优化的部分能不能从手动的GD改为利用Pytorch的优化器进行优化\n",
    "# [7] 最开始的两个QP构造函数的准确性验证\n",
    "# [8]优化中有时qpth会warning非稳定解, 需要关注其稳定性和影响\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    调用 qpth.QPFunction 求解给定超参数下的内层 QP 问题, 得到最优 beta 系数\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "    \n",
    "\n",
    "def compute_outer_gradients(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg,\n",
    "    loss_type=\"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes the chosen validation loss (MSE or MAE),\n",
    "    and calculates gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "        loss_type (str): Type of loss function on validation set. Either \"mse\" or \"mae\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss_outer\": Validation loss (i.e. OUTER LOSS) (float, MSE or MAE).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients w.r.t. U, V, S, T.\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss is chosen based on 'loss_type':\n",
    "            * \"mse\": (1/n_val) * sum((y_val - X_val @ beta_opt)^2)\n",
    "            * \"mae\": (1/n_val) * sum(|y_val - X_val @ beta_opt|)\n",
    "        - If an unknown loss_type is provided, raises ValueError.\n",
    "        - Uses PyTorch autograd; any None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 1) Solve inner QP\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # 2) Compute outer loss on validation set\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        # Mean Squared Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    elif loss_type == \"mae\":\n",
    "        # Mean Absolute Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).abs().sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type '{loss_type}'. Choose 'mse' or 'mae'.\")\n",
    "\n",
    "    # 3) Backprop to get gradients w.r.t. U, V, S, T\n",
    "    loss_outer.backward()\n",
    "\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 4) Zero out grads to avoid accumulation\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss_outer\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      num_hyperparam_iterations=50,\n",
    "                      loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_hyperparam_iterations (int, optional): Number of iterations for hyperparameter by GD. Defaults to 50.\n",
    "        loss_type (str, optional): Type of loss function on validation set. Defaults to \"mse\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "            - loss_history_val (list): MSE loss on validation set per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_outer_history = []\n",
    "\n",
    "    inner_range = trange(num_hyperparam_iterations, desc='Hyperparam Updates', leave=True)\n",
    "    \n",
    "    for step in inner_range:\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg,\n",
    "                                          loss_type)\n",
    "\n",
    "        loss_val = results[\"loss_outer\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_outer_history.append(loss_val)\n",
    "        captial_loss_type = loss_type.upper()\n",
    "        inner_range.set_postfix(val_loss = f\" {loss_val:.6f} ({captial_loss_type})\")\n",
    "        # if (step+1) % 10 == 0:\n",
    "        #     print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_outer_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    4)  核心: 生成数据 & 运行实验\n",
    "################################################\n",
    "\n",
    "def generate_data(n, d, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate synthetic data (X, y, beta_true) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    true coefficients beta_true from a uniform distribution, and targets y are computed\n",
    "    as a linear combination of X and beta_true plus noise from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        d (int): Number of features.\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal', etc.). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y, beta_true)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n, d).\n",
    "            - y (torch.Tensor): Target vector, shape (n,).\n",
    "            - beta_true (torch.Tensor): True coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - beta_true is sampled from Uniform(0, 10).\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    # Generate true beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # Generate noise based on specified distribution\n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def log_msg(message, verbose = True):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def run_experiment(total_sample_size=200, feature_dimension=5,\n",
    "                   L=2, H=2,\n",
    "                   lambda_reg=0.1,\n",
    "                   num_hyperparam_iterations=10,\n",
    "                   lr=1e-2,\n",
    "                   num_global_updates=20,\n",
    "                   num_training_samples=150,\n",
    "                   seed=42,\n",
    "                   distribution='laplace',\n",
    "                   scale=1.0,\n",
    "                   device=None,\n",
    "                   loss_type=\"mse\",\n",
    "                   verbose=True,\n",
    "                   visualize=True):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a regression experiment with specified noise distribution.\n",
    "\n",
    "    Generates data, splits it into train/val sets, and runs multiple iterations of hyperparameter\n",
    "    optimization using gradient descent. Evaluates performance on both sets each iteration.\n",
    "\n",
    "    Args:\n",
    "        total_sample_size (int, optional): Total samples. Defaults to 200.\n",
    "        feature_dimension (int, optional): Features. Defaults to 5.\n",
    "        L (int, optional): Size of U, V. Defaults to 2.\n",
    "        H (int, optional): Size of S, T. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization for inner QP. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Steps per iteration. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_global_updates (int, optional): Outer iterations. Defaults to 20.\n",
    "        num_training_samples (int, optional): Training samples. Defaults to 150.\n",
    "        seed (int, optional): Random seed. Defaults to 42.\n",
    "        distribution (str, optional): Noise type ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Noise scale. Defaults to 1.0.\n",
    "        device (str or torch.device, optional): Device (auto-detects CUDA if None). Defaults to None.\n",
    "        loss_type (str, optional): Type of loss function ('mse' or 'mae'). Defaults to \"mse\".\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, beta_opt)\n",
    "            - U, V (torch.Tensor): Optimized constraints, shape (L,).\n",
    "            - S, T (torch.Tensor): Optimized constraints, shape (H,).\n",
    "            - beta_opt (torch.Tensor): Final coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - Uses generate_data for data creation.\n",
    "        - Optimizes U, V, S, T via train_hyperparams.\n",
    "        - Prints train/val performance per iteration.\n",
    "        - Assumes solve_inner_qpth and evaluate_and_print are defined.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_msg(f\"[*] Using device: {device}\", verbose)\n",
    "    \n",
    "    # 1) Generate data\n",
    "    log_msg(f\"\\n==== Generating {distribution.capitalize()} Noise Data ====\", verbose)\n",
    "    X, y, beta_true = generate_data(total_sample_size, feature_dimension, distribution=distribution, scale=scale, seed=seed, device=device)\n",
    "    log_msg(f\"[*] True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "    # 2) Train-Val Split\n",
    "    log_msg(f\"[*] Splitting data into train/val sets ({num_training_samples}/{total_sample_size-num_training_samples})\", verbose)\n",
    "    X_train, y_train = X[:num_training_samples], y[:num_training_samples]\n",
    "    X_val,   y_val   = X[num_training_samples:], y[num_training_samples:]\n",
    "    \n",
    "    # 3) Initialize hyperparameters\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    \n",
    "    # 4) Multi-round for-loop: Perform train_hyperparams in each round\n",
    "\n",
    "    all_val_losses = []\n",
    "\n",
    "    \n",
    "    # outer_range = trange(num_global_updates, desc=\"Global Iterations (qpth + GD)\") \n",
    "\n",
    "    for it in range(num_global_updates):\n",
    "        log_msg(f\"\\n--- {distribution.capitalize()} Iteration {it+1}/{num_global_updates} ---\", verbose)\n",
    "        U, V, S, T, val_loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg = lambda_reg,\n",
    "            lr = lr,\n",
    "            num_hyperparam_iterations = num_hyperparam_iterations,\n",
    "            loss_type = loss_type\n",
    "        )\n",
    "        # Solve for final beta_opt after each round\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        log_msg(f\"> Temp beta: {beta_opt.detach().cpu().numpy()}\", \n",
    "        verbose)\n",
    "        log_msg(f\"> True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "        log_msg(f\"> Beta Difference: {beta_true.detach().cpu().numpy()-beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "        all_val_losses.append(val_loss_hist)\n",
    "\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"U\": U.detach().clone(),\n",
    "        \"V\": V.detach().clone(),\n",
    "        \"S\": S.detach().clone(),\n",
    "        \"T\": T.detach().clone(),\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"num_H\":H,\n",
    "        \"num_L\":L,\n",
    "        \"lambda_reg\":lambda_reg,\n",
    "        \"tau\":tau,\n",
    "        \"data_distribution\":distribution,\n",
    "        \"scale\":scale,\n",
    "        \"metric\":loss_type\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"beta_true\": beta_true\n",
    "    }\n",
    "\n",
    "\n",
    "    log_msg(f\"[*] Final beta_opt: {beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final U: {U.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final V: {V.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final S: {S.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final T: {T.detach().cpu().numpy()}\", verbose)\n",
    "    if verbose:\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=f\"[*] Train Autoloss ({distribution})\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=f\"[*] Val Autoloss ({distribution})\")   \n",
    "\n",
    "    val_losses_flat   = [val for iteration_losses in all_val_losses   for val in iteration_losses]\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        # plt.plot(train_losses_flat, label=\"Train Loss\")\n",
    "        plt.plot(val_losses_flat, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(f\"{loss_type.upper()} Loss\")\n",
    "        plt.title(f\"Train vs Val Loss ({distribution}, {loss_type})\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return params, data, all_val_losses\n",
    "\n",
    "################################################\n",
    "#  运行实验\n",
    "################################################\n",
    "\n",
    "# 运行前请确认下列参数是否符合实验需求\n",
    "\n",
    "TOTAL_SAMPLE_SIZE = 200\n",
    "FEATURE_DIMENSION = 5\n",
    "L = 2\n",
    "H = 2\n",
    "LAMBDA_REG = 0.1\n",
    "NUM_HYPERPARAM_ITERATIONS = 2\n",
    "LR = 1e-2\n",
    "NUM_GLOBAL_UPDATES = 2\n",
    "NUM_TRAINING_SAMPLES = 150\n",
    "SEED = 42\n",
    "DISTRIBUTION = 'laplace'\n",
    "SCALE = 1.0\n",
    "DEVICE = None\n",
    "LOSS_TYPE = \"mse\"\n",
    "VERBOSE = True\n",
    "VISUALIZE = True\n",
    "\n",
    "# 运行实验\n",
    "autoloss_result, data, val_losses_list = run_experiment(total_sample_size=TOTAL_SAMPLE_SIZE,   \n",
    "                    feature_dimension=FEATURE_DIMENSION,\n",
    "                    L=L, H=H,\n",
    "                    lambda_reg=LAMBDA_REG,\n",
    "                    num_hyperparam_iterations=NUM_HYPERPARAM_ITERATIONS,\n",
    "                    lr=LR,\n",
    "                    num_global_updates=NUM_GLOBAL_UPDATES,\n",
    "                    num_training_samples=NUM_TRAINING_SAMPLES,\n",
    "                    seed=SEED,\n",
    "                    distribution=DISTRIBUTION,\n",
    "                    scale=SCALE,\n",
    "                    device=DEVICE,\n",
    "                    loss_type=LOSS_TYPE,\n",
    "                    verbose=VERBOSE,\n",
    "                    visualize=VISUALIZE)\n",
    "\n",
    "# 保存实验结果\n",
    "with open('autoloss_result.pkl', 'wb') as f:\n",
    "    pickle.dump(autoloss_result, f)\n",
    "    f.close\n",
    "with open('autoloss_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    f.close\n",
    "print(\"[*] Result and data saved to 'autoloss_result.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Test Results =====\n",
      "OLS Pred MSE: 2.859593, AutoLoss Pred MSE: 2.971714 (OLS better)\n",
      "OLS Pred MAE: 1.160136, AutoLoss Pred MAE: 1.161533 (OLS better)\n",
      "OLS Beta MSE: 0.009779, AutoLoss Beta MSE: 0.013712 (OLS better)\n",
      "OLS Beta MAE: 0.077359, AutoLoss Beta MAE: 0.095979 (OLS better)\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "#    5)  测试: 读取数据 & 测试\n",
    "################################################\n",
    "\n",
    "# OLS 训练\n",
    "def train_ols(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train OLS on the given training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Estimated beta coefficients, shape (n_features,).\n",
    "    \"\"\"\n",
    "    beta_ols = torch.linalg.lstsq(X_train, y_train).solution\n",
    "    return beta_ols\n",
    "\n",
    "#  生成测试数据\n",
    "def generate_test_data(num_test_sample, feature_dimension, beta_true, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic test data (X, y) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    and targets y are computed as a linear combination of X and beta_true plus noise\n",
    "    from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        num_test_sample (int): Number of test samples.\n",
    "        feature_dimension (int): Number of features.\n",
    "        beta_true (torch.Tensor): True coefficients, shape (n_features,).\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "            - y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "\n",
    "    Notes:\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    X = torch.randn(num_test_sample, feature_dimension, device=device)\n",
    "    \n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((num_test_sample,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y\n",
    "\n",
    "#  测试 \n",
    "\n",
    "def compute_test_Xbeta(X_test, y_test, beta_est, beta_true):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X_test, y_test) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients by Y_pred = X @ beta_est,\n",
    "    calculates MSE and MAE on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X_test (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y_test (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,).\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (y_mse, y_mae, beta_mse, beta_mae)\n",
    "            - y_mse (float): MSE for predictions on y_test.\n",
    "            - y_mae (float): MAE for predictions on y_test.\n",
    "            - beta_mse (float): MSE for beta_est compared to beta_true.\n",
    "            - beta_mae (float): MAE for beta_est compared to beta_true.\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X_test @ beta_est\n",
    "        y_mse = ((y_pred - y_test)**2).mean().item()\n",
    "        y_mae = (y_pred - y_test).abs().mean().item()\n",
    "        # print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            # print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "    return y_mse, y_mae, beta_mse, beta_mae\n",
    "\n",
    "# 读取训练/验证数据\n",
    "train_val_data = pickle.load(open('autoloss_data.pkl', 'rb'))\n",
    "X_train, y_train, X_val, y_val, beta_true = train_val_data[\"X_train\"], train_val_data[\"y_train\"], train_val_data[\"X_val\"], train_val_data[\"y_val\"], train_val_data[\"beta_true\"]\n",
    "X_full = torch.cat([X_train, X_val], dim=0)\n",
    "y_full = torch.cat([y_train, y_val], dim=0)\n",
    "\n",
    "# Train OLS\n",
    "beta_ols_train = train_ols(X_train, y_train)\n",
    "beta_ols_train_val = train_ols(X_full, y_full)\n",
    "\n",
    "# Test on OLS\n",
    "X_test, y_test = generate_test_data(50, FEATURE_DIMENSION, beta_true, distribution=DISTRIBUTION, scale=SCALE, seed=SEED, device=DEVICE)\n",
    "mse_ols_test, mae_ols_test, beta_mse_ols, beta_mae_ols = compute_test_Xbeta(X_test, y_test, beta_ols_train, beta_true)\n",
    "\n",
    "# Load AutoLoss Result\n",
    "autoloss_result = pickle.load(open('autoloss_result.pkl', 'rb'))\n",
    "U, V, S, T, beta_opt = autoloss_result[\"U\"], autoloss_result[\"V\"], autoloss_result[\"S\"], autoloss_result[\"T\"], autoloss_result[\"beta_opt\"]\n",
    "\n",
    "# Test on AutoLoss\n",
    "mse_autoloss_test, mae_autoloss_test, beta_mse_autoloss, beta_mae_autoloss = compute_test_Xbeta(X_test, y_test, beta_opt, beta_true)\n",
    "\n",
    "# Print Results\n",
    "print(\"===== Test Results =====\")\n",
    "pred_mse_better = \"OLS\" if mse_ols_test < mse_autoloss_test else \"AutoLoss\"\n",
    "pred_mae_better = \"OLS\" if mae_ols_test < mae_autoloss_test else \"AutoLoss\"\n",
    "beta_mse_better = \"OLS\" if beta_mse_ols < beta_mse_autoloss else \"AutoLoss\"\n",
    "beta_mae_better = \"OLS\" if beta_mae_ols < beta_mae_autoloss else \"AutoLoss\"\n",
    "print(f\"OLS Pred MSE: {mse_ols_test:.6f}, AutoLoss Pred MSE: {mse_autoloss_test:.6f}\", f\"({pred_mse_better} better)\")\n",
    "print(f\"OLS Pred MAE: {mae_ols_test:.6f}, AutoLoss Pred MAE: {mae_autoloss_test:.6f}\", f\"({pred_mae_better} better)\")\n",
    "print(f\"OLS Beta MSE: {beta_mse_ols:.6f}, AutoLoss Beta MSE: {beta_mse_autoloss:.6f}\", f\"({beta_mse_better} better)\")\n",
    "print(f\"OLS Beta MAE: {beta_mae_ols:.6f}, AutoLoss Beta MAE: {beta_mae_autoloss:.6f}\", f\"({beta_mae_better} better)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################################\n",
    "#    6)  生成理论AutoLoss 并绘制\n",
    "################################################\n",
    "\n",
    "# 1) 计算理论AutoLoss (L=\\sum_{i=1}^n\\sum_{l=1}^L\\text{ReLU}[u_l(y_i-x_i^T\\beta) +v_l]+\\sum_{i=1}^n\\sum_{h=1}^H\\text{ReHU}_{\\gamma_h}[s_h(y_i-x_i^T\\beta )+t_h]) )\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reHU_piecewise(x, gamma):\n",
    "    r\"\"\"\n",
    "    实现分段函数 ReHU_gamma(x):\n",
    "       = 0,                                       if x <= 0\n",
    "       = 0.5 * x^2,                               if 0 < x < gamma\n",
    "       = (x^2)/4 + (gamma*x)/2 - (gamma^2)/4,      if x >= gamma\n",
    "\n",
    "    x, gamma 均可为张量，通过 torch.where 分段拼接。\n",
    "    \"\"\"\n",
    "    cond0 = (x <= 0)\n",
    "    cond2 = (x >= gamma)\n",
    "    \n",
    "    val0 = torch.zeros_like(x)                          # x <= 0\n",
    "    val1 = 0.5 * x**2                                   # 0 < x < gamma\n",
    "    val2 = (x**2)/4 + 0.5*gamma*x - (gamma**2)/4        # x >= gamma\n",
    "    \n",
    "    out = torch.where(cond0, val0, val1)\n",
    "    out = torch.where(cond2, val2, out)\n",
    "    return out\n",
    "\n",
    "def single_autoloss(r, U, V, S, T, tau):\n",
    "    r\"\"\"\n",
    "    对单个标量残差 r（或一批 r 向量）计算:\n",
    "      L_single(r) = sum_{l=1}^L ReLU(u_l*r + v_l)  \n",
    "                  + sum_{h=1}^H ReHU_tau_h(s_h*r + t_h).\n",
    "\n",
    "    Args:\n",
    "        r (torch.Tensor): shape [N], 表示多个残差点\n",
    "        U, V: shape [L]\n",
    "        S, T, tau: shape [H]\n",
    "    Returns:\n",
    "        torch.Tensor: shape [N], 每个 r 下对应的损失值\n",
    "    \"\"\"\n",
    "    if not isinstance(r, torch.Tensor):\n",
    "        r = torch.tensor(r, dtype=U.dtype, device=U.device)\n",
    "    if r.dim() == 0:\n",
    "        r = r.view(1)  # 保证至少1D\n",
    "    \n",
    "    # 部分1: sum_l ReLU(u_l * r + v_l)\n",
    "    r_vals = r.unsqueeze(0)                 # => shape [1,N]\n",
    "    uv = U.unsqueeze(1)*r_vals + V.unsqueeze(1)  # => [L,N]\n",
    "    partL = torch.relu(uv).sum(dim=0)            # => [N]\n",
    "\n",
    "    # 部分2: sum_h ReHU_{tau_h}(s_h*r + t_h)\n",
    "    st = S.unsqueeze(1)*r_vals + T.unsqueeze(1)     # [H,N]\n",
    "    tau_b = tau.unsqueeze(1).expand_as(st)          # [H,N]\n",
    "    partH = reHU_piecewise(st, tau_b).sum(dim=0)    # [N]\n",
    "    \n",
    "    return partL + partH\n",
    "\n",
    "def plot_theoretical_autoloss(params, r_min=-5, r_max=5, num_points=200):\n",
    "    r\"\"\"\n",
    "    绘制“单点”理论 Autoloss 随残差 r 的分段曲线.\n",
    "\n",
    "    Args:\n",
    "        params (dict): 包含 U, V, S, T, tau 等键的字典\n",
    "        r_min, r_max (float): 绘制区间\n",
    "        num_points (int): 采样点数\n",
    "    \"\"\"\n",
    "    U = params[\"U\"]\n",
    "    V = params[\"V\"]\n",
    "    S = params[\"S\"]\n",
    "    T = params[\"T\"]\n",
    "    # 需要在 params 里也存一下 tau\n",
    "    # 假设 params[\"tau\"] = tau\n",
    "    if \"tau\" not in params:\n",
    "        raise KeyError(\"params中必须包含 'tau' 用来表示 ReHU 的门限.\")\n",
    "    tau = params[\"tau\"]\n",
    "\n",
    "    device = U.device\n",
    "    r_vals = torch.linspace(r_min, r_max, steps=num_points, device=device)\n",
    "    \n",
    "    L_vals = single_autoloss(r_vals, U, V, S, T, tau)\n",
    "    \n",
    "    # 转回 numpy 画图\n",
    "    r_np = r_vals.cpu().numpy()\n",
    "    L_np = L_vals.cpu().numpy()\n",
    "\n",
    "    data_distribution = params[\"data_distribution\"]\n",
    "    scale = params[\"scale\"]\n",
    "    metric = params[\"metric\"]\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(r_np, L_np, label=\"Single-point Autoloss\")\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel(\"Residual r\")\n",
    "    plt.ylabel(\"Autoloss(r)\")\n",
    "    plt.title(f\"Theoretical Autoloss ({data_distribution}, {metric})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 2) 绘制理论AutoLoss\n",
    "\n",
    "plot_theoretical_autoloss(autoloss_result, r_min=-5, r_max=5, num_points=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

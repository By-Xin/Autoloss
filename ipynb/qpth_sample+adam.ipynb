{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Using device: cpu\n",
      "\n",
      "==== Generating Normal Noise Data ====\n",
      "[*] True beta: [8.822693  9.15004   3.8286376 9.593057  3.9044821]\n",
      "\n",
      "--- Normal Iteration 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparam Updates: 100%|██████████| 4/4 [01:09<00:00, 17.39s/it, val_loss=0.799534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Temp beta: [8.7856    9.132548  3.8739126 9.624201  3.8256476]\n",
      "> True beta: [8.822693  9.15004   3.8286376 9.593057  3.9044821]\n",
      "> Beta Difference: [ 0.03709316  0.01749134 -0.04527497 -0.03114414  0.07883453]\n",
      "\n",
      "--- Normal Iteration 2/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparam Updates:  25%|██▌       | 1/4 [00:21<01:05, 21.70s/it, val_loss=0.706258]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/qpth/solvers/pdipm/batch.py:111\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(Q, p, G, h, A, b, Q_LU, S_LU, R, eps, verbose, notImprovedLim, maxIter, solver)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mfactor_kkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_LU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/qpth/solvers/pdipm/batch.py:448\u001b[0m, in \u001b[0;36mfactor_kkt\u001b[0;34m(S_LU, R, d)\u001b[0m\n\u001b[1;32m    446\u001b[0m T[factor_kkt_eye] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m d)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 448\u001b[0m T_LU \u001b[38;5;241m=\u001b[39m \u001b[43mlu_hack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m T\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;66;03m# TODO: Don't use pivoting in most cases because\u001b[39;00m\n\u001b[1;32m    452\u001b[0m     \u001b[38;5;66;03m# torch.lu_unpack is inefficient here:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/qpth/solvers/pdipm/batch.py:9\u001b[0m, in \u001b[0;36mlu_hack\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlu_hack\u001b[39m(x):\n\u001b[0;32m----> 9\u001b[0m     data, pivots \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlu_factor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpivot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mis_cuda:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: torch.linalg.lu_factor: (Batch element 0): U[5,5] is zero and using it on lu_solve would result in a division by zero. If you still want to perform the factorization, consider calling linalg.lu(A, pivot) or linalg.lu_factor_ex(A, pivot)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 579\u001b[0m\n\u001b[1;32m    576\u001b[0m OPTIMIZER_CHOICE \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# \"adam\", \"sgd\", \"adamw\"\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;66;03m# 运行实验\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m autoloss_result, data, val_losses_list \u001b[38;5;241m=\u001b[39m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_sample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOTAL_SAMPLE_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfeature_dimension\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFEATURE_DIMENSION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mL\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLAMBDA_REG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_hyperparam_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_HYPERPARAM_ITERATIONS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    585\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_global_updates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_GLOBAL_UPDATES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    586\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mnum_training_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TRAINING_SAMPLES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    587\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDISTRIBUTION\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    589\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSCALE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    591\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLOSS_TYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    592\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVISUALIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                    \u001b[49m\u001b[43moptimizer_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mOPTIMIZER_CHOICE\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;66;03m# 保存实验结果\u001b[39;00m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mautoloss_result.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[4], line 483\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(total_sample_size, feature_dimension, L, H, lambda_reg, num_hyperparam_iterations, lr, num_global_updates, num_training_samples, num_val_samples, seed, distribution, scale, device, loss_type, verbose, visualize, optimizer_choice)\u001b[0m\n\u001b[1;32m    481\u001b[0m X_train, y_train, X_remaining, y_remaining \u001b[38;5;241m=\u001b[39m sample_data(num_training_samples, X_full, y_full, seed\u001b[38;5;241m=\u001b[39mseed, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    482\u001b[0m X_val, y_val, _, _ \u001b[38;5;241m=\u001b[39m sample_data(num_val_samples, X_remaining, y_remaining, seed\u001b[38;5;241m=\u001b[39mseed, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m--> 483\u001b[0m U, V, S, T, val_loss_hist \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_hyperparams\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_hyperparam_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_hyperparam_iterations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_choice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_choice\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Solve for final beta_opt after each round\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;66;03m#这个beta不需要计算，直接在前一个函数return一个beta出来就可以\u001b[39;00m\n\u001b[1;32m    495\u001b[0m beta_opt \u001b[38;5;241m=\u001b[39m solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
      "Cell \u001b[0;32mIn[4], line 254\u001b[0m, in \u001b[0;36mtrain_hyperparams\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, lr, num_hyperparam_iterations, loss_type, optimizer_choice)\u001b[0m\n\u001b[1;32m    251\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# 2) 前向图: 解内层 QP 并计算最外层loss\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m loss_outer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_outer_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mloss_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# 3) 反向传播\u001b[39;00m\n\u001b[1;32m    261\u001b[0m loss_outer\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[4], line 178\u001b[0m, in \u001b[0;36mcompute_outer_loss\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, loss_type)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m只计算最外层的验证集 Loss (MSE 或 MAE)，不在这里做 backward。\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m返回: (loss_outer, beta_opt)\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# 1) 先解内层 QP，得到 beta_opt\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m beta_opt \u001b[38;5;241m=\u001b[39m \u001b[43msolve_inner_qpth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mV\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# 2) 计算 outer loss\u001b[39;00m\n\u001b[1;32m    181\u001b[0m n_val \u001b[38;5;241m=\u001b[39m X_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 155\u001b[0m, in \u001b[0;36msolve_inner_qpth\u001b[0;34m(U, V, S, T, tau, X_train, y_train, lambda_reg)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;03mSolve inner QP problem to obtain optimal beta coefficients.\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;03m    - Assumes all tensors are on the same device.\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    154\u001b[0m Q, p, G, h \u001b[38;5;241m=\u001b[39m build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n\u001b[0;32m--> 155\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mQPFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m d \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    157\u001b[0m beta_opt \u001b[38;5;241m=\u001b[39m z[:, :d]\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/torch/autograd/function.py:598\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/qpth/qp.py:94\u001b[0m, in \u001b[0;36mQPFunction.<locals>.QPFunctionFn.forward\u001b[0;34m(ctx, Q_, p_, G_, h_, A_, b_)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m QPSolvers\u001b[38;5;241m.\u001b[39mPDIPM_BATCHED:\n\u001b[1;32m     93\u001b[0m     ctx\u001b[38;5;241m.\u001b[39mQ_LU, ctx\u001b[38;5;241m.\u001b[39mS_LU, ctx\u001b[38;5;241m.\u001b[39mR \u001b[38;5;241m=\u001b[39m pdipm_b\u001b[38;5;241m.\u001b[39mpre_factor_kkt(Q, G, A)\n\u001b[0;32m---> 94\u001b[0m     zhats, ctx\u001b[38;5;241m.\u001b[39mnus, ctx\u001b[38;5;241m.\u001b[39mlams, ctx\u001b[38;5;241m.\u001b[39mslacks \u001b[38;5;241m=\u001b[39m \u001b[43mpdipm_b\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQ_LU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS_LU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnotImprovedLim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m QPSolvers\u001b[38;5;241m.\u001b[39mCVXPY:\n\u001b[1;32m     98\u001b[0m     vals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(nBatch)\u001b[38;5;241m.\u001b[39mtype_as(Q)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/optnet/lib/python3.9/site-packages/qpth/solvers/pdipm/batch.py:111\u001b[0m, in \u001b[0;36mforward\u001b[0;34m(Q, p, G, h, A, b, Q_LU, S_LU, R, eps, verbose, notImprovedLim, maxIter, solver)\u001b[0m\n\u001b[1;32m    109\u001b[0m d \u001b[38;5;241m=\u001b[39m z \u001b[38;5;241m/\u001b[39m s\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     \u001b[43mfactor_kkt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS_LU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m], best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m], best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ####### VERSION 2.1.0 ########\n",
    "# 1. 改进了优化器为 pytorch 的 Adam\n",
    "\n",
    "# ####### VERSION 2.0.0 ########\n",
    "# 1. check了qp构造函数\n",
    "\n",
    "# ####### VERSION 1.2.1 ########\n",
    "# 1. 完整增加了 Test 部分 和 Theoretical Loss 的绘制\n",
    "# 2. 实现了保存结果到文件的功能并且优化了保存的参数\n",
    "# 3. 增加了一个新的全局参数 VISUALIZE 用于控制是否绘制图像(以便在服务器上运行)\n",
    "\n",
    "######## VERSION 1.2.0 ########\n",
    "# 1. 使用 tqdm 为外层迭代及每轮训练添加进度条。\n",
    "# 2. 记录并保存每次训练迭代的 train_loss 和 val_loss，在训练结束后进行可视化（用 Matplotlib 画图）。\n",
    "# 3. 在训练完成后，保存最终学到的超参数 (U, V, S, T) 以及由它们解出的 beta_opt 到字典中，方便后续使用或持久化。\n",
    "# 4. 可以自主选择 MAE / MSE 作为外层训练的损失函数，通过 loss_type 参数指定。\n",
    "# 5. 优化了代码结构，将核心代码封装为函数，方便调用和复用。\n",
    "# 6. 优化了部分变量名和注释，提高代码可读性。\n",
    "\n",
    "######## TODO ########\n",
    "# --- 重要 ---\n",
    "# [2] ! 用大规模数据集测试 MAE+Gaussian & MSE+Laplace 的效果\n",
    "# [3] ! 加一个最小一乘求beta的对照\n",
    "# [4] ! Tau 的处理\n",
    "# [5] ! 加入梯度下降训练mae代替线性回归做对照\n",
    "\n",
    "# --- 一般 ---\n",
    "# [4] 代码注释和文档整理. [!! 由于改变/增加了部分变量或名称, 需要更新docstring !!]\n",
    "# [6] 中间优化的部分能不能从手动的GD改为利用Pytorch的优化器进行优化\n",
    "# x[7] 最开始的两个QP构造函数的准确性验证 \n",
    "# [8]优化中有时qpth会warning非稳定解, 需要关注其稳定性和影响\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    #Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d : d + L*n] = 1.0\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l 转为 -pi_li - u_li x_i^T beta <= -u_li y_i - v_li\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = -U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = -U[l]*y_train[i] - V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h 转为 -theta_hi - sigma_hi - s_hi x_i^T beta <= -s_hi y_i - t_hi\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = -S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = -S[h_]*y_train[i] - T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    调用 qpth.QPFunction 求解给定超参数下的内层 QP 问题, 得到最优 beta 系数，调用的是抽样的xtrain和ytrain\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "    \n",
    "\n",
    "def compute_outer_loss(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg,\n",
    "    loss_type=\"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    只计算最外层的验证集 Loss (MSE 或 MAE)，不在这里做 backward。\n",
    "\n",
    "    返回: (loss_outer, beta_opt)\n",
    "    \"\"\"\n",
    "    # 1) 先解内层 QP，得到 beta_opt\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # 2) 计算 outer loss\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    elif loss_type == \"mae\":\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).abs().sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type '{loss_type}'. Choose 'mse' or 'mae'.\")\n",
    "\n",
    "    return loss_outer, beta_opt\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      num_hyperparam_iterations=50,\n",
    "                      loss_type=\"mse\",\n",
    "                      optimizer_choice = \"adam\"\n",
    "                      ):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_hyperparam_iterations (int, optional): Number of iterations for hyperparameter by GD. Defaults to 50.\n",
    "        loss_type (str, optional): Type of loss function on validation set. Defaults to \"mse\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "            - loss_history_val (list): MSE loss on validation set per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    if optimizer_choice == \"adam\":\n",
    "        # Adam optimizer\n",
    "        optimizer = torch.optim.Adam([U, V, S, T], lr=lr)\n",
    "    elif optimizer_choice == \"sgd\":\n",
    "        # SGD optimizer\n",
    "        optimizer = torch.optim.SGD([U, V, S, T], lr=lr)\n",
    "    elif optimizer_choice == \"adamw\":\n",
    "        # AdamW optimizer\n",
    "        optimizer = torch.optim.AdamW([U, V, S, T], lr=lr)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer choice '{optimizer_choice}'. Choose 'adam', 'sgd', or 'adamw'.\")\n",
    "\n",
    "    loss_outer_history = []\n",
    "    inner_range = trange(num_hyperparam_iterations, desc='Hyperparam Updates', leave=True)\n",
    "    \n",
    "    for step in inner_range:\n",
    "        # 1) 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 2) 前向图: 解内层 QP 并计算最外层loss\n",
    "        loss_outer, _ = compute_outer_loss(X_train, y_train,\n",
    "                                           X_val,   y_val,\n",
    "                                           U, V, S, T, tau,\n",
    "                                           lambda_reg,\n",
    "                                           loss_type=loss_type)\n",
    "\n",
    "        # 3) 反向传播\n",
    "        loss_outer.backward()\n",
    "\n",
    "        # 4) Adam 更新超参数\n",
    "        optimizer.step()\n",
    "\n",
    "        # 记录损失\n",
    "        loss_val = loss_outer.item()\n",
    "        loss_outer_history.append(loss_val)\n",
    "        inner_range.set_postfix(val_loss=f\"{loss_val:.6f}\")\n",
    "\n",
    "    # 最后返回超参数和历史损失\n",
    "    return U, V, S, T, loss_outer_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    4)  核心: 生成数据 & 运行实验\n",
    "################################################\n",
    "\n",
    "def generate_full_data(n, d, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate synthetic data (X, y, beta_true) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    true coefficients beta_true from a uniform distribution, and targets y are computed\n",
    "    as a linear combination of X and beta_true plus noise from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        d (int): Number of features.\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal', etc.). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y, beta_true)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n, d).\n",
    "            - y (torch.Tensor): Target vector, shape (n,).\n",
    "            - beta_true (torch.Tensor): True coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - beta_true is sampled from Uniform(0, 10).\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "                                                  \n",
    "    # Generate true beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # Generate noise based on specified distribution\n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)   \n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "#def sample_data(n_train, X_full,y_full,seed=42, device=\"cpu\"):\n",
    "#sample train and val都用这个来做\n",
    "def sample_data(n_sample, X_full,y_full,seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Sample data from the full dataset.\n",
    "\n",
    "    Args:\n",
    "        n_train (int): Number of training samples.\n",
    "        X_full (torch.Tensor): Full feature matrix, shape (n_full_samples, n_features).\n",
    "        y_full (torch.Tensor): Full target vector, shape (n_full_samples,).\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train, y_train)\n",
    "            - X_train (torch.Tensor): Sampled training feature matrix, shape (n_train_samples, n_features).\n",
    "            - y_train (torch.Tensor): Sampled training target vector, shape (n_train_samples,).\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    indices = torch.randperm(X_full.size(0))\n",
    "    selected_indices = indices[:n_sample]\n",
    "    remaining_indices = indices[n_sample:]\n",
    "    \n",
    "    X_sample = X_full[selected_indices].to(device)\n",
    "    y_sample = y_full[selected_indices].to(device)\n",
    "    X_remaining = X_full[remaining_indices].to(device)\n",
    "    y_remaining = y_full[remaining_indices].to(device)\n",
    "    \n",
    "    return X_sample, y_sample, X_remaining, y_remaining\n",
    "\n",
    "\n",
    "def log_msg(message, verbose = True):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def run_experiment(total_sample_size=200, feature_dimension=5,\n",
    "                   L=2, H=2,\n",
    "                   lambda_reg=0.1,\n",
    "                   num_hyperparam_iterations=10,\n",
    "                   lr=1e-2,\n",
    "                   num_global_updates=20,\n",
    "                   num_training_samples=150,\n",
    "                   num_val_samples=50,\n",
    "                   seed=42,\n",
    "                   distribution='laplace',\n",
    "                   scale=1.0,\n",
    "                   device=None,\n",
    "                   loss_type=\"mse\",\n",
    "                   verbose=True,\n",
    "                   visualize=True,\n",
    "                   optimizer_choice = \"adam\"\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a regression experiment with specified noise distribution.\n",
    "\n",
    "    Generates data, splits it into train/val sets, and runs multiple iterations of hyperparameter\n",
    "    optimization using gradient descent. Evaluates performance on both sets each iteration.\n",
    "\n",
    "    Args:\n",
    "        total_sample_size (int, optional): Total samples. Defaults to 200.\n",
    "        feature_dimension (int, optional): Features. Defaults to 5.\n",
    "        L (int, optional): Size of U, V. Defaults to 2.\n",
    "        H (int, optional): Size of S, T. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization for inner QP. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Steps per iteration. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_global_updates (int, optional): Outer iterations. Defaults to 20.\n",
    "        num_training_samples (int, optional): Training samples. Defaults to 150.\n",
    "        seed (int, optional): Random seed. Defaults to 42.\n",
    "        distribution (str, optional): Noise type ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Noise scale. Defaults to 1.0.\n",
    "        device (str or torch.device, optional): Device (auto-detects CUDA if None). Defaults to None.\n",
    "        loss_type (str, optional): Type of loss function ('mse' or 'mae'). Defaults to \"mse\".\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, beta_opt)\n",
    "            - U, V (torch.Tensor): Optimized constraints, shape (L,).\n",
    "            - S, T (torch.Tensor): Optimized constraints, shape (H,).\n",
    "            - beta_opt (torch.Tensor): Final coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - Uses generate_data for data creation.\n",
    "        - Optimizes U, V, S, T via train_hyperparams.\n",
    "        - Prints train/val performance per iteration.\n",
    "        - Assumes solve_inner_qpth and evaluate_and_print are defined.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_msg(f\"[*] Using device: {device}\", verbose)\n",
    "    \n",
    "    # 1) Generate data\n",
    "    log_msg(f\"\\n==== Generating {distribution.capitalize()} Noise Data ====\", verbose)\n",
    "    X_full, y_full, beta_true = generate_full_data(total_sample_size, feature_dimension, distribution=distribution, scale=scale, seed=seed, device=device)\n",
    "    log_msg(f\"[*] True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "    # 2) Train-Val Split\n",
    "    #log_msg(f\"[*] Splitting data into train/val sets ({num_training_samples}/{total_sample_size-num_training_samples})\", verbose)\n",
    "    #X_train, y_train = X[:num_training_samples], y[:num_training_samples]\n",
    "    #X_val,   y_val   = X[num_training_samples:], y[num_training_samples:]\n",
    "    #这里改用sample函数或者直接用shuffle来做，位置改到下面第一个for那里\n",
    "    \n",
    "    # 3) Initialize hyperparameters\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False) *10 # Usually fixed\n",
    "    \n",
    "    \n",
    "    # 4) Multi-round for-loop: Perform train_hyperparams in each round\n",
    "\n",
    "    all_val_losses = []\n",
    "\n",
    "    \n",
    "    # outer_range = trange(num_global_updates, desc=\"Global Iterations (qpth + GD)\") \n",
    "\n",
    "    for it in range(num_global_updates):\n",
    "        log_msg(f\"\\n--- {distribution.capitalize()} Iteration {it+1}/{num_global_updates} ---\", verbose)\n",
    "        #sample放在这里\n",
    "        seed = random.randint(0, 2**32 - 1)\n",
    "        X_train, y_train, X_remaining, y_remaining = sample_data(num_training_samples, X_full, y_full, seed=seed, device=device)\n",
    "        X_val, y_val, _, _ = sample_data(num_val_samples, X_remaining, y_remaining, seed=seed, device=device)\n",
    "        U, V, S, T, val_loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg = lambda_reg,\n",
    "            lr = lr,\n",
    "            num_hyperparam_iterations = num_hyperparam_iterations,\n",
    "            loss_type = loss_type,\n",
    "            optimizer_choice=optimizer_choice\n",
    "        )\n",
    "        # Solve for final beta_opt after each round\n",
    "        #这个beta不需要计算，直接在前一个函数return一个beta出来就可以\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        log_msg(f\"> Temp beta: {beta_opt.detach().cpu().numpy()}\", \n",
    "        verbose)\n",
    "        log_msg(f\"> True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "        log_msg(f\"> Beta Difference: {beta_true.detach().cpu().numpy()-beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "        all_val_losses.append(val_loss_hist)\n",
    "\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"U\": U.detach().clone(),\n",
    "        \"V\": V.detach().clone(),\n",
    "        \"S\": S.detach().clone(),\n",
    "        \"T\": T.detach().clone(),\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"num_H\":H,\n",
    "        \"num_L\":L,\n",
    "        \"lambda_reg\":lambda_reg,\n",
    "        \"tau\":tau,\n",
    "        \"data_distribution\":distribution,\n",
    "        \"scale\":scale,\n",
    "        \"metric\":loss_type\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"beta_true\": beta_true\n",
    "    }\n",
    "\n",
    "\n",
    "    log_msg(f\"[*] Final beta_opt: {beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final U: {U.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final V: {V.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final S: {S.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final T: {T.detach().cpu().numpy()}\", verbose)\n",
    "    if verbose:\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=f\"[*] Train Autoloss ({distribution})\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=f\"[*] Val Autoloss ({distribution})\")   \n",
    "\n",
    "    val_losses_flat   = [val for iteration_losses in all_val_losses   for val in iteration_losses]\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        # plt.plot(train_losses_flat, label=\"Train Loss\")\n",
    "        plt.plot(val_losses_flat, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(f\"{loss_type.upper()} Loss\")\n",
    "        plt.title(f\"Train vs Val Loss ({distribution}, {loss_type})\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return params, data, all_val_losses\n",
    "\n",
    "################################################\n",
    "#  运行实验\n",
    "################################################\n",
    "\n",
    "# 运行前请确认下列参数是否符合实验需求\n",
    "\n",
    "TOTAL_SAMPLE_SIZE = 1000\n",
    "FEATURE_DIMENSION = 5\n",
    "L = 0\n",
    "H = 5\n",
    "LAMBDA_REG = 0.1\n",
    "NUM_HYPERPARAM_ITERATIONS = 4\n",
    "LR = 1e-2\n",
    "NUM_GLOBAL_UPDATES = 5\n",
    "NUM_TRAINING_SAMPLES = int(0.3*TOTAL_SAMPLE_SIZE)\n",
    "NUM_VAL_SAMPLES = int(0.2*TOTAL_SAMPLE_SIZE)\n",
    "SEED = 42\n",
    "DISTRIBUTION = 'normal'\n",
    "SCALE = 1.0\n",
    "DEVICE = None\n",
    "LOSS_TYPE = \"mae\"\n",
    "VERBOSE = True\n",
    "VISUALIZE = True\n",
    "OPTIMIZER_CHOICE = \"adam\"  # \"adam\", \"sgd\", \"adamw\"\n",
    "\n",
    "# 运行实验\n",
    "autoloss_result, data, val_losses_list = run_experiment(total_sample_size=TOTAL_SAMPLE_SIZE,   \n",
    "                    feature_dimension=FEATURE_DIMENSION,\n",
    "                    L=L, H=H,\n",
    "                    lambda_reg=LAMBDA_REG,\n",
    "                    num_hyperparam_iterations=NUM_HYPERPARAM_ITERATIONS,\n",
    "                    lr=LR,\n",
    "                    num_global_updates=NUM_GLOBAL_UPDATES,\n",
    "                    num_training_samples=NUM_TRAINING_SAMPLES,\n",
    "                    seed=SEED,\n",
    "                    distribution=DISTRIBUTION,\n",
    "                    scale=SCALE,\n",
    "                    device=DEVICE,\n",
    "                    loss_type=LOSS_TYPE,\n",
    "                    verbose=VERBOSE,\n",
    "                    visualize=VISUALIZE,\n",
    "                    optimizer_choice=OPTIMIZER_CHOICE\n",
    "                    )\n",
    "\n",
    "# 保存实验结果\n",
    "with open('autoloss_result.pkl', 'wb') as f:\n",
    "    pickle.dump(autoloss_result, f)\n",
    "    f.close\n",
    "with open('autoloss_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    f.close\n",
    "print(\"[*] Result and data saved to 'autoloss_result.pkl'.\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    5)  测试: 读取数据 & 测试\n",
    "################################################\n",
    "\n",
    "# OLS 训练\n",
    "def train_ols(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train OLS on the given training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Estimated beta coefficients, shape (n_features,).\n",
    "    \"\"\"\n",
    "    beta_ols = torch.linalg.lstsq(X_train, y_train).solution\n",
    "    return beta_ols\n",
    "\n",
    "#加一个最小一乘的函数\n",
    "\n",
    "#  生成测试数据\n",
    "def generate_test_data(num_test_sample, feature_dimension, beta_true, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic test data (X, y) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    and targets y are computed as a linear combination of X and beta_true plus noise\n",
    "    from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        num_test_sample (int): Number of test samples.\n",
    "        feature_dimension (int): Number of features.\n",
    "        beta_true (torch.Tensor): True coefficients, shape (n_features,).\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "            - y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "\n",
    "    Notes:\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    X = torch.randn(num_test_sample, feature_dimension, device=device)\n",
    "    \n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((num_test_sample,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y\n",
    "\n",
    "#  测试 \n",
    "\n",
    "def compute_test_Xbeta(X_test, y_test, beta_est, beta_true):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X_test, y_test) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients by Y_pred = X @ beta_est,\n",
    "    calculates MSE and MAE on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X_test (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y_test (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,).\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (y_mse, y_mae, beta_mse, beta_mae)\n",
    "            - y_mse (float): MSE for predictions on y_test.\n",
    "            - y_mae (float): MAE for predictions on y_test.\n",
    "            - beta_mse (float): MSE for beta_est compared to beta_true.\n",
    "            - beta_mae (float): MAE for beta_est compared to beta_true.\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    beta_est = beta_est.to(X_test.device)\n",
    "    beta_true = beta_true.to(X_test.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = X_test @ beta_est\n",
    "        y_mse = ((y_pred - y_test)**2).mean().item()\n",
    "        y_mae = (y_pred - y_test).abs().mean().item()\n",
    "        # print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            # print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "    return y_mse, y_mae, beta_mse, beta_mae\n",
    "\n",
    "# 读取训练/验证数据\n",
    "train_val_data = pickle.load(open('autoloss_data.pkl', 'rb'))\n",
    "X_train, y_train, X_val, y_val, beta_true = train_val_data[\"X_train\"], train_val_data[\"y_train\"], train_val_data[\"X_val\"], train_val_data[\"y_val\"], train_val_data[\"beta_true\"]\n",
    "X_full = torch.cat([X_train, X_val], dim=0)\n",
    "y_full = torch.cat([y_train, y_val], dim=0)\n",
    "\n",
    "DEVICE = X_train.device \n",
    "# Train OLS\n",
    "beta_ols_train = train_ols(X_train, y_train).to(DEVICE)\n",
    "beta_ols_full = train_ols(X_full, y_full).to(DEVICE)\n",
    "\n",
    "beta_true = beta_true.to(DEVICE)\n",
    "X_train = X_train.to(DEVICE)\n",
    "y_train = y_train.to(DEVICE)\n",
    "X_val = X_val.to(DEVICE)\n",
    "y_val = y_val.to(DEVICE)\n",
    "\n",
    "# Test on OLS\n",
    "X_test, y_test = generate_test_data(50, FEATURE_DIMENSION, beta_true, distribution=DISTRIBUTION, scale=SCALE, seed=SEED, device=DEVICE)\n",
    "mse_ols_test, mae_ols_test, beta_mse_ols, beta_mae_ols = compute_test_Xbeta(X_test, y_test, beta_ols_train, beta_true)\n",
    "mse_ols_test_full, mae_ols_test_full, beta_mse_ols_full, beta_mae_ols_full = compute_test_Xbeta(X_test, y_test, beta_ols_full, beta_true)\n",
    "\n",
    "# Load AutoLoss Result\n",
    "autoloss_result = pickle.load(open('autoloss_result.pkl', 'rb'))\n",
    "U, V, S, T, beta_opt = autoloss_result[\"U\"], autoloss_result[\"V\"], autoloss_result[\"S\"], autoloss_result[\"T\"], autoloss_result[\"beta_opt\"]\n",
    "\n",
    "# Test on AutoLoss\n",
    "mse_autoloss_test, mae_autoloss_test, beta_mse_autoloss, beta_mae_autoloss = compute_test_Xbeta(X_test, y_test, beta_opt, beta_true)\n",
    "\n",
    "# Print Results\n",
    "print(\"===== Test Results =====\")\n",
    "pred_mse_better = \"OLS\" if mse_ols_test < mse_autoloss_test else \"AutoLoss\"\n",
    "pred_mae_better = \"OLS\" if mae_ols_test < mae_autoloss_test else \"AutoLoss\"\n",
    "beta_mse_better = \"OLS\" if beta_mse_ols < beta_mse_autoloss else \"AutoLoss\"\n",
    "beta_mae_better = \"OLS\" if beta_mae_ols < beta_mae_autoloss else \"AutoLoss\"\n",
    "pred_mse_better_full = \"OLS_full\" if mse_ols_test_full < mse_autoloss_test else \"AutoLoss\"\n",
    "pred_mae_better_full = \"OLS_full\" if mae_ols_test_full < mae_autoloss_test else \"AutoLoss\"\n",
    "beta_mse_better_full = \"OLS_full\" if beta_mse_ols_full < beta_mse_autoloss else \"AutoLoss\"\n",
    "beta_mae_better_full = \"OLS_full\" if beta_mae_ols_full < beta_mae_autoloss else \"AutoLoss\"\n",
    "\n",
    "print(f\"OLS Pred MSE: {mse_ols_test:.6f}, AutoLoss Pred MSE: {mse_autoloss_test:.6f}\", f\"({pred_mse_better} better)\")\n",
    "print(f\"OLS Pred MAE: {mae_ols_test:.6f}, AutoLoss Pred MAE: {mae_autoloss_test:.6f}\", f\"({pred_mae_better} better)\")\n",
    "print(f\"OLS Beta MSE: {beta_mse_ols:.6f}, AutoLoss Beta MSE: {beta_mse_autoloss:.6f}\", f\"({beta_mse_better} better)\")\n",
    "print(f\"OLS Beta MAE: {beta_mae_ols:.6f}, AutoLoss Beta MAE: {beta_mae_autoloss:.6f}\", f\"({beta_mae_better} better)\")\n",
    "print(f\"OLS_full Pred MSE: {mse_ols_test_full:.6f}, AutoLoss Pred MSE: {mse_autoloss_test:.6f}\", f\"({pred_mse_better_full} better)\")\n",
    "print(f\"OLS_full Pred MAE: {mae_ols_test_full:.6f}, AutoLoss Pred MAE: {mae_autoloss_test:.6f}\", f\"({pred_mae_better_full} better)\")\n",
    "print(f\"OLS_full Beta MSE: {beta_mse_ols_full:.6f}, AutoLoss Beta MSE: {beta_mse_autoloss:.6f}\", f\"({beta_mse_better_full} better)\")\n",
    "print(f\"OLS_full Beta MAE: {beta_mae_ols_full:.6f}, AutoLoss Beta MAE: {beta_mae_autoloss:.6f}\", f\"({beta_mae_better_full} better)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################\n",
    "#    6)  生成理论AutoLoss 并绘制\n",
    "################################################\n",
    "\n",
    "# 1) 计算理论AutoLoss (L=\\sum_{i=1}^n\\sum_{l=1}^L\\text{ReLU}[u_l(y_i-x_i^T\\beta) +v_l]+\\sum_{i=1}^n\\sum_{h=1}^H\\text{ReHU}_{\\gamma_h}[s_h(y_i-x_i^T\\beta )+t_h]) )\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reHU_piecewise(x, gamma):\n",
    "    r\"\"\"\n",
    "    实现分段函数 ReHU_gamma(x):\n",
    "       = 0,                                       if x <= 0\n",
    "       = 0.5 * x^2,                               if 0 < x < gamma\n",
    "       = (x^2)/4 + (gamma*x)/2 - (gamma^2)/4,      if x >= gamma\n",
    "\n",
    "    x, gamma 均可为张量，通过 torch.where 分段拼接。\n",
    "    \"\"\"\n",
    "    cond0 = (x <= 0)\n",
    "    cond2 = (x >= gamma)\n",
    "    \n",
    "    val0 = torch.zeros_like(x)                          # x <= 0\n",
    "    val1 = 0.5 * x**2                                   # 0 < x < gamma\n",
    "    val2 = (x**2)/4 + 0.5*gamma*x - (gamma**2)/4        # x >= gamma\n",
    "    \n",
    "    out = torch.where(cond0, val0, val1)\n",
    "    out = torch.where(cond2, val2, out)\n",
    "    return out\n",
    "\n",
    "def single_autoloss(r, U, V, S, T, tau):\n",
    "    r\"\"\"\n",
    "    对单个标量残差 r（或一批 r 向量）计算:\n",
    "      L_single(r) = sum_{l=1}^L ReLU(u_l*r + v_l)  \n",
    "                  + sum_{h=1}^H ReHU_tau_h(s_h*r + t_h).\n",
    "\n",
    "    Args:\n",
    "        r (torch.Tensor): shape [N], 表示多个残差点\n",
    "        U, V: shape [L]\n",
    "        S, T, tau: shape [H]\n",
    "    Returns:\n",
    "        torch.Tensor: shape [N], 每个 r 下对应的损失值\n",
    "    \"\"\"\n",
    "    if not isinstance(r, torch.Tensor):\n",
    "        r = torch.tensor(r, dtype=U.dtype, device=U.device)\n",
    "    if r.dim() == 0:\n",
    "        r = r.view(1)  # 保证至少1D\n",
    "    \n",
    "    # 部分1: sum_l ReLU(u_l * r + v_l)\n",
    "    r_vals = r.unsqueeze(0)                 # => shape [1,N]\n",
    "    uv = U.unsqueeze(1)*r_vals + V.unsqueeze(1)  # => [L,N]\n",
    "    partL = torch.relu(uv).sum(dim=0)            # => [N]\n",
    "\n",
    "    # 部分2: sum_h ReHU_{tau_h}(s_h*r + t_h)\n",
    "    st = S.unsqueeze(1)*r_vals + T.unsqueeze(1)     # [H,N]\n",
    "    tau_b = tau.unsqueeze(1).expand_as(st)          # [H,N]\n",
    "    partH = reHU_piecewise(st, tau_b).sum(dim=0)    # [N]\n",
    "    \n",
    "    return partL + partH\n",
    "\n",
    "def plot_theoretical_autoloss(params, r_min=-5, r_max=5, num_points=200):\n",
    "    r\"\"\"\n",
    "    绘制“单点”理论 Autoloss 随残差 r 的分段曲线.\n",
    "\n",
    "    Args:\n",
    "        params (dict): 包含 U, V, S, T, tau 等键的字典\n",
    "        r_min, r_max (float): 绘制区间\n",
    "        num_points (int): 采样点数\n",
    "    \"\"\"\n",
    "    U = params[\"U\"]\n",
    "    V = params[\"V\"]\n",
    "    S = params[\"S\"]\n",
    "    T = params[\"T\"]\n",
    "    # 需要在 params 里也存一下 tau\n",
    "    # 假设 params[\"tau\"] = tau\n",
    "    if \"tau\" not in params:\n",
    "        raise KeyError(\"params中必须包含 'tau' 用来表示 ReHU 的门限.\")\n",
    "    tau = params[\"tau\"]\n",
    "\n",
    "    device = U.device\n",
    "    r_vals = torch.linspace(r_min, r_max, steps=num_points, device=device)\n",
    "    \n",
    "    L_vals = single_autoloss(r_vals, U, V, S, T, tau)\n",
    "    \n",
    "    # 转回 numpy 画图\n",
    "    r_np = r_vals.cpu().numpy()\n",
    "    L_np = L_vals.cpu().numpy()\n",
    "\n",
    "    data_distribution = params[\"data_distribution\"]\n",
    "    scale = params[\"scale\"]\n",
    "    metric = params[\"metric\"]\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(r_np, L_np, label=\"Single-point Autoloss\")\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel(\"Residual r\")\n",
    "    plt.ylabel(\"Autoloss(r)\")\n",
    "    plt.title(f\"Theoretical Autoloss ({data_distribution}, {metric})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 2) 绘制理论AutoLoss\n",
    "\n",
    "plot_theoretical_autoloss(autoloss_result, r_min=-10, r_max=10, num_points=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

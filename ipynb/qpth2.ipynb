{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==== Laplace Noise Data ====\n",
      "\n",
      "-- Train Autoloss on Laplace data --\n",
      "\n",
      "--- Laplace Iteration 1 ---\n",
      "[outer step 10/10] MSE loss = 1.147810\n",
      "Val Autoloss(Laplace) MSE: 1.147461\n",
      "Val Autoloss(Laplace) MAE: 0.761749\n",
      "Val Autoloss(Laplace) Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "Train Autoloss(Laplace) MSE: 2.374356\n",
      "Train Autoloss(Laplace) MAE: 1.123015\n",
      "Train Autoloss(Laplace) Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "\n",
      "--- Laplace Iteration 2 ---\n",
      "[outer step 10/10] MSE loss = 1.144650\n",
      "Val Autoloss(Laplace) MSE: 1.144608\n",
      "Val Autoloss(Laplace) MAE: 0.760095\n",
      "Val Autoloss(Laplace) Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "Train Autoloss(Laplace) MSE: 2.371023\n",
      "Train Autoloss(Laplace) MAE: 1.121967\n",
      "Train Autoloss(Laplace) Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "\n",
      "--- Laplace Iteration 3 ---\n",
      "[outer step 10/10] MSE loss = 1.142888\n",
      "Val Autoloss(Laplace) MSE: 1.142743\n",
      "Val Autoloss(Laplace) MAE: 0.758937\n",
      "Val Autoloss(Laplace) Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "Train Autoloss(Laplace) MSE: 2.368653\n",
      "Train Autoloss(Laplace) MAE: 1.121201\n",
      "Train Autoloss(Laplace) Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "\n",
      "--- Laplace Iteration 4 ---\n",
      "[outer step 10/10] MSE loss = 1.141346\n",
      "Val Autoloss(Laplace) MSE: 1.141182\n",
      "Val Autoloss(Laplace) MAE: 0.757896\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "Train Autoloss(Laplace) MSE: 2.366111\n",
      "Train Autoloss(Laplace) MAE: 1.120396\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "\n",
      "--- Laplace Iteration 5 ---\n",
      "[outer step 10/10] MSE loss = 1.139794\n",
      "Val Autoloss(Laplace) MSE: 1.139638\n",
      "Val Autoloss(Laplace) MAE: 0.756842\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "Train Autoloss(Laplace) MSE: 2.363569\n",
      "Train Autoloss(Laplace) MAE: 1.119596\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "\n",
      "--- Laplace Iteration 6 ---\n",
      "[outer step 10/10] MSE loss = 1.138410\n",
      "Val Autoloss(Laplace) MSE: 1.138213\n",
      "Val Autoloss(Laplace) MAE: 0.755841\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039341, Beta MAE: 0.167564\n",
      "Train Autoloss(Laplace) MSE: 2.361296\n",
      "Train Autoloss(Laplace) MAE: 1.118862\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039341, Beta MAE: 0.167564\n",
      "\n",
      "--- Laplace Iteration 7 ---\n",
      "[outer step 10/10] MSE loss = 1.136786\n",
      "Val Autoloss(Laplace) MSE: 1.136613\n",
      "Val Autoloss(Laplace) MAE: 0.754719\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039049, Beta MAE: 0.167943\n",
      "Train Autoloss(Laplace) MSE: 2.359017\n",
      "Train Autoloss(Laplace) MAE: 1.118107\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039049, Beta MAE: 0.167943\n",
      "\n",
      "--- Laplace Iteration 8 ---\n",
      "[outer step 10/10] MSE loss = 1.135127\n",
      "Val Autoloss(Laplace) MSE: 1.134945\n",
      "Val Autoloss(Laplace) MAE: 0.753553\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038788, Beta MAE: 0.168332\n",
      "Train Autoloss(Laplace) MSE: 2.356928\n",
      "Train Autoloss(Laplace) MAE: 1.117394\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038788, Beta MAE: 0.168332\n",
      "\n",
      "--- Laplace Iteration 9 ---\n",
      "[outer step 10/10] MSE loss = 1.133299\n",
      "Val Autoloss(Laplace) MSE: 1.133102\n",
      "Val Autoloss(Laplace) MAE: 0.752237\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038537, Beta MAE: 0.168776\n",
      "Train Autoloss(Laplace) MSE: 2.354850\n",
      "Train Autoloss(Laplace) MAE: 1.116662\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038537, Beta MAE: 0.168776\n",
      "\n",
      "--- Laplace Iteration 10 ---\n",
      "[outer step 10/10] MSE loss = 1.131017\n",
      "Val Autoloss(Laplace) MSE: 1.130586\n",
      "Val Autoloss(Laplace) MAE: 0.750808\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038241, Beta MAE: 0.169089\n",
      "Train Autoloss(Laplace) MSE: 2.352823\n",
      "Train Autoloss(Laplace) MAE: 1.115885\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038241, Beta MAE: 0.169089\n",
      "\n",
      "--- Laplace Iteration 11 ---\n",
      "[outer step 10/10] MSE loss = 1.126881\n",
      "Val Autoloss(Laplace) MSE: 1.126454\n",
      "Val Autoloss(Laplace) MAE: 0.748560\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037843, Beta MAE: 0.169232\n",
      "Train Autoloss(Laplace) MSE: 2.350700\n",
      "Train Autoloss(Laplace) MAE: 1.114957\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037843, Beta MAE: 0.169232\n",
      "\n",
      "--- Laplace Iteration 12 ---\n",
      "[outer step 10/10] MSE loss = 1.122040\n",
      "Val Autoloss(Laplace) MSE: 1.121576\n",
      "Val Autoloss(Laplace) MAE: 0.745869\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037383, Beta MAE: 0.169352\n",
      "Train Autoloss(Laplace) MSE: 2.348271\n",
      "Train Autoloss(Laplace) MAE: 1.113838\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037383, Beta MAE: 0.169352\n",
      "\n",
      "--- Laplace Iteration 13 ---\n",
      "[outer step 10/10] MSE loss = 1.117755\n",
      "Val Autoloss(Laplace) MSE: 1.117354\n",
      "Val Autoloss(Laplace) MAE: 0.743707\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037094, Beta MAE: 0.169706\n",
      "Train Autoloss(Laplace) MSE: 2.346398\n",
      "Train Autoloss(Laplace) MAE: 1.112978\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037094, Beta MAE: 0.169706\n",
      "\n",
      "--- Laplace Iteration 14 ---\n",
      "[outer step 10/10] MSE loss = 1.114911\n",
      "Val Autoloss(Laplace) MSE: 1.114775\n",
      "Val Autoloss(Laplace) MAE: 0.742085\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036811, Beta MAE: 0.169458\n",
      "Train Autoloss(Laplace) MSE: 2.345076\n",
      "Train Autoloss(Laplace) MAE: 1.112272\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036811, Beta MAE: 0.169458\n",
      "\n",
      "--- Laplace Iteration 15 ---\n",
      "[outer step 10/10] MSE loss = 1.113696\n",
      "Val Autoloss(Laplace) MSE: 1.113587\n",
      "Val Autoloss(Laplace) MAE: 0.741002\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036541, Beta MAE: 0.168765\n",
      "Train Autoloss(Laplace) MSE: 2.344244\n",
      "Train Autoloss(Laplace) MAE: 1.111742\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036541, Beta MAE: 0.168765\n",
      "\n",
      "--- Laplace Iteration 16 ---\n",
      "[outer step 10/10] MSE loss = 1.112621\n",
      "Val Autoloss(Laplace) MSE: 1.112520\n",
      "Val Autoloss(Laplace) MAE: 0.739986\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036290, Beta MAE: 0.168104\n",
      "Train Autoloss(Laplace) MSE: 2.343477\n",
      "Train Autoloss(Laplace) MAE: 1.111238\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036290, Beta MAE: 0.168104\n",
      "\n",
      "--- Laplace Iteration 17 ---\n",
      "[outer step 10/10] MSE loss = 1.111647\n",
      "Val Autoloss(Laplace) MSE: 1.111555\n",
      "Val Autoloss(Laplace) MAE: 0.739273\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036054, Beta MAE: 0.167472\n",
      "Train Autoloss(Laplace) MSE: 2.342761\n",
      "Train Autoloss(Laplace) MAE: 1.110860\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036054, Beta MAE: 0.167472\n",
      "\n",
      "--- Laplace Iteration 18 ---\n",
      "[outer step 10/10] MSE loss = 1.110763\n",
      "Val Autoloss(Laplace) MSE: 1.110613\n",
      "Val Autoloss(Laplace) MAE: 0.738562\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035821, Beta MAE: 0.166794\n",
      "Train Autoloss(Laplace) MSE: 2.342147\n",
      "Train Autoloss(Laplace) MAE: 1.110528\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035821, Beta MAE: 0.166794\n",
      "\n",
      "--- Laplace Iteration 19 ---\n",
      "[outer step 10/10] MSE loss = 1.109815\n",
      "Val Autoloss(Laplace) MSE: 1.109726\n",
      "Val Autoloss(Laplace) MAE: 0.737857\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035595, Beta MAE: 0.166075\n",
      "Train Autoloss(Laplace) MSE: 2.341624\n",
      "Train Autoloss(Laplace) MAE: 1.110219\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035595, Beta MAE: 0.166075\n",
      "\n",
      "--- Laplace Iteration 20 ---\n",
      "[outer step 10/10] MSE loss = 1.108775\n",
      "Val Autoloss(Laplace) MSE: 1.108644\n",
      "Val Autoloss(Laplace) MAE: 0.737002\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035332, Beta MAE: 0.165084\n",
      "Train Autoloss(Laplace) MSE: 2.341305\n",
      "Train Autoloss(Laplace) MAE: 1.109940\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035332, Beta MAE: 0.165084\n"
     ]
    }
   ],
   "source": [
    "######## VERSION 1.0.0 ########\n",
    "# 经过改动正式还原了原始算法\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0      # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0            # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train, X_val, y_val , U, V, S, T, tau, lambda_reg):\n",
    "    \"\"\"\n",
    "    给定超参数(U,V,S,T), 先解内层beta，再对外层loss=MSE做 backward\n",
    "    \"\"\"\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    n = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n) * (y_val - y_val_pred).pow(2).sum()\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg,\n",
    "                      lr=1e-2, outer_steps=50, loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    外层多步迭代, 手动梯度下降更新 U,V,S,T\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, loss_type)\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history.\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    在 (X, y) 上用 beta_est 做预测, 打印 MSE/MAE, 并可对比 beta_true\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X.matmul(beta_est)\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "# ========== 生成: Laplace \n",
    "# 设置超超参数\n",
    "total_sample_size, feature_dimension = 200, 5\n",
    "L, H = 2, 2\n",
    "lambda_reg = 0.1\n",
    "num_hyperparam_iterations = 10\n",
    "lr = 1e-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# (a) Laplace 数据\n",
    "print(\"\\n==== Laplace Noise Data ====\")\n",
    "torch.manual_seed(42)\n",
    "from torch.distributions import Laplace\n",
    "beta_true_lap = torch.rand(feature_dimension, device=device) * 10\n",
    "X_lap = torch.randn(total_sample_size, feature_dimension, device=device)\n",
    "eps_lap = Laplace(0.0, 1.0).sample((total_sample_size,)).to(device)  # scale=1\n",
    "y_lap = X_lap @ beta_true_lap + eps_lap\n",
    "\n",
    "# 1) Laplace\n",
    "print(\"\\n-- Train Autoloss on Laplace data --\")\n",
    "U_lap = torch.randn(L, device=device, requires_grad=True)\n",
    "V_lap = torch.randn(L, device=device, requires_grad=True)\n",
    "S_lap = torch.randn(H, device=device, requires_grad=True)\n",
    "T_lap = torch.randn(H, device=device, requires_grad=True)\n",
    "tau_lap = torch.ones(H, device=device, requires_grad=False)\n",
    "\n",
    "# 2) Train-Val Split\n",
    "num_training_samples = 150\n",
    "X_lap_train, y_lap_train = X_lap[:num_training_samples], y_lap[:num_training_samples]\n",
    "X_lap_val, y_lap_val = X_lap[num_training_samples:], y_lap[num_training_samples:]\n",
    "\n",
    "for iter in range(20):\n",
    "    print(f\"\\n--- Laplace Iteration {iter+1} ---\")\n",
    "    U_lap, V_lap, S_lap, T_lap, lap_loss_hist = train_hyperparams(\n",
    "        X_lap_train, y_lap_train, X_lap_val, y_lap_val,\n",
    "        U_lap, V_lap, S_lap, T_lap, tau_lap,\n",
    "        lambda_reg=lambda_reg, lr=lr, outer_steps=num_hyperparam_iterations\n",
    "    )\n",
    "\n",
    "    beta_autoloss_lap = solve_inner_qpth(U_lap, V_lap, S_lap, T_lap, tau_lap, \n",
    "                                            X_lap_train, y_lap_train, lambda_reg)\n",
    "    \n",
    "    evaluate_and_print(X_lap_val, y_lap_val, beta_autoloss_lap, beta_true_lap, label=\"Val Autoloss(Laplace)\")\n",
    "    evaluate_and_print(X_lap_train, y_lap_train, beta_autoloss_lap, beta_true_lap, label=\"Train Autoloss(Laplace)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0      # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0            # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve the inner quadratic programming (QP) problem to obtain optimal beta coefficients.\n",
    "\n",
    "    This function constructs and solves a QP problem defined by the given hyperparameters\n",
    "    and training data. It uses the QPFunction from the qpth library to minimize the\n",
    "    objective function subject to specified constraints, returning the optimized beta\n",
    "    coefficients.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Coefficient tensor for linear constraints, shape (L,).\n",
    "        V (torch.Tensor): Bias tensor for linear constraints, shape (L,).\n",
    "        S (torch.Tensor): Coefficient tensor for additional constraints, shape (H,).\n",
    "        T (torch.Tensor): Bias tensor for additional constraints, shape (H,).\n",
    "        tau (torch.Tensor): Penalty coefficient tensor for slack variables, shape (H,).\n",
    "        X_train (torch.Tensor): Training feature matrix, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training target vector, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization parameter for beta coefficients.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - The QP problem is formulated as:\n",
    "          minimize 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma\n",
    "          subject to various linear constraints defined by U, V, S, T.\n",
    "        - The function assumes all input tensors are on the same device (e.g., CPU or GPU).\n",
    "        - The qpth.QPFunction solver is used with verbose=False to suppress logging.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train, X_val, y_val , U, V, S, T, tau, lambda_reg, loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Compute gradients of the outer loss with respect to hyperparameters U, V, S, T.\n",
    "\n",
    "    This function solves the inner QP problem to obtain optimal beta coefficients using\n",
    "    the given hyperparameters and training data, then computes the outer loss (MSE) on\n",
    "    validation data. It performs backpropagation to calculate gradients of the outer\n",
    "    loss with respect to the hyperparameters U, V, S, and T.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training feature matrix, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training target vector, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation feature matrix, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation target vector, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Coefficient tensor for linear constraints, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Bias tensor for linear constraints, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Coefficient tensor for additional constraints, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Bias tensor for additional constraints, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Penalty coefficient tensor for slack variables, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for beta coefficients in the inner QP.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"beta_opt\" (torch.Tensor): Optimal beta coefficients, shape (n_features,).\n",
    "            - \"loss\" (float): Mean squared error (MSE) on validation data.\n",
    "            - \"U_grad\" (torch.Tensor): Gradient of outer loss w.r.t. U, shape (L,).\n",
    "            - \"V_grad\" (torch.Tensor): Gradient of outer loss w.r.t. V, shape (L,).\n",
    "            - \"S_grad\" (torch.Tensor): Gradient of outer loss w.r.t. S, shape (H,).\n",
    "            - \"T_grad\" (torch.Tensor): Gradient of outer loss w.r.t. T, shape (H,).\n",
    "\n",
    "    Notes:\n",
    "        - The outer loss is defined as MSE = (1/n_val) * ||y_val - X_val @ beta_opt||^2.\n",
    "        - Gradients are computed via PyTorch's autograd by calling backward() on the loss.\n",
    "        - If a hyperparameter's gradient is None (e.g., requires_grad=False), it is replaced\n",
    "            with a zero tensor of the same shape.\n",
    "        - Gradients are cleared after cloning to avoid accumulation across calls.\n",
    "        - All tensors are assumed to be on the same device (e.g., CPU or GPU).\n",
    "    \"\"\"\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    n = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n) * (y_val - y_val_pred).pow(2).sum()\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "\n",
    "def train_hyperparams(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg,\n",
    "                      lr=1e-2, outer_steps=50, loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        outer_steps (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, loss_type)\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X.matmul(beta_est)\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "            \n",
    "def setup_experiment(n=200, d=5, L=2, H=2, lambda_reg=0.1, outer_steps=10, lr=1e-2, seed=42):\n",
    "    \"\"\"\n",
    "    Set up experiment parameters and environment.\n",
    "\n",
    "    Initializes random seed, selects device (CPU/GPU), and returns a dictionary of\n",
    "    experiment settings.\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): Number of samples. Defaults to 200.\n",
    "        d (int, optional): Number of features. Defaults to 5.\n",
    "        L (int, optional): Size of U and V tensors. Defaults to 2.\n",
    "        H (int, optional): Size of S, T, and tau tensors. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization parameter. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Number of outer iterations. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        dict: Experiment settings with keys: n, d, L, H, lambda_reg, outer_steps, lr, device, seed.\n",
    "\n",
    "    Notes:\n",
    "        - Device is set to CUDA if available, otherwise CPU.\n",
    "        - Random seed is applied via torch.manual_seed.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    return {\n",
    "        \"n\": n, \n",
    "        \"d\": d,\n",
    "        \"L\": L,\n",
    "        \"H\": H,\n",
    "        \"lambda_reg\": lambda_reg,\n",
    "        \"outer_steps\": outer_steps,\n",
    "        \"lr\": lr,\n",
    "        \"device\": device,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "def generate_synthetic_data(config, noise_type=\"laplace\", scale=1.0, df=3.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with specified noise distribution.\n",
    "\n",
    "    Creates feature matrix X, true beta coefficients, and target y with added noise\n",
    "    (Laplace or Student-t), then splits into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Experiment config with keys: n, d, device.\n",
    "        noise_type (str, optional): Noise distribution (\"laplace\" or \"student\"). Defaults to \"laplace\".\n",
    "        scale (float, optional): Noise scale factor. Defaults to 1.0.\n",
    "        df (float, optional): Degrees of freedom for Student-t noise. Defaults to 3.0.\n",
    "\n",
    "    Returns:\n",
    "        dict: Synthetic data with keys: X, y, beta_true, X_train, y_train, X_val, y_val, noise_type.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If noise_type is not \"laplace\" or \"student\".\n",
    "\n",
    "    Notes:\n",
    "        - Splits data with 75% for training, 25% for validation.\n",
    "        - Prints data summary (sample sizes, features).\n",
    "    \"\"\"\n",
    "    n, d, device = config[\"n\"], config[\"d\"], config[\"device\"]\n",
    "    \n",
    "    # 生成真实beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    \n",
    "    # 生成特征\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # 根据噪声类型生成噪声\n",
    "    if noise_type.lower() == \"laplace\":\n",
    "        from torch.distributions import Laplace\n",
    "        eps = Laplace(0.0, scale).sample((n,)).to(device)\n",
    "    elif noise_type.lower() == \"student\":\n",
    "        from torch.distributions import StudentT\n",
    "        eps = StudentT(df=df).sample((n,)).to(device) * scale\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported noise type: {noise_type}\")\n",
    "    \n",
    "    # 生成目标值\n",
    "    y = X @ beta_true + eps\n",
    "    \n",
    "    # 训练验证集划分\n",
    "    N_train = int(0.75 * n)\n",
    "    X_train, y_train = X[:N_train], y[:N_train]\n",
    "    X_val, y_val = X[N_train:], y[N_train:]\n",
    "    \n",
    "    print(f\"\\n==== {noise_type.capitalize()} Noise Data ====\")\n",
    "    print(f\"Number of samples: {n} (Train: {N_train}, Val: {n-N_train})\")\n",
    "    print(f\"Number of features: {d}\")\n",
    "    \n",
    "    return {\n",
    "        \"X\": X, \n",
    "        \"y\": y,\n",
    "        \"beta_true\": beta_true,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"noise_type\": noise_type\n",
    "    }\n",
    "\n",
    "def initialize_hyperparameters(config):\n",
    "    \"\"\"\n",
    "    Initialize model hyperparameters U, V, S, T, and tau.\n",
    "\n",
    "    Creates random tensors for U, V, S, T with gradients enabled, and a constant tau tensor.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Experiment config with keys: L, H, device.\n",
    "\n",
    "    Returns:\n",
    "        dict: Initialized hyperparameters with keys: U, V, S, T, tau.\n",
    "\n",
    "    Notes:\n",
    "        - U, V, S, T are random tensors with requires_grad=True.\n",
    "        - tau is a tensor of ones with requires_grad=False.\n",
    "    \"\"\"\n",
    "    L, H, device = config[\"L\"], config[\"H\"], config[\"device\"]\n",
    "    \n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)\n",
    "    \n",
    "    return {\n",
    "        \"U\": U,\n",
    "        \"V\": V,\n",
    "        \"S\": S,\n",
    "        \"T\": T,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "\n",
    "def train_autoloss(data, hyperparams, config, max_iterations=20, eval_every=1):\n",
    "    \"\"\"\n",
    "    Train AutoLoss model over multiple iterations.\n",
    "\n",
    "    Executes the full AutoLoss training process, optimizing hyperparameters across\n",
    "    iterations and evaluating performance periodically.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Data dictionary with keys: X_train, y_train, X_val, y_val, beta_true, noise_type.\n",
    "        hyperparams (dict): Hyperparameter dictionary with keys: U, V, S, T, tau.\n",
    "        config (dict): Config dictionary with keys: lambda_reg, lr, outer_steps.\n",
    "        max_iterations (int, optional): Total number of iterations. Defaults to 20.\n",
    "        eval_every (int, optional): Evaluate and print every nth iteration. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results with keys:\n",
    "            - \"hyperparams\": Updated hyperparameters (U, V, S, T, tau).\n",
    "            - \"beta\": Final optimized beta coefficients.\n",
    "            - \"metrics\": Lists of train_losses, val_losses, and beta_mses.\n",
    "\n",
    "    Notes:\n",
    "        - Uses train_hyperparams for optimization and solve_inner_qpth for beta computation.\n",
    "        - Prints metrics for train and validation sets at specified intervals.\n",
    "    \"\"\"\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
    "    beta_true = data[\"beta_true\"]\n",
    "    \n",
    "    U, V, S, T, tau = (\n",
    "        hyperparams[\"U\"], \n",
    "        hyperparams[\"V\"],\n",
    "        hyperparams[\"S\"],\n",
    "        hyperparams[\"T\"],\n",
    "        hyperparams[\"tau\"]\n",
    "    )\n",
    "    \n",
    "    lambda_reg = config[\"lambda_reg\"]\n",
    "    lr = config[\"lr\"]\n",
    "    outer_steps = config[\"outer_steps\"]\n",
    "    \n",
    "    noise_type = data[\"noise_type\"]\n",
    "    print(f\"\\n-- Training AutoLoss on {noise_type} data --\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    beta_mses = []\n",
    "    \n",
    "    for iter in range(max_iterations):\n",
    "        print(f\"\\n--- {noise_type} Iteration {iter+1}/{max_iterations} ---\")\n",
    "        \n",
    "        # 训练超参数\n",
    "        U, V, S, T, loss_hist = train_hyperparams(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg=lambda_reg, lr=lr, outer_steps=outer_steps\n",
    "        )\n",
    "        \n",
    "        # 获取最终的beta\n",
    "        beta_autoloss = solve_inner_qpth(\n",
    "            U, V, S, T, tau, \n",
    "            X_train, y_train, lambda_reg\n",
    "        )\n",
    "        \n",
    "        # 评估结果\n",
    "        if (iter + 1) % eval_every == 0 or iter == max_iterations - 1:\n",
    "            val_metrics = evaluate_model(X_val, y_val, beta_autoloss, beta_true)\n",
    "            train_metrics = evaluate_model(X_train, y_train, beta_autoloss, beta_true)\n",
    "            \n",
    "            # 打印评估结果\n",
    "            evaluate_and_print(X_val, y_val, beta_autoloss, beta_true, \n",
    "                              label=f\"Val AutoLoss({noise_type})\")\n",
    "            evaluate_and_print(X_train, y_train, beta_autoloss, beta_true, \n",
    "                              label=f\"Train AutoLoss({noise_type})\")\n",
    "            \n",
    "            train_losses.append(train_metrics[\"mse\"])\n",
    "            val_losses.append(val_metrics[\"mse\"])\n",
    "            beta_mses.append(val_metrics[\"beta_mse\"])\n",
    "    \n",
    "    # 返回训练结果\n",
    "    return {\n",
    "        \"hyperparams\": {\n",
    "            \"U\": U, \"V\": V, \"S\": S, \"T\": T, \"tau\": tau\n",
    "        },\n",
    "        \"beta\": beta_autoloss,\n",
    "        \"metrics\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"beta_mses\": beta_mses\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_model(X, y, beta_est, beta_true=None):\n",
    "    \"\"\"\n",
    "    评估模型性能，返回各种指标\n",
    "    \n",
    "    参数:\n",
    "        X, y: 数据\n",
    "        beta_est: 估计的beta\n",
    "        beta_true: 真实beta (可选)\n",
    "        \n",
    "    返回:\n",
    "        dict: 包含性能指标的字典\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        \n",
    "        metrics = {\n",
    "            \"mse\": mse,\n",
    "            \"mae\": mae,\n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "        \n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            metrics.update({\n",
    "                \"beta_mse\": beta_mse,\n",
    "                \"beta_mae\": beta_mae\n",
    "            })\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "def plot_results(results, title=\"AutoLoss Training Results\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with estimated beta coefficients.\n",
    "\n",
    "    Computes predictions and metrics (MSE, MAE) on given data, optionally comparing\n",
    "    estimated beta to true beta.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor, optional): True beta coefficients, shape (n_features,). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics dictionary with keys:\n",
    "            - \"mse\": Mean squared error (float).\n",
    "            - \"mae\": Mean absolute error (float).\n",
    "            - \"y_pred\": Predicted values (torch.Tensor).\n",
    "            - \"beta_mse\", \"beta_mae\": Beta errors if beta_true is provided (float).\n",
    "\n",
    "    Notes:\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 训练验证损失\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(metrics[\"train_losses\"], label=\"Train Loss\")\n",
    "    plt.plot(metrics[\"val_losses\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    \n",
    "    # Beta MSE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(metrics[\"beta_mses\"], marker='o')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Beta MSE\")\n",
    "    plt.title(\"Beta Estimation Error\")\n",
    "    \n",
    "    # 超参数可视化\n",
    "    plt.subplot(1, 3, 3)\n",
    "    hyperparams = results[\"hyperparams\"]\n",
    "    \n",
    "    # 创建组合索引\n",
    "    param_names = []\n",
    "    param_values = []\n",
    "    \n",
    "    for i, u in enumerate(hyperparams[\"U\"].cpu().detach()):\n",
    "        param_names.append(f\"U_{i}\")\n",
    "        param_values.append(u.item())\n",
    "    \n",
    "    for i, v in enumerate(hyperparams[\"V\"].cpu().detach()):\n",
    "        param_names.append(f\"V_{i}\")\n",
    "        param_values.append(v.item())\n",
    "        \n",
    "    for i, s in enumerate(hyperparams[\"S\"].cpu().detach()):\n",
    "        param_names.append(f\"S_{i}\")\n",
    "        param_values.append(s.item())\n",
    "        \n",
    "    for i, t in enumerate(hyperparams[\"T\"].cpu().detach()):\n",
    "        param_names.append(f\"T_{i}\")\n",
    "        param_values.append(t.item())\n",
    "    \n",
    "    plt.bar(param_names, param_values)\n",
    "    plt.xlabel(\"Parameter\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Final Hyperparameters\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "def run_experiment(noise_type=\"laplace\", **kwargs):\n",
    "    \"\"\"\n",
    "    Run a complete AutoLoss experiment with specified noise type.\n",
    "\n",
    "    Sets up the experiment, generates synthetic data, initializes hyperparameters,\n",
    "    trains the model, and plots results.\n",
    "\n",
    "    Args:\n",
    "        noise_type (str, optional): Noise distribution (\"laplace\" or \"student\"). Defaults to \"laplace\".\n",
    "        **kwargs: Additional arguments passed to setup_experiment (e.g., n, d, lambda_reg).\n",
    "\n",
    "    Returns:\n",
    "        dict: Experiment results with keys:\n",
    "            - \"config\": Experiment configuration.\n",
    "            - \"data\": Generated synthetic data.\n",
    "            - \"results\": Training results from train_autoloss.\n",
    "\n",
    "    Notes:\n",
    "        - Calls setup_experiment, generate_synthetic_data, initialize_hyperparameters,\n",
    "          train_autoloss, and plot_results sequentially.\n",
    "        - Plots results with a title based on noise_type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 设置实验\n",
    "    config = setup_experiment(**kwargs)\n",
    "    \n",
    "    # 生成数据\n",
    "    data = generate_synthetic_data(config, noise_type=noise_type)\n",
    "    \n",
    "    # 初始化超参数\n",
    "    hyperparams = initialize_hyperparameters(config)\n",
    "    \n",
    "    # 训练模型\n",
    "    results = train_autoloss(data, hyperparams, config)\n",
    "    \n",
    "    # 绘制结果\n",
    "    plot_results(results, title=f\"AutoLoss on {noise_type.capitalize()} Noise\")\n",
    "    \n",
    "    # 合并结果\n",
    "    experiment_results = {\n",
    "        \"config\": config,\n",
    "        \"data\": data,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    return experiment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\xinby\\AppData\\Local\\Temp\\ipykernel_18120\\2738433465.py\", line 2, in <module>\n",
      "    laplace_results = run_experiment(\n",
      "                      ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Local\\Temp\\ipykernel_18120\\1819228730.py\", line 637, in run_experiment\n",
      "    config = setup_experiment(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: setup_experiment() got an unexpected keyword argument 'max_iterations'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "# 运行Laplace噪声实验\n",
    "laplace_results = run_experiment(\n",
    "    noise_type=\"laplace\",\n",
    "    n=200, \n",
    "    d=5,\n",
    "    lambda_reg=0.1,\n",
    "    outer_steps=10,\n",
    "    lr=1e-2,\n",
    "    max_iterations=20\n",
    ")\n",
    "\n",
    "# 如果需要，也可以运行Student-t噪声实验\n",
    "student_results = run_experiment(\n",
    "    noise_type=\"student\",\n",
    "    n=200, \n",
    "    d=5,\n",
    "    lambda_reg=0.1,\n",
    "    outer_steps=10,\n",
    "    lr=5e-3,\n",
    "    max_iterations=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==== Generating Laplace Noise Data ====\n",
      "\n",
      "--- Laplace Iteration 1/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.147810\n",
      "[Validation set] Val Autoloss MSE: 1.147461\n",
      "Val Autoloss MAE: 0.761749\n",
      "Val Autoloss Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "[Train set] Train Autoloss MSE: 2.374356\n",
      "Train Autoloss MAE: 1.123015\n",
      "Train Autoloss Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "\n",
      "--- Laplace Iteration 2/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.144650\n",
      "[Validation set] Val Autoloss MSE: 1.144608\n",
      "Val Autoloss MAE: 0.760095\n",
      "Val Autoloss Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "[Train set] Train Autoloss MSE: 2.371023\n",
      "Train Autoloss MAE: 1.121967\n",
      "Train Autoloss Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "\n",
      "--- Laplace Iteration 3/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.142888\n",
      "[Validation set] Val Autoloss MSE: 1.142743\n",
      "Val Autoloss MAE: 0.758937\n",
      "Val Autoloss Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "[Train set] Train Autoloss MSE: 2.368653\n",
      "Train Autoloss MAE: 1.121201\n",
      "Train Autoloss Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "\n",
      "--- Laplace Iteration 4/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.141346\n",
      "[Validation set] Val Autoloss MSE: 1.141182\n",
      "Val Autoloss MAE: 0.757896\n",
      "Val Autoloss Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "[Train set] Train Autoloss MSE: 2.366111\n",
      "Train Autoloss MAE: 1.120396\n",
      "Train Autoloss Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "\n",
      "--- Laplace Iteration 5/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.139794\n",
      "[Validation set] Val Autoloss MSE: 1.139638\n",
      "Val Autoloss MAE: 0.756842\n",
      "Val Autoloss Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "[Train set] Train Autoloss MSE: 2.363569\n",
      "Train Autoloss MAE: 1.119596\n",
      "Train Autoloss Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "\n",
      "Done. Final hyperparams and beta have been obtained.\n"
     ]
    }
   ],
   "source": [
    "######## VERSION 1.1.0 ########\n",
    "# 在 v1.0.0 基础上，封装数据生成与训练逻辑，使代码结构更清晰\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train,\n",
    "                            X_val,   y_val,\n",
    "                            U, V, S, T, tau,\n",
    "                            lambda_reg,\n",
    "                            loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes MSE on validation data, and calculates\n",
    "    gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results with keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss\": Validation MSE (float).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients, shapes (L,) or (H,).\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss: MSE = (1/n_val) * ||y_val - X_val @ beta_opt||^2.\n",
    "        - Gradients computed via PyTorch autograd; None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 解内层\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    # 算外层loss\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    \n",
    "    # backward\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 梯度清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      outer_steps=50):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        outer_steps (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg,\n",
    "                                          loss_type=\"mse\")\n",
    "        \n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "################################################\n",
    "#    4) 核心功能：生成 Laplace 数据 & 训练循环\n",
    "################################################\n",
    "\n",
    "def generate_laplace_data(n, d, scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    生成 (X,y, beta_true) 其中:\n",
    "     - beta_true ~ Uniform(0,10)\n",
    "     - X ~ N(0,1)\n",
    "     - eps ~ Laplace(0, scale)\n",
    "     - y = X beta_true + eps\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace\n",
    "    \n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    dist = Laplace(0.0, scale)\n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    \n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def run_laplace_experiment(n=200, d=5,\n",
    "                           L=2, H=2,\n",
    "                           lambda_reg=0.1,\n",
    "                           outer_steps=10,\n",
    "                           lr=1e-2,\n",
    "                           num_global_updates=20,\n",
    "                           N_train=150,\n",
    "                           seed=42,\n",
    "                           device=None):\n",
    "    \"\"\"\n",
    "    封装一个 Laplace 噪声下的完整实验:\n",
    "     1) 生成数据\n",
    "     2) 初始化超参数\n",
    "     3) 进行多轮(for iter in range(num_global_updates))的外层训练\n",
    "        - 每轮里都再 train_hyperparams(outer_steps)\n",
    "        - 解出beta, 打印在Val/Train集上的性能\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1) 生成数据\n",
    "    print(\"\\n==== Generating Laplace Noise Data ====\")\n",
    "    X, y, beta_true = generate_laplace_data(n, d, scale=1.0, seed=seed, device=device)\n",
    "    \n",
    "    # 2) Train-Val Split\n",
    "    X_train, y_train = X[:N_train], y[:N_train]\n",
    "    X_val,   y_val   = X[N_train:], y[N_train:]\n",
    "    \n",
    "    # 3) 初始化超参数\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    # 4) 多轮 for-loop: 每轮都再执行 train_hyperparams\n",
    "    for it in range(num_global_updates):\n",
    "        print(f\"\\n--- Laplace Iteration {it+1}/{num_global_updates} ---\")\n",
    "        U, V, S, T, loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg=lambda_reg,\n",
    "            lr=lr,\n",
    "            outer_steps=outer_steps\n",
    "        )\n",
    "        # 每轮结束后, 解出最终 beta_opt\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        \n",
    "        print(f\"[Validation set] \", end=\"\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=\"Val Autoloss\")\n",
    "        \n",
    "        print(f\"[Train set] \", end=\"\")\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=\"Train Autoloss\")\n",
    "\n",
    "    return U, V, S, T, beta_opt\n",
    "\n",
    "################################################\n",
    "#  5) 主函数 (可选), 演示\n",
    "################################################\n",
    "\n",
    "def main():\n",
    "    # 在主函数中调用封装后的 run_laplace_experiment\n",
    "    final_U, final_V, final_S, final_T, final_beta = run_laplace_experiment(\n",
    "        n=200, d=5,\n",
    "        L=2, H=2,\n",
    "        lambda_reg=0.1,\n",
    "        outer_steps=10,\n",
    "        lr=1e-2,\n",
    "        n_iter=5,      # 迭代 5 轮 (每轮里再训练 outer_steps=10)\n",
    "        N_train=150,\n",
    "        seed=42\n",
    "    )\n",
    "    print(\"\\nDone. Final hyperparams and beta have been obtained.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "==== Generating Laplace Noise Data ====\n",
      "\n",
      "--- Laplace Iteration 1/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.806671\n",
      "[Validation set] Val Autoloss (laplace) MSE: 1.806652\n",
      "Val Autoloss (laplace) MAE: 0.996116\n",
      "Val Autoloss (laplace) Beta MSE: 0.013706, Beta MAE: 0.095962\n",
      "[Train set] Train Autoloss (laplace) MSE: 2.314499\n",
      "Train Autoloss (laplace) MAE: 1.075226\n",
      "Train Autoloss (laplace) Beta MSE: 0.013706, Beta MAE: 0.095962\n",
      "\n",
      "--- Laplace Iteration 2/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.806482\n",
      "[Validation set] Val Autoloss (laplace) MSE: 1.806463\n",
      "Val Autoloss (laplace) MAE: 0.996059\n",
      "Val Autoloss (laplace) Beta MSE: 0.013641, Beta MAE: 0.095791\n",
      "[Train set] Train Autoloss (laplace) MSE: 2.314416\n",
      "Train Autoloss (laplace) MAE: 1.075272\n",
      "Train Autoloss (laplace) Beta MSE: 0.013641, Beta MAE: 0.095791\n",
      "\n",
      "--- Laplace Iteration 3/5 ---\n"
     ]
    }
   ],
   "source": [
    "######## VERSION 1.1.1 ########\n",
    "# 补充了一些注释和文档，以及一些小的改进\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    调用 qpth.QPFunction 求解给定超参数下的内层 QP 问题, 得到最优 beta 系数\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "    \n",
    "\n",
    "def compute_outer_gradients(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg,\n",
    "    loss_type=\"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes the chosen validation loss (MSE or MAE),\n",
    "    and calculates gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "        loss_type (str): Type of loss function on validation set. Either \"mse\" or \"mae\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss\": Validation loss (float, MSE or MAE).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients w.r.t. U, V, S, T.\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss is chosen based on 'loss_type':\n",
    "            * \"mse\": (1/n_val) * sum((y_val - X_val @ beta_opt)^2)\n",
    "            * \"mae\": (1/n_val) * sum(|y_val - X_val @ beta_opt|)\n",
    "        - If an unknown loss_type is provided, raises ValueError.\n",
    "        - Uses PyTorch autograd; any None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 1) Solve inner QP\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # 2) Compute outer loss on validation set\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        # Mean Squared Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    elif loss_type == \"mae\":\n",
    "        # Mean Absolute Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).abs().sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type '{loss_type}'. Choose 'mse' or 'mae'.\")\n",
    "\n",
    "    # 3) Backprop to get gradients w.r.t. U, V, S, T\n",
    "    loss_outer.backward()\n",
    "\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 4) Zero out grads to avoid accumulation\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      outer_steps=50,\n",
    "                      loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        outer_steps (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg,\n",
    "                                          loss_type)\n",
    "\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    4)  核心: 生成数据 & 运行实验\n",
    "################################################\n",
    "\n",
    "def generate_data(n, d, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate synthetic data (X, y, beta_true) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    true coefficients beta_true from a uniform distribution, and targets y are computed\n",
    "    as a linear combination of X and beta_true plus noise from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        d (int): Number of features.\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal', etc.). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y, beta_true)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n, d).\n",
    "            - y (torch.Tensor): Target vector, shape (n,).\n",
    "            - beta_true (torch.Tensor): True coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - beta_true is sampled from Uniform(0, 10).\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    # Generate true beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # Generate noise based on specified distribution\n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def run_experiment(n=200, d=5,\n",
    "                   L=2, H=2,\n",
    "                   lambda_reg=0.1,\n",
    "                   outer_steps=10,\n",
    "                   lr=1e-2,\n",
    "                   n_iter=20,\n",
    "                   n_train=150,\n",
    "                   seed=42,\n",
    "                   distribution='laplace',\n",
    "                   scale=1.0,\n",
    "                   device=None,\n",
    "                   loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a regression experiment with specified noise distribution.\n",
    "\n",
    "    Generates data, splits it into train/val sets, and runs multiple iterations of hyperparameter\n",
    "    optimization using gradient descent. Evaluates performance on both sets each iteration.\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): Total samples. Defaults to 200.\n",
    "        d (int, optional): Features. Defaults to 5.\n",
    "        L (int, optional): Size of U, V. Defaults to 2.\n",
    "        H (int, optional): Size of S, T. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization for inner QP. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Steps per iteration. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        n_iter (int, optional): Outer iterations. Defaults to 20.\n",
    "        N_train (int, optional): Training samples. Defaults to 150.\n",
    "        seed (int, optional): Random seed. Defaults to 42.\n",
    "        distribution (str, optional): Noise type ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Noise scale. Defaults to 1.0.\n",
    "        device (str or torch.device, optional): Device (auto-detects CUDA if None). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, beta_opt)\n",
    "            - U, V (torch.Tensor): Optimized constraints, shape (L,).\n",
    "            - S, T (torch.Tensor): Optimized constraints, shape (H,).\n",
    "            - beta_opt (torch.Tensor): Final coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - Uses generate_data for data creation.\n",
    "        - Optimizes U, V, S, T via train_hyperparams.\n",
    "        - Prints train/val performance per iteration.\n",
    "        - Assumes solve_inner_qpth and evaluate_and_print are defined.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1) Generate data\n",
    "    print(f\"\\n==== Generating {distribution.capitalize()} Noise Data ====\")\n",
    "    X, y, beta_true = generate_data(n, d, distribution=distribution, scale=scale, seed=seed, device=device)\n",
    "    \n",
    "    # 2) Train-Val Split\n",
    "    X_train, y_train = X[:n_train], y[:n_train]\n",
    "    X_val,   y_val   = X[n_train:], y[n_train:]\n",
    "    \n",
    "    # 3) Initialize hyperparameters\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    # 4) Multi-round for-loop: Perform train_hyperparams in each round\n",
    "    for it in range(n_iter):\n",
    "        print(f\"\\n--- {distribution.capitalize()} Iteration {it+1}/{n_iter} ---\")\n",
    "        U, V, S, T, loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg=lambda_reg,\n",
    "            lr=lr,\n",
    "            outer_steps=outer_steps\n",
    "        )\n",
    "        # Solve for final beta_opt after each round\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        \n",
    "        print(f\"[Validation set] \", end=\"\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=f\"Val Autoloss ({distribution})\")\n",
    "        \n",
    "        print(f\"[Train set] \", end=\"\")\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=f\"Train Autoloss ({distribution})\")\n",
    "\n",
    "    return U, V, S, T, beta_opt\n",
    "\n",
    "\n",
    "run_experiment(n=200, d=5, \n",
    "                L=2, H=2, \n",
    "                lambda_reg=0.1, \n",
    "                outer_steps=10, \n",
    "                lr=1e-2, \n",
    "                n_iter=5, \n",
    "                n_train =150, \n",
    "                seed=42, \n",
    "                distribution='laplace', \n",
    "                scale=1.0, \n",
    "                device=None,\n",
    "                loss_type=\"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Using device: cpu\n",
      "\n",
      "==== Generating Laplace Noise Data ====\n",
      "[*] Splitting data into train/val sets (150/50)\n",
      "\n",
      "--- Laplace Iteration 1/5 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hyperparam Updates:  42%|████▏     | 42/100 [00:40<00:56,  1.04it/s, val_loss=1.806075 (MSE)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 512\u001b[0m\n\u001b[1;32m    506\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m learned_params\n\u001b[0;32m--> 512\u001b[0m run_experiment(total_sample_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, feature_dimension\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m    513\u001b[0m                 L\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, H\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m    514\u001b[0m                 lambda_reg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m    515\u001b[0m                 num_hyperparam_iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m    516\u001b[0m                 lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-2\u001b[39m, \n\u001b[1;32m    517\u001b[0m                 num_global_updates\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, \n\u001b[1;32m    518\u001b[0m                 num_training_samples \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, \n\u001b[1;32m    519\u001b[0m                 seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, \n\u001b[1;32m    520\u001b[0m                 distribution\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlaplace\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    521\u001b[0m                 scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, \n\u001b[1;32m    522\u001b[0m                 device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    523\u001b[0m                 loss_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[35], line 459\u001b[0m, in \u001b[0;36mrun_experiment\u001b[0;34m(total_sample_size, feature_dimension, L, H, lambda_reg, num_hyperparam_iterations, lr, num_global_updates, num_training_samples, seed, distribution, scale, device, loss_type, verbose)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_global_updates):\n\u001b[1;32m    458\u001b[0m     log_msg(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdistribution\u001b[38;5;241m.\u001b[39mcapitalize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mit\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_global_updates\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose)\n\u001b[0;32m--> 459\u001b[0m     U, V, S, T, val_loss_hist \u001b[38;5;241m=\u001b[39m train_hyperparams(\n\u001b[1;32m    460\u001b[0m         X_train, y_train,\n\u001b[1;32m    461\u001b[0m         X_val,   y_val,\n\u001b[1;32m    462\u001b[0m         U, V, S, T, tau,\n\u001b[1;32m    463\u001b[0m         lambda_reg \u001b[38;5;241m=\u001b[39m lambda_reg,\n\u001b[1;32m    464\u001b[0m         lr \u001b[38;5;241m=\u001b[39m lr,\n\u001b[1;32m    465\u001b[0m         num_hyperparam_iterations \u001b[38;5;241m=\u001b[39m num_hyperparam_iterations,\n\u001b[1;32m    466\u001b[0m         loss_type \u001b[38;5;241m=\u001b[39m loss_type\n\u001b[1;32m    467\u001b[0m     )\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;66;03m# Solve for final beta_opt after each round\u001b[39;00m\n\u001b[1;32m    469\u001b[0m     beta_opt \u001b[38;5;241m=\u001b[39m solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
      "Cell \u001b[0;32mIn[35], line 260\u001b[0m, in \u001b[0;36mtrain_hyperparams\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, lr, num_hyperparam_iterations, loss_type)\u001b[0m\n\u001b[1;32m    257\u001b[0m inner_range \u001b[38;5;241m=\u001b[39m trange(num_hyperparam_iterations, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHyperparam Updates\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m inner_range:\n\u001b[0;32m--> 260\u001b[0m     results \u001b[38;5;241m=\u001b[39m compute_outer_gradients(X_train, y_train,\n\u001b[1;32m    261\u001b[0m                                       X_val,   y_val,\n\u001b[1;32m    262\u001b[0m                                       U, V, S, T, tau,\n\u001b[1;32m    263\u001b[0m                                       lambda_reg,\n\u001b[1;32m    264\u001b[0m                                       loss_type)\n\u001b[1;32m    266\u001b[0m     loss_val \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_outer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    267\u001b[0m     U_grad, V_grad \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mU_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m], results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV_grad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[35], line 190\u001b[0m, in \u001b[0;36mcompute_outer_gradients\u001b[0;34m(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg, loss_type)\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported loss_type \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Choose \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmse\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# 3) Backprop to get gradients w.r.t. U, V, S, T\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m loss_outer\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    192\u001b[0m U_grad \u001b[38;5;241m=\u001b[39m U\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m U\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(U)\n\u001b[1;32m    193\u001b[0m V_grad \u001b[38;5;241m=\u001b[39m V\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mif\u001b[39;00m V\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(V)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "######## VERSION 1.1.2 ########\n",
    "# 1. 使用 tqdm 为外层迭代（n_iter）及每轮训练（outer_steps）添加进度条。\n",
    "# 2. 记录并保存每次训练迭代的 train_loss 和 val_loss，在训练结束后进行可视化（用 Matplotlib 画图）。\n",
    "# 3. 在训练完成后，保存最终学到的超参数 (U, V, S, T) 以及由它们解出的 beta_opt 到字典中，方便后续使用或持久化。\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    调用 qpth.QPFunction 求解给定超参数下的内层 QP 问题, 得到最优 beta 系数\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "    \n",
    "\n",
    "def compute_outer_gradients(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg,\n",
    "    loss_type=\"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes the chosen validation loss (MSE or MAE),\n",
    "    and calculates gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "        loss_type (str): Type of loss function on validation set. Either \"mse\" or \"mae\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss_outer\": Validation loss (i.e. OUTER LOSS) (float, MSE or MAE).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients w.r.t. U, V, S, T.\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss is chosen based on 'loss_type':\n",
    "            * \"mse\": (1/n_val) * sum((y_val - X_val @ beta_opt)^2)\n",
    "            * \"mae\": (1/n_val) * sum(|y_val - X_val @ beta_opt|)\n",
    "        - If an unknown loss_type is provided, raises ValueError.\n",
    "        - Uses PyTorch autograd; any None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 1) Solve inner QP\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # 2) Compute outer loss on validation set\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        # Mean Squared Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    elif loss_type == \"mae\":\n",
    "        # Mean Absolute Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).abs().sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type '{loss_type}'. Choose 'mse' or 'mae'.\")\n",
    "\n",
    "    # 3) Backprop to get gradients w.r.t. U, V, S, T\n",
    "    loss_outer.backward()\n",
    "\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 4) Zero out grads to avoid accumulation\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss_outer\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      num_hyperparam_iterations=50,\n",
    "                      loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_hyperparam_iterations (int, optional): Number of iterations for hyperparameter by GD. Defaults to 50.\n",
    "        loss_type (str, optional): Type of loss function on validation set. Defaults to \"mse\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "            - loss_history_val (list): MSE loss on validation set per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_outer_history = []\n",
    "\n",
    "    inner_range = trange(num_hyperparam_iterations, desc='Hyperparam Updates', leave=True)\n",
    "    \n",
    "    for step in inner_range:\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg,\n",
    "                                          loss_type)\n",
    "\n",
    "        loss_val = results[\"loss_outer\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_outer_history.append(loss_val)\n",
    "        captial_loss_type = loss_type.upper()\n",
    "        inner_range.set_postfix(val_loss = f\" {loss_val:.6f} ({captial_loss_type})\")\n",
    "        # if (step+1) % 10 == 0:\n",
    "        #     print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_outer_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    4)  核心: 生成数据 & 运行实验\n",
    "################################################\n",
    "\n",
    "def generate_data(n, d, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate synthetic data (X, y, beta_true) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    true coefficients beta_true from a uniform distribution, and targets y are computed\n",
    "    as a linear combination of X and beta_true plus noise from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        d (int): Number of features.\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal', etc.). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y, beta_true)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n, d).\n",
    "            - y (torch.Tensor): Target vector, shape (n,).\n",
    "            - beta_true (torch.Tensor): True coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - beta_true is sampled from Uniform(0, 10).\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    # Generate true beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # Generate noise based on specified distribution\n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def log_msg(message, verbose = True):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def run_experiment(total_sample_size=200, feature_dimension=5,\n",
    "                   L=2, H=2,\n",
    "                   lambda_reg=0.1,\n",
    "                   num_hyperparam_iterations=10,\n",
    "                   lr=1e-2,\n",
    "                   num_global_updates=20,\n",
    "                   num_training_samples=150,\n",
    "                   seed=42,\n",
    "                   distribution='laplace',\n",
    "                   scale=1.0,\n",
    "                   device=None,\n",
    "                   loss_type=\"mse\",\n",
    "                   verbose=True):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a regression experiment with specified noise distribution.\n",
    "\n",
    "    Generates data, splits it into train/val sets, and runs multiple iterations of hyperparameter\n",
    "    optimization using gradient descent. Evaluates performance on both sets each iteration.\n",
    "\n",
    "    Args:\n",
    "        total_sample_size (int, optional): Total samples. Defaults to 200.\n",
    "        feature_dimension (int, optional): Features. Defaults to 5.\n",
    "        L (int, optional): Size of U, V. Defaults to 2.\n",
    "        H (int, optional): Size of S, T. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization for inner QP. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Steps per iteration. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_global_updates (int, optional): Outer iterations. Defaults to 20.\n",
    "        num_training_samples (int, optional): Training samples. Defaults to 150.\n",
    "        seed (int, optional): Random seed. Defaults to 42.\n",
    "        distribution (str, optional): Noise type ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Noise scale. Defaults to 1.0.\n",
    "        device (str or torch.device, optional): Device (auto-detects CUDA if None). Defaults to None.\n",
    "        loss_type (str, optional): Type of loss function ('mse' or 'mae'). Defaults to \"mse\".\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, beta_opt)\n",
    "            - U, V (torch.Tensor): Optimized constraints, shape (L,).\n",
    "            - S, T (torch.Tensor): Optimized constraints, shape (H,).\n",
    "            - beta_opt (torch.Tensor): Final coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - Uses generate_data for data creation.\n",
    "        - Optimizes U, V, S, T via train_hyperparams.\n",
    "        - Prints train/val performance per iteration.\n",
    "        - Assumes solve_inner_qpth and evaluate_and_print are defined.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_msg(f\"[*] Using device: {device}\", verbose)\n",
    "    \n",
    "    # 1) Generate data\n",
    "    log_msg(f\"\\n==== Generating {distribution.capitalize()} Noise Data ====\", verbose)\n",
    "    X, y, beta_true = generate_data(total_sample_size, feature_dimension, distribution=distribution, scale=scale, seed=seed, device=device)\n",
    "    \n",
    "    # 2) Train-Val Split\n",
    "    log_msg(f\"[*] Splitting data into train/val sets ({num_training_samples}/{total_sample_size-num_training_samples})\", verbose)\n",
    "    X_train, y_train = X[:num_training_samples], y[:num_training_samples]\n",
    "    X_val,   y_val   = X[num_training_samples:], y[num_training_samples:]\n",
    "    \n",
    "    # 3) Initialize hyperparameters\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    \n",
    "    # 4) Multi-round for-loop: Perform train_hyperparams in each round\n",
    "\n",
    "    all_val_losses = []\n",
    "\n",
    "    \n",
    "    # outer_range = trange(num_global_updates, desc=\"Global Iterations (qpth + GD)\") \n",
    "\n",
    "    for it in range(num_global_updates):\n",
    "        log_msg(f\"\\n--- {distribution.capitalize()} Iteration {it+1}/{num_global_updates} ---\", verbose)\n",
    "        U, V, S, T, val_loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg = lambda_reg,\n",
    "            lr = lr,\n",
    "            num_hyperparam_iterations = num_hyperparam_iterations,\n",
    "            loss_type = loss_type\n",
    "        )\n",
    "        # Solve for final beta_opt after each round\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "        # all_train_losses.append(train_loss_hist)\n",
    "        all_val_losses.append(val_loss_hist)\n",
    "        \n",
    "        # print(f\"[Validation set] \", end=\"\")\n",
    "        # evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=f\"Val Autoloss ({distribution})\")\n",
    "        \n",
    "        # print(f\"[Train set] \", end=\"\")\n",
    "        # evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=f\"Train Autoloss ({distribution})\")\n",
    "    \n",
    "    learned_params = {\n",
    "        \"U\": U.detach().clone(),\n",
    "        \"V\": V.detach().clone(),\n",
    "        \"S\": S.detach().clone(),\n",
    "        \"T\": T.detach().clone(),\n",
    "        \"beta_opt\": beta_opt.detach().clone()\n",
    "    }\n",
    "\n",
    "    log_msg(\"\\nFinal learned params saved to 'learned_params' dictionary.\", verbose)\n",
    "    log_msg(f\"[*] Final beta_opt: {beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final U: {U.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final V: {V.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final S: {S.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final T: {T.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "    # train_losses_flat = [val for iteration_losses in all_train_losses for val in iteration_losses]\n",
    "    val_losses_flat   = [val for iteration_losses in all_val_losses   for val in iteration_losses]\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    # plt.plot(train_losses_flat, label=\"Train Loss\")\n",
    "    plt.plot(val_losses_flat, label=\"Val Loss\")\n",
    "    plt.xlabel(\"Overall Steps (num_global_updates * outer_steps)\")\n",
    "    plt.ylabel(f\"{loss_type.upper()} Loss\")\n",
    "    plt.title(f\"Train vs Val Loss ({distribution}, {loss_type})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return learned_params\n",
    "\n",
    "\n",
    "\n",
    "run_experiment(total_sample_size=200, feature_dimension=5, \n",
    "                L=2, H=2, \n",
    "                lambda_reg=0.1, \n",
    "                num_hyperparam_iterations=100, \n",
    "                lr=1e-2, \n",
    "                num_global_updates=5, \n",
    "                num_training_samples =150, \n",
    "                seed=42, \n",
    "                distribution='laplace', \n",
    "                scale=1.0, \n",
    "                device=None,\n",
    "                loss_type=\"mse\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

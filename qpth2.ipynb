{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==== Laplace Noise Data ====\n",
      "\n",
      "-- Train Autoloss on Laplace data --\n",
      "\n",
      "--- Laplace Iteration 1 ---\n",
      "[outer step 10/10] MSE loss = 1.147810\n",
      "Val Autoloss(Laplace) MSE: 1.147461\n",
      "Val Autoloss(Laplace) MAE: 0.761749\n",
      "Val Autoloss(Laplace) Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "Train Autoloss(Laplace) MSE: 2.374356\n",
      "Train Autoloss(Laplace) MAE: 1.123015\n",
      "Train Autoloss(Laplace) Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "\n",
      "--- Laplace Iteration 2 ---\n",
      "[outer step 10/10] MSE loss = 1.144650\n",
      "Val Autoloss(Laplace) MSE: 1.144608\n",
      "Val Autoloss(Laplace) MAE: 0.760095\n",
      "Val Autoloss(Laplace) Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "Train Autoloss(Laplace) MSE: 2.371023\n",
      "Train Autoloss(Laplace) MAE: 1.121967\n",
      "Train Autoloss(Laplace) Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "\n",
      "--- Laplace Iteration 3 ---\n",
      "[outer step 10/10] MSE loss = 1.142888\n",
      "Val Autoloss(Laplace) MSE: 1.142743\n",
      "Val Autoloss(Laplace) MAE: 0.758937\n",
      "Val Autoloss(Laplace) Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "Train Autoloss(Laplace) MSE: 2.368653\n",
      "Train Autoloss(Laplace) MAE: 1.121201\n",
      "Train Autoloss(Laplace) Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "\n",
      "--- Laplace Iteration 4 ---\n",
      "[outer step 10/10] MSE loss = 1.141346\n",
      "Val Autoloss(Laplace) MSE: 1.141182\n",
      "Val Autoloss(Laplace) MAE: 0.757896\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "Train Autoloss(Laplace) MSE: 2.366111\n",
      "Train Autoloss(Laplace) MAE: 1.120396\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "\n",
      "--- Laplace Iteration 5 ---\n",
      "[outer step 10/10] MSE loss = 1.139794\n",
      "Val Autoloss(Laplace) MSE: 1.139638\n",
      "Val Autoloss(Laplace) MAE: 0.756842\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "Train Autoloss(Laplace) MSE: 2.363569\n",
      "Train Autoloss(Laplace) MAE: 1.119596\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "\n",
      "--- Laplace Iteration 6 ---\n",
      "[outer step 10/10] MSE loss = 1.138410\n",
      "Val Autoloss(Laplace) MSE: 1.138213\n",
      "Val Autoloss(Laplace) MAE: 0.755841\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039341, Beta MAE: 0.167564\n",
      "Train Autoloss(Laplace) MSE: 2.361296\n",
      "Train Autoloss(Laplace) MAE: 1.118862\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039341, Beta MAE: 0.167564\n",
      "\n",
      "--- Laplace Iteration 7 ---\n",
      "[outer step 10/10] MSE loss = 1.136786\n",
      "Val Autoloss(Laplace) MSE: 1.136613\n",
      "Val Autoloss(Laplace) MAE: 0.754719\n",
      "Val Autoloss(Laplace) Beta MSE: 0.039049, Beta MAE: 0.167943\n",
      "Train Autoloss(Laplace) MSE: 2.359017\n",
      "Train Autoloss(Laplace) MAE: 1.118107\n",
      "Train Autoloss(Laplace) Beta MSE: 0.039049, Beta MAE: 0.167943\n",
      "\n",
      "--- Laplace Iteration 8 ---\n",
      "[outer step 10/10] MSE loss = 1.135127\n",
      "Val Autoloss(Laplace) MSE: 1.134945\n",
      "Val Autoloss(Laplace) MAE: 0.753553\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038788, Beta MAE: 0.168332\n",
      "Train Autoloss(Laplace) MSE: 2.356928\n",
      "Train Autoloss(Laplace) MAE: 1.117394\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038788, Beta MAE: 0.168332\n",
      "\n",
      "--- Laplace Iteration 9 ---\n",
      "[outer step 10/10] MSE loss = 1.133299\n",
      "Val Autoloss(Laplace) MSE: 1.133102\n",
      "Val Autoloss(Laplace) MAE: 0.752237\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038537, Beta MAE: 0.168776\n",
      "Train Autoloss(Laplace) MSE: 2.354850\n",
      "Train Autoloss(Laplace) MAE: 1.116662\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038537, Beta MAE: 0.168776\n",
      "\n",
      "--- Laplace Iteration 10 ---\n",
      "[outer step 10/10] MSE loss = 1.131017\n",
      "Val Autoloss(Laplace) MSE: 1.130586\n",
      "Val Autoloss(Laplace) MAE: 0.750808\n",
      "Val Autoloss(Laplace) Beta MSE: 0.038241, Beta MAE: 0.169089\n",
      "Train Autoloss(Laplace) MSE: 2.352823\n",
      "Train Autoloss(Laplace) MAE: 1.115885\n",
      "Train Autoloss(Laplace) Beta MSE: 0.038241, Beta MAE: 0.169089\n",
      "\n",
      "--- Laplace Iteration 11 ---\n",
      "[outer step 10/10] MSE loss = 1.126881\n",
      "Val Autoloss(Laplace) MSE: 1.126454\n",
      "Val Autoloss(Laplace) MAE: 0.748560\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037843, Beta MAE: 0.169232\n",
      "Train Autoloss(Laplace) MSE: 2.350700\n",
      "Train Autoloss(Laplace) MAE: 1.114957\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037843, Beta MAE: 0.169232\n",
      "\n",
      "--- Laplace Iteration 12 ---\n",
      "[outer step 10/10] MSE loss = 1.122040\n",
      "Val Autoloss(Laplace) MSE: 1.121576\n",
      "Val Autoloss(Laplace) MAE: 0.745869\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037383, Beta MAE: 0.169352\n",
      "Train Autoloss(Laplace) MSE: 2.348271\n",
      "Train Autoloss(Laplace) MAE: 1.113838\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037383, Beta MAE: 0.169352\n",
      "\n",
      "--- Laplace Iteration 13 ---\n",
      "[outer step 10/10] MSE loss = 1.117755\n",
      "Val Autoloss(Laplace) MSE: 1.117354\n",
      "Val Autoloss(Laplace) MAE: 0.743707\n",
      "Val Autoloss(Laplace) Beta MSE: 0.037094, Beta MAE: 0.169706\n",
      "Train Autoloss(Laplace) MSE: 2.346398\n",
      "Train Autoloss(Laplace) MAE: 1.112978\n",
      "Train Autoloss(Laplace) Beta MSE: 0.037094, Beta MAE: 0.169706\n",
      "\n",
      "--- Laplace Iteration 14 ---\n",
      "[outer step 10/10] MSE loss = 1.114911\n",
      "Val Autoloss(Laplace) MSE: 1.114775\n",
      "Val Autoloss(Laplace) MAE: 0.742085\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036811, Beta MAE: 0.169458\n",
      "Train Autoloss(Laplace) MSE: 2.345076\n",
      "Train Autoloss(Laplace) MAE: 1.112272\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036811, Beta MAE: 0.169458\n",
      "\n",
      "--- Laplace Iteration 15 ---\n",
      "[outer step 10/10] MSE loss = 1.113696\n",
      "Val Autoloss(Laplace) MSE: 1.113587\n",
      "Val Autoloss(Laplace) MAE: 0.741002\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036541, Beta MAE: 0.168765\n",
      "Train Autoloss(Laplace) MSE: 2.344244\n",
      "Train Autoloss(Laplace) MAE: 1.111742\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036541, Beta MAE: 0.168765\n",
      "\n",
      "--- Laplace Iteration 16 ---\n",
      "[outer step 10/10] MSE loss = 1.112621\n",
      "Val Autoloss(Laplace) MSE: 1.112520\n",
      "Val Autoloss(Laplace) MAE: 0.739986\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036290, Beta MAE: 0.168104\n",
      "Train Autoloss(Laplace) MSE: 2.343477\n",
      "Train Autoloss(Laplace) MAE: 1.111238\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036290, Beta MAE: 0.168104\n",
      "\n",
      "--- Laplace Iteration 17 ---\n",
      "[outer step 10/10] MSE loss = 1.111647\n",
      "Val Autoloss(Laplace) MSE: 1.111555\n",
      "Val Autoloss(Laplace) MAE: 0.739273\n",
      "Val Autoloss(Laplace) Beta MSE: 0.036054, Beta MAE: 0.167472\n",
      "Train Autoloss(Laplace) MSE: 2.342761\n",
      "Train Autoloss(Laplace) MAE: 1.110860\n",
      "Train Autoloss(Laplace) Beta MSE: 0.036054, Beta MAE: 0.167472\n",
      "\n",
      "--- Laplace Iteration 18 ---\n",
      "[outer step 10/10] MSE loss = 1.110763\n",
      "Val Autoloss(Laplace) MSE: 1.110613\n",
      "Val Autoloss(Laplace) MAE: 0.738562\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035821, Beta MAE: 0.166794\n",
      "Train Autoloss(Laplace) MSE: 2.342147\n",
      "Train Autoloss(Laplace) MAE: 1.110528\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035821, Beta MAE: 0.166794\n",
      "\n",
      "--- Laplace Iteration 19 ---\n",
      "[outer step 10/10] MSE loss = 1.109815\n",
      "Val Autoloss(Laplace) MSE: 1.109726\n",
      "Val Autoloss(Laplace) MAE: 0.737857\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035595, Beta MAE: 0.166075\n",
      "Train Autoloss(Laplace) MSE: 2.341624\n",
      "Train Autoloss(Laplace) MAE: 1.110219\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035595, Beta MAE: 0.166075\n",
      "\n",
      "--- Laplace Iteration 20 ---\n",
      "[outer step 10/10] MSE loss = 1.108775\n",
      "Val Autoloss(Laplace) MSE: 1.108644\n",
      "Val Autoloss(Laplace) MAE: 0.737002\n",
      "Val Autoloss(Laplace) Beta MSE: 0.035332, Beta MAE: 0.165084\n",
      "Train Autoloss(Laplace) MSE: 2.341305\n",
      "Train Autoloss(Laplace) MAE: 1.109940\n",
      "Train Autoloss(Laplace) Beta MSE: 0.035332, Beta MAE: 0.165084\n"
     ]
    }
   ],
   "source": [
    "######## VERSION 1.0.0 ########\n",
    "# 经过改动正式还原了原始算法\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0      # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0            # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train, X_val, y_val , U, V, S, T, tau, lambda_reg):\n",
    "    \"\"\"\n",
    "    给定超参数(U,V,S,T), 先解内层beta，再对外层loss=MSE做 backward\n",
    "    \"\"\"\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    n = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n) * (y_val - y_val_pred).pow(2).sum()\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg,\n",
    "                      lr=1e-2, outer_steps=50):\n",
    "    \"\"\"\n",
    "    外层多步迭代, 手动梯度下降更新 U,V,S,T\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg)\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history.\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    在 (X, y) 上用 beta_est 做预测, 打印 MSE/MAE, 并可对比 beta_true\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X.matmul(beta_est)\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "# ========== 生成: Laplace \n",
    "# 设置超超参数\n",
    "n, d = 200, 5\n",
    "L, H = 2, 2\n",
    "lambda_reg = 0.1\n",
    "outer_steps = 10\n",
    "lr = 1e-2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# (a) Laplace 数据\n",
    "print(\"\\n==== Laplace Noise Data ====\")\n",
    "torch.manual_seed(42)\n",
    "from torch.distributions import Laplace\n",
    "beta_true_lap = torch.rand(d, device=device) * 10\n",
    "X_lap = torch.randn(n, d, device=device)\n",
    "eps_lap = Laplace(0.0, 1.0).sample((n,)).to(device)  # scale=1\n",
    "y_lap = X_lap @ beta_true_lap + eps_lap\n",
    "\n",
    "# 1) Laplace\n",
    "print(\"\\n-- Train Autoloss on Laplace data --\")\n",
    "U_lap = torch.randn(L, device=device, requires_grad=True)\n",
    "V_lap = torch.randn(L, device=device, requires_grad=True)\n",
    "S_lap = torch.randn(H, device=device, requires_grad=True)\n",
    "T_lap = torch.randn(H, device=device, requires_grad=True)\n",
    "tau_lap = torch.ones(H, device=device, requires_grad=False)\n",
    "\n",
    "# 2) Train-Val Split\n",
    "N_train = 150\n",
    "X_lap_train, y_lap_train = X_lap[:N_train], y_lap[:N_train]\n",
    "X_lap_val, y_lap_val = X_lap[N_train:], y_lap[N_train:]\n",
    "\n",
    "for iter in range(20):\n",
    "    print(f\"\\n--- Laplace Iteration {iter+1} ---\")\n",
    "    U_lap, V_lap, S_lap, T_lap, lap_loss_hist = train_hyperparams(\n",
    "        X_lap_train, y_lap_train, X_lap_val, y_lap_val,\n",
    "        U_lap, V_lap, S_lap, T_lap, tau_lap,\n",
    "        lambda_reg=lambda_reg, lr=lr, outer_steps=outer_steps\n",
    "    )\n",
    "\n",
    "    beta_autoloss_lap = solve_inner_qpth(U_lap, V_lap, S_lap, T_lap, tau_lap, \n",
    "                                            X_lap_train, y_lap_train, lambda_reg)\n",
    "    \n",
    "    evaluate_and_print(X_lap_val, y_lap_val, beta_autoloss_lap, beta_true_lap, label=\"Val Autoloss(Laplace)\")\n",
    "    evaluate_and_print(X_lap_train, y_lap_train, beta_autoloss_lap, beta_true_lap, label=\"Train Autoloss(Laplace)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0      # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0            # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve the inner quadratic programming (QP) problem to obtain optimal beta coefficients.\n",
    "\n",
    "    This function constructs and solves a QP problem defined by the given hyperparameters\n",
    "    and training data. It uses the QPFunction from the qpth library to minimize the\n",
    "    objective function subject to specified constraints, returning the optimized beta\n",
    "    coefficients.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Coefficient tensor for linear constraints, shape (L,).\n",
    "        V (torch.Tensor): Bias tensor for linear constraints, shape (L,).\n",
    "        S (torch.Tensor): Coefficient tensor for additional constraints, shape (H,).\n",
    "        T (torch.Tensor): Bias tensor for additional constraints, shape (H,).\n",
    "        tau (torch.Tensor): Penalty coefficient tensor for slack variables, shape (H,).\n",
    "        X_train (torch.Tensor): Training feature matrix, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training target vector, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization parameter for beta coefficients.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - The QP problem is formulated as:\n",
    "          minimize 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma\n",
    "          subject to various linear constraints defined by U, V, S, T.\n",
    "        - The function assumes all input tensors are on the same device (e.g., CPU or GPU).\n",
    "        - The qpth.QPFunction solver is used with verbose=False to suppress logging.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train, X_val, y_val , U, V, S, T, tau, lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute gradients of the outer loss with respect to hyperparameters U, V, S, T.\n",
    "\n",
    "    This function solves the inner QP problem to obtain optimal beta coefficients using\n",
    "    the given hyperparameters and training data, then computes the outer loss (MSE) on\n",
    "    validation data. It performs backpropagation to calculate gradients of the outer\n",
    "    loss with respect to the hyperparameters U, V, S, and T.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training feature matrix, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training target vector, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation feature matrix, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation target vector, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Coefficient tensor for linear constraints, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Bias tensor for linear constraints, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Coefficient tensor for additional constraints, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Bias tensor for additional constraints, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Penalty coefficient tensor for slack variables, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for beta coefficients in the inner QP.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - \"beta_opt\" (torch.Tensor): Optimal beta coefficients, shape (n_features,).\n",
    "            - \"loss\" (float): Mean squared error (MSE) on validation data.\n",
    "            - \"U_grad\" (torch.Tensor): Gradient of outer loss w.r.t. U, shape (L,).\n",
    "            - \"V_grad\" (torch.Tensor): Gradient of outer loss w.r.t. V, shape (L,).\n",
    "            - \"S_grad\" (torch.Tensor): Gradient of outer loss w.r.t. S, shape (H,).\n",
    "            - \"T_grad\" (torch.Tensor): Gradient of outer loss w.r.t. T, shape (H,).\n",
    "\n",
    "    Notes:\n",
    "        - The outer loss is defined as MSE = (1/n_val) * ||y_val - X_val @ beta_opt||^2.\n",
    "        - Gradients are computed via PyTorch's autograd by calling backward() on the loss.\n",
    "        - If a hyperparameter's gradient is None (e.g., requires_grad=False), it is replaced\n",
    "            with a zero tensor of the same shape.\n",
    "        - Gradients are cleared after cloning to avoid accumulation across calls.\n",
    "        - All tensors are assumed to be on the same device (e.g., CPU or GPU).\n",
    "    \"\"\"\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    n = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n) * (y_val - y_val_pred).pow(2).sum()\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "\n",
    "def train_hyperparams(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg,\n",
    "                      lr=1e-2, outer_steps=50):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        outer_steps (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train, X_val, y_val, U, V, S, T, tau, lambda_reg)\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X.matmul(beta_est)\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "            \n",
    "def setup_experiment(n=200, d=5, L=2, H=2, lambda_reg=0.1, outer_steps=10, lr=1e-2, seed=42):\n",
    "    \"\"\"\n",
    "    Set up experiment parameters and environment.\n",
    "\n",
    "    Initializes random seed, selects device (CPU/GPU), and returns a dictionary of\n",
    "    experiment settings.\n",
    "\n",
    "    Args:\n",
    "        n (int, optional): Number of samples. Defaults to 200.\n",
    "        d (int, optional): Number of features. Defaults to 5.\n",
    "        L (int, optional): Size of U and V tensors. Defaults to 2.\n",
    "        H (int, optional): Size of S, T, and tau tensors. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization parameter. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Number of outer iterations. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        dict: Experiment settings with keys: n, d, L, H, lambda_reg, outer_steps, lr, device, seed.\n",
    "\n",
    "    Notes:\n",
    "        - Device is set to CUDA if available, otherwise CPU.\n",
    "        - Random seed is applied via torch.manual_seed.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    \n",
    "    return {\n",
    "        \"n\": n, \n",
    "        \"d\": d,\n",
    "        \"L\": L,\n",
    "        \"H\": H,\n",
    "        \"lambda_reg\": lambda_reg,\n",
    "        \"outer_steps\": outer_steps,\n",
    "        \"lr\": lr,\n",
    "        \"device\": device,\n",
    "        \"seed\": seed\n",
    "    }\n",
    "\n",
    "def generate_synthetic_data(config, noise_type=\"laplace\", scale=1.0, df=3.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic data with specified noise distribution.\n",
    "\n",
    "    Creates feature matrix X, true beta coefficients, and target y with added noise\n",
    "    (Laplace or Student-t), then splits into training and validation sets.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Experiment config with keys: n, d, device.\n",
    "        noise_type (str, optional): Noise distribution (\"laplace\" or \"student\"). Defaults to \"laplace\".\n",
    "        scale (float, optional): Noise scale factor. Defaults to 1.0.\n",
    "        df (float, optional): Degrees of freedom for Student-t noise. Defaults to 3.0.\n",
    "\n",
    "    Returns:\n",
    "        dict: Synthetic data with keys: X, y, beta_true, X_train, y_train, X_val, y_val, noise_type.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If noise_type is not \"laplace\" or \"student\".\n",
    "\n",
    "    Notes:\n",
    "        - Splits data with 75% for training, 25% for validation.\n",
    "        - Prints data summary (sample sizes, features).\n",
    "    \"\"\"\n",
    "    n, d, device = config[\"n\"], config[\"d\"], config[\"device\"]\n",
    "    \n",
    "    # 生成真实beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    \n",
    "    # 生成特征\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # 根据噪声类型生成噪声\n",
    "    if noise_type.lower() == \"laplace\":\n",
    "        from torch.distributions import Laplace\n",
    "        eps = Laplace(0.0, scale).sample((n,)).to(device)\n",
    "    elif noise_type.lower() == \"student\":\n",
    "        from torch.distributions import StudentT\n",
    "        eps = StudentT(df=df).sample((n,)).to(device) * scale\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported noise type: {noise_type}\")\n",
    "    \n",
    "    # 生成目标值\n",
    "    y = X @ beta_true + eps\n",
    "    \n",
    "    # 训练验证集划分\n",
    "    N_train = int(0.75 * n)\n",
    "    X_train, y_train = X[:N_train], y[:N_train]\n",
    "    X_val, y_val = X[N_train:], y[N_train:]\n",
    "    \n",
    "    print(f\"\\n==== {noise_type.capitalize()} Noise Data ====\")\n",
    "    print(f\"Number of samples: {n} (Train: {N_train}, Val: {n-N_train})\")\n",
    "    print(f\"Number of features: {d}\")\n",
    "    \n",
    "    return {\n",
    "        \"X\": X, \n",
    "        \"y\": y,\n",
    "        \"beta_true\": beta_true,\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"noise_type\": noise_type\n",
    "    }\n",
    "\n",
    "def initialize_hyperparameters(config):\n",
    "    \"\"\"\n",
    "    Initialize model hyperparameters U, V, S, T, and tau.\n",
    "\n",
    "    Creates random tensors for U, V, S, T with gradients enabled, and a constant tau tensor.\n",
    "\n",
    "    Args:\n",
    "        config (dict): Experiment config with keys: L, H, device.\n",
    "\n",
    "    Returns:\n",
    "        dict: Initialized hyperparameters with keys: U, V, S, T, tau.\n",
    "\n",
    "    Notes:\n",
    "        - U, V, S, T are random tensors with requires_grad=True.\n",
    "        - tau is a tensor of ones with requires_grad=False.\n",
    "    \"\"\"\n",
    "    L, H, device = config[\"L\"], config[\"H\"], config[\"device\"]\n",
    "    \n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)\n",
    "    \n",
    "    return {\n",
    "        \"U\": U,\n",
    "        \"V\": V,\n",
    "        \"S\": S,\n",
    "        \"T\": T,\n",
    "        \"tau\": tau\n",
    "    }\n",
    "\n",
    "def train_autoloss(data, hyperparams, config, max_iterations=20, eval_every=1):\n",
    "    \"\"\"\n",
    "    Train AutoLoss model over multiple iterations.\n",
    "\n",
    "    Executes the full AutoLoss training process, optimizing hyperparameters across\n",
    "    iterations and evaluating performance periodically.\n",
    "\n",
    "    Args:\n",
    "        data (dict): Data dictionary with keys: X_train, y_train, X_val, y_val, beta_true, noise_type.\n",
    "        hyperparams (dict): Hyperparameter dictionary with keys: U, V, S, T, tau.\n",
    "        config (dict): Config dictionary with keys: lambda_reg, lr, outer_steps.\n",
    "        max_iterations (int, optional): Total number of iterations. Defaults to 20.\n",
    "        eval_every (int, optional): Evaluate and print every nth iteration. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Training results with keys:\n",
    "            - \"hyperparams\": Updated hyperparameters (U, V, S, T, tau).\n",
    "            - \"beta\": Final optimized beta coefficients.\n",
    "            - \"metrics\": Lists of train_losses, val_losses, and beta_mses.\n",
    "\n",
    "    Notes:\n",
    "        - Uses train_hyperparams for optimization and solve_inner_qpth for beta computation.\n",
    "        - Prints metrics for train and validation sets at specified intervals.\n",
    "    \"\"\"\n",
    "    X_train, y_train = data[\"X_train\"], data[\"y_train\"]\n",
    "    X_val, y_val = data[\"X_val\"], data[\"y_val\"]\n",
    "    beta_true = data[\"beta_true\"]\n",
    "    \n",
    "    U, V, S, T, tau = (\n",
    "        hyperparams[\"U\"], \n",
    "        hyperparams[\"V\"],\n",
    "        hyperparams[\"S\"],\n",
    "        hyperparams[\"T\"],\n",
    "        hyperparams[\"tau\"]\n",
    "    )\n",
    "    \n",
    "    lambda_reg = config[\"lambda_reg\"]\n",
    "    lr = config[\"lr\"]\n",
    "    outer_steps = config[\"outer_steps\"]\n",
    "    \n",
    "    noise_type = data[\"noise_type\"]\n",
    "    print(f\"\\n-- Training AutoLoss on {noise_type} data --\")\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    beta_mses = []\n",
    "    \n",
    "    for iter in range(max_iterations):\n",
    "        print(f\"\\n--- {noise_type} Iteration {iter+1}/{max_iterations} ---\")\n",
    "        \n",
    "        # 训练超参数\n",
    "        U, V, S, T, loss_hist = train_hyperparams(\n",
    "            X_train, y_train, X_val, y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg=lambda_reg, lr=lr, outer_steps=outer_steps\n",
    "        )\n",
    "        \n",
    "        # 获取最终的beta\n",
    "        beta_autoloss = solve_inner_qpth(\n",
    "            U, V, S, T, tau, \n",
    "            X_train, y_train, lambda_reg\n",
    "        )\n",
    "        \n",
    "        # 评估结果\n",
    "        if (iter + 1) % eval_every == 0 or iter == max_iterations - 1:\n",
    "            val_metrics = evaluate_model(X_val, y_val, beta_autoloss, beta_true)\n",
    "            train_metrics = evaluate_model(X_train, y_train, beta_autoloss, beta_true)\n",
    "            \n",
    "            # 打印评估结果\n",
    "            evaluate_and_print(X_val, y_val, beta_autoloss, beta_true, \n",
    "                              label=f\"Val AutoLoss({noise_type})\")\n",
    "            evaluate_and_print(X_train, y_train, beta_autoloss, beta_true, \n",
    "                              label=f\"Train AutoLoss({noise_type})\")\n",
    "            \n",
    "            train_losses.append(train_metrics[\"mse\"])\n",
    "            val_losses.append(val_metrics[\"mse\"])\n",
    "            beta_mses.append(val_metrics[\"beta_mse\"])\n",
    "    \n",
    "    # 返回训练结果\n",
    "    return {\n",
    "        \"hyperparams\": {\n",
    "            \"U\": U, \"V\": V, \"S\": S, \"T\": T, \"tau\": tau\n",
    "        },\n",
    "        \"beta\": beta_autoloss,\n",
    "        \"metrics\": {\n",
    "            \"train_losses\": train_losses,\n",
    "            \"val_losses\": val_losses,\n",
    "            \"beta_mses\": beta_mses\n",
    "        }\n",
    "    }\n",
    "\n",
    "def evaluate_model(X, y, beta_est, beta_true=None):\n",
    "    \"\"\"\n",
    "    评估模型性能，返回各种指标\n",
    "    \n",
    "    参数:\n",
    "        X, y: 数据\n",
    "        beta_est: 估计的beta\n",
    "        beta_true: 真实beta (可选)\n",
    "        \n",
    "    返回:\n",
    "        dict: 包含性能指标的字典\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        \n",
    "        metrics = {\n",
    "            \"mse\": mse,\n",
    "            \"mae\": mae,\n",
    "            \"y_pred\": y_pred\n",
    "        }\n",
    "        \n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            metrics.update({\n",
    "                \"beta_mse\": beta_mse,\n",
    "                \"beta_mae\": beta_mae\n",
    "            })\n",
    "            \n",
    "        return metrics\n",
    "\n",
    "def plot_results(results, title=\"AutoLoss Training Results\"):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with estimated beta coefficients.\n",
    "\n",
    "    Computes predictions and metrics (MSE, MAE) on given data, optionally comparing\n",
    "    estimated beta to true beta.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor, optional): True beta coefficients, shape (n_features,). Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: Metrics dictionary with keys:\n",
    "            - \"mse\": Mean squared error (float).\n",
    "            - \"mae\": Mean absolute error (float).\n",
    "            - \"y_pred\": Predicted values (torch.Tensor).\n",
    "            - \"beta_mse\", \"beta_mae\": Beta errors if beta_true is provided (float).\n",
    "\n",
    "    Notes:\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    metrics = results[\"metrics\"]\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 训练验证损失\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(metrics[\"train_losses\"], label=\"Train Loss\")\n",
    "    plt.plot(metrics[\"val_losses\"], label=\"Val Loss\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    \n",
    "    # Beta MSE\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(metrics[\"beta_mses\"], marker='o')\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Beta MSE\")\n",
    "    plt.title(\"Beta Estimation Error\")\n",
    "    \n",
    "    # 超参数可视化\n",
    "    plt.subplot(1, 3, 3)\n",
    "    hyperparams = results[\"hyperparams\"]\n",
    "    \n",
    "    # 创建组合索引\n",
    "    param_names = []\n",
    "    param_values = []\n",
    "    \n",
    "    for i, u in enumerate(hyperparams[\"U\"].cpu().detach()):\n",
    "        param_names.append(f\"U_{i}\")\n",
    "        param_values.append(u.item())\n",
    "    \n",
    "    for i, v in enumerate(hyperparams[\"V\"].cpu().detach()):\n",
    "        param_names.append(f\"V_{i}\")\n",
    "        param_values.append(v.item())\n",
    "        \n",
    "    for i, s in enumerate(hyperparams[\"S\"].cpu().detach()):\n",
    "        param_names.append(f\"S_{i}\")\n",
    "        param_values.append(s.item())\n",
    "        \n",
    "    for i, t in enumerate(hyperparams[\"T\"].cpu().detach()):\n",
    "        param_names.append(f\"T_{i}\")\n",
    "        param_values.append(t.item())\n",
    "    \n",
    "    plt.bar(param_names, param_values)\n",
    "    plt.xlabel(\"Parameter\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.title(\"Final Hyperparameters\")\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(title, fontsize=16)\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.show()\n",
    "\n",
    "def run_experiment(noise_type=\"laplace\", **kwargs):\n",
    "    \"\"\"\n",
    "    Run a complete AutoLoss experiment with specified noise type.\n",
    "\n",
    "    Sets up the experiment, generates synthetic data, initializes hyperparameters,\n",
    "    trains the model, and plots results.\n",
    "\n",
    "    Args:\n",
    "        noise_type (str, optional): Noise distribution (\"laplace\" or \"student\"). Defaults to \"laplace\".\n",
    "        **kwargs: Additional arguments passed to setup_experiment (e.g., n, d, lambda_reg).\n",
    "\n",
    "    Returns:\n",
    "        dict: Experiment results with keys:\n",
    "            - \"config\": Experiment configuration.\n",
    "            - \"data\": Generated synthetic data.\n",
    "            - \"results\": Training results from train_autoloss.\n",
    "\n",
    "    Notes:\n",
    "        - Calls setup_experiment, generate_synthetic_data, initialize_hyperparameters,\n",
    "          train_autoloss, and plot_results sequentially.\n",
    "        - Plots results with a title based on noise_type.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 设置实验\n",
    "    config = setup_experiment(**kwargs)\n",
    "    \n",
    "    # 生成数据\n",
    "    data = generate_synthetic_data(config, noise_type=noise_type)\n",
    "    \n",
    "    # 初始化超参数\n",
    "    hyperparams = initialize_hyperparameters(config)\n",
    "    \n",
    "    # 训练模型\n",
    "    results = train_autoloss(data, hyperparams, config)\n",
    "    \n",
    "    # 绘制结果\n",
    "    plot_results(results, title=f\"AutoLoss on {noise_type.capitalize()} Noise\")\n",
    "    \n",
    "    # 合并结果\n",
    "    experiment_results = {\n",
    "        \"config\": config,\n",
    "        \"data\": data,\n",
    "        \"results\": results\n",
    "    }\n",
    "    \n",
    "    return experiment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3508, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\xinby\\AppData\\Local\\Temp\\ipykernel_18120\\2738433465.py\", line 2, in <module>\n",
      "    laplace_results = run_experiment(\n",
      "                      ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Local\\Temp\\ipykernel_18120\\1819228730.py\", line 637, in run_experiment\n",
      "    config = setup_experiment(**kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: setup_experiment() got an unexpected keyword argument 'max_iterations'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 2105, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1396, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1287, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 1055, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 955, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py\", line 778, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 677, in included_pieces\n",
      "    scope_pieces = self.scope_pieces\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 614, in scope_pieces\n",
      "    scope_start, scope_end = self.source.line_range(self.scope)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\xinby\\AppData\\Roaming\\Python\\Python311\\site-packages\\stack_data\\core.py\", line 178, in line_range\n",
      "    return line_range(self.asttext(), node)\n",
      "                      ^^^^^^^^^^^^\n",
      "AttributeError: 'Source' object has no attribute 'asttext'\n"
     ]
    }
   ],
   "source": [
    "# 运行Laplace噪声实验\n",
    "laplace_results = run_experiment(\n",
    "    noise_type=\"laplace\",\n",
    "    n=200, \n",
    "    d=5,\n",
    "    lambda_reg=0.1,\n",
    "    outer_steps=10,\n",
    "    lr=1e-2,\n",
    "    max_iterations=20\n",
    ")\n",
    "\n",
    "# 如果需要，也可以运行Student-t噪声实验\n",
    "student_results = run_experiment(\n",
    "    noise_type=\"student\",\n",
    "    n=200, \n",
    "    d=5,\n",
    "    lambda_reg=0.1,\n",
    "    outer_steps=10,\n",
    "    lr=5e-3,\n",
    "    max_iterations=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "==== Generating Laplace Noise Data ====\n",
      "\n",
      "--- Laplace Iteration 1/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.147810\n",
      "[Validation set] Val Autoloss MSE: 1.147461\n",
      "Val Autoloss MAE: 0.761749\n",
      "Val Autoloss Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "[Train set] Train Autoloss MSE: 2.374356\n",
      "Train Autoloss MAE: 1.123015\n",
      "Train Autoloss Beta MSE: 0.041236, Beta MAE: 0.165974\n",
      "\n",
      "--- Laplace Iteration 2/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.144650\n",
      "[Validation set] Val Autoloss MSE: 1.144608\n",
      "Val Autoloss MAE: 0.760095\n",
      "Val Autoloss Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "[Train set] Train Autoloss MSE: 2.371023\n",
      "Train Autoloss MAE: 1.121967\n",
      "Train Autoloss Beta MSE: 0.040721, Beta MAE: 0.166360\n",
      "\n",
      "--- Laplace Iteration 3/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.142888\n",
      "[Validation set] Val Autoloss MSE: 1.142743\n",
      "Val Autoloss MAE: 0.758937\n",
      "Val Autoloss Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "[Train set] Train Autoloss MSE: 2.368653\n",
      "Train Autoloss MAE: 1.121201\n",
      "Train Autoloss Beta MSE: 0.040346, Beta MAE: 0.166572\n",
      "\n",
      "--- Laplace Iteration 4/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.141346\n",
      "[Validation set] Val Autoloss MSE: 1.141182\n",
      "Val Autoloss MAE: 0.757896\n",
      "Val Autoloss Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "[Train set] Train Autoloss MSE: 2.366111\n",
      "Train Autoloss MAE: 1.120396\n",
      "Train Autoloss Beta MSE: 0.039981, Beta MAE: 0.166861\n",
      "\n",
      "--- Laplace Iteration 5/5 ---\n",
      "[outer step 10/10] Val MSE loss = 1.139794\n",
      "[Validation set] Val Autoloss MSE: 1.139638\n",
      "Val Autoloss MAE: 0.756842\n",
      "Val Autoloss Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "[Train set] Train Autoloss MSE: 2.363569\n",
      "Train Autoloss MAE: 1.119596\n",
      "Train Autoloss Beta MSE: 0.039641, Beta MAE: 0.167224\n",
      "\n",
      "Done. Final hyperparams and beta have been obtained.\n"
     ]
    }
   ],
   "source": [
    "######## VERSION 1.1.0 ########\n",
    "# 在 v1.0.0 基础上，封装数据生成与训练逻辑，使代码结构更清晰\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "\n",
    "def compute_outer_gradients(X_train, y_train,\n",
    "                            X_val,   y_val,\n",
    "                            U, V, S, T, tau,\n",
    "                            lambda_reg):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes MSE on validation data, and calculates\n",
    "    gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "\n",
    "    Returns:\n",
    "        dict: Results with keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss\": Validation MSE (float).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients, shapes (L,) or (H,).\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss: MSE = (1/n_val) * ||y_val - X_val @ beta_opt||^2.\n",
    "        - Gradients computed via PyTorch autograd; None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 解内层\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    \n",
    "    # 算外层loss\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "    loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    \n",
    "    # backward\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 梯度清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      outer_steps=50):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        outer_steps (int, optional): Number of iterations. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_history = []\n",
    "    for step in range(outer_steps):\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg)\n",
    "        loss_val = results[\"loss\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_history.append(loss_val)\n",
    "        if (step+1) % 10 == 0:\n",
    "            print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE: {mse:.6f}\")\n",
    "        print(f\"{label} MAE: {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} Beta MSE: {beta_mse:.6f}, Beta MAE: {beta_mae:.6f}\")\n",
    "\n",
    "################################################\n",
    "#    4) 核心功能：生成 Laplace 数据 & 训练循环\n",
    "################################################\n",
    "\n",
    "def generate_laplace_data(n, d, scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    生成 (X,y, beta_true) 其中:\n",
    "     - beta_true ~ Uniform(0,10)\n",
    "     - X ~ N(0,1)\n",
    "     - eps ~ Laplace(0, scale)\n",
    "     - y = X beta_true + eps\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace\n",
    "    \n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    dist = Laplace(0.0, scale)\n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    \n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def run_laplace_experiment(n=200, d=5,\n",
    "                           L=2, H=2,\n",
    "                           lambda_reg=0.1,\n",
    "                           outer_steps=10,\n",
    "                           lr=1e-2,\n",
    "                           n_iter=20,\n",
    "                           N_train=150,\n",
    "                           seed=42,\n",
    "                           device=None):\n",
    "    \"\"\"\n",
    "    封装一个 Laplace 噪声下的完整实验:\n",
    "     1) 生成数据\n",
    "     2) 初始化超参数\n",
    "     3) 进行多轮(for iter in range(n_iter))的外层训练\n",
    "        - 每轮里都再 train_hyperparams(outer_steps)\n",
    "        - 解出beta, 打印在Val/Train集上的性能\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1) 生成数据\n",
    "    print(\"\\n==== Generating Laplace Noise Data ====\")\n",
    "    X, y, beta_true = generate_laplace_data(n, d, scale=1.0, seed=seed, device=device)\n",
    "    \n",
    "    # 2) Train-Val Split\n",
    "    X_train, y_train = X[:N_train], y[:N_train]\n",
    "    X_val,   y_val   = X[N_train:], y[N_train:]\n",
    "    \n",
    "    # 3) 初始化超参数\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    # 4) 多轮 for-loop: 每轮都再执行 train_hyperparams\n",
    "    for it in range(n_iter):\n",
    "        print(f\"\\n--- Laplace Iteration {it+1}/{n_iter} ---\")\n",
    "        U, V, S, T, loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg=lambda_reg,\n",
    "            lr=lr,\n",
    "            outer_steps=outer_steps\n",
    "        )\n",
    "        # 每轮结束后, 解出最终 beta_opt\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        \n",
    "        print(f\"[Validation set] \", end=\"\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=\"Val Autoloss\")\n",
    "        \n",
    "        print(f\"[Train set] \", end=\"\")\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=\"Train Autoloss\")\n",
    "\n",
    "    return U, V, S, T, beta_opt\n",
    "\n",
    "################################################\n",
    "#  5) 主函数 (可选), 演示\n",
    "################################################\n",
    "\n",
    "def main():\n",
    "    # 在主函数中调用封装后的 run_laplace_experiment\n",
    "    final_U, final_V, final_S, final_T, final_beta = run_laplace_experiment(\n",
    "        n=200, d=5,\n",
    "        L=2, H=2,\n",
    "        lambda_reg=0.1,\n",
    "        outer_steps=10,\n",
    "        lr=1e-2,\n",
    "        n_iter=5,      # 迭代 5 轮 (每轮里再训练 outer_steps=10)\n",
    "        N_train=150,\n",
    "        seed=42\n",
    "    )\n",
    "    print(\"\\nDone. Final hyperparams and beta have been obtained.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####### VERSION 1.2.1 ########\n",
    "# 1. 完整增加了 Test 部分 和 Theoretical Loss 的绘制\n",
    "# 2. 实现了保存结果到文件的功能并且优化了保存的参数\n",
    "# 3. 增加了一个新的全局参数 VISUALIZE 用于控制是否绘制图像(以便在服务器上运行)\n",
    "\n",
    "######## VERSION 1.2.0 ########\n",
    "# 1. 使用 tqdm 为外层迭代及每轮训练添加进度条。\n",
    "# 2. 记录并保存每次训练迭代的 train_loss 和 val_loss，在训练结束后进行可视化（用 Matplotlib 画图）。\n",
    "# 3. 在训练完成后，保存最终学到的超参数 (U, V, S, T) 以及由它们解出的 beta_opt 到字典中，方便后续使用或持久化。\n",
    "# 4. 可以自主选择 MAE / MSE 作为外层训练的损失函数，通过 loss_type 参数指定。\n",
    "# 5. 优化了代码结构，将核心代码封装为函数，方便调用和复用。\n",
    "# 6. 优化了部分变量名和注释，提高代码可读性。\n",
    "\n",
    "######## TODO ########\n",
    "# --- 重要 ---\n",
    "# [1] ! tau 这个参数似乎处理的有问题. 这个是给定的还是要学习的?\n",
    "# [2] ! 用大规模数据集测试 MAE+Gaussian & MSE+Laplace 的效果\n",
    "\n",
    "# --- 一般 ---\n",
    "# [4] 代码注释和文档整理. [!! 由于改变/增加了部分变量或名称, 需要更新docstring !!]\n",
    "# [6] 中间优化的部分能不能从手动的GD改为利用Pytorch的优化器进行优化\n",
    "# [7] 最开始的两个QP构造函数的准确性验证\n",
    "# [8]优化中有时qpth会warning非稳定解, 需要关注其稳定性和影响\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import pickle\n",
    "\n",
    "################################################\n",
    "#            1)   内层 QP 构造与求解\n",
    "################################################\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    构建Q, p, G, h，用于内层QP：\n",
    "      minimize 0.5 * [beta^T * diag(lambda_reg) * beta + theta^T theta + sigma^T sigma]\n",
    "               + tau^T sigma\n",
    "      subject to  pi_li >= U_l(...) + V_l, ...\n",
    "    \"\"\"\n",
    "    n, d = X_train.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    total_vars = d + L*n + 2*H*n  # [beta, pi, theta, sigma]\n",
    "    \n",
    "    # 对角线 Q\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    Q_diag[:d] = lambda_reg              # beta 的正则\n",
    "    Q_diag[d + L*n : d + L*n + H*n] = 1.0  # theta\n",
    "    Q_diag[d + L*n + H*n : ] = 1.0         # sigma\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    # p 向量\n",
    "    p = torch.zeros(total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    p[d + L*n + H*n:] = tau.repeat(n)  # sigma部分\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 不等式约束 G z <= h\n",
    "    G_rows = 2*L*n + 2*H*n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X_train.dtype, device=X_train.device)\n",
    "    h_val = torch.zeros(G_rows, dtype=X_train.dtype, device=X_train.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # pi_li >= U_l * (y_i - x_i^T beta) + V_l\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X_train[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = U[l]*y_train[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # theta_hi + sigma_hi >= S_h*(y_i - x_i^T beta) + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X_train[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = S[h_]*y_train[i] + T[h_]\n",
    "            row_idx += 1\n",
    "\n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0\n",
    "            h_val[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "\n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_val[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_val.unsqueeze(0)\n",
    "    \n",
    "    # 数值扰动，确保 Q SPD\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X_train.dtype, device=X_train.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg):\n",
    "    \"\"\"\n",
    "    Solve inner QP problem to obtain optimal beta coefficients.\n",
    "\n",
    "    调用 qpth.QPFunction 求解给定超参数下的内层 QP 问题, 得到最优 beta 系数\n",
    "\n",
    "    Constructs and solves a quadratic programming problem using qpth.QPFunction,\n",
    "    minimizing a regularized objective subject to constraints defined by hyperparameters.\n",
    "\n",
    "    Args:\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,).\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,).\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,).\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,).\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        X_train (torch.Tensor): Training features, shape (n_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_samples,).\n",
    "        lambda_reg (float): Regularization strength for beta.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Optimal beta coefficients, shape (n_features,).\n",
    "\n",
    "    Notes:\n",
    "        - Objective: 0.5 * (beta^T * diag(lambda_reg) * beta + theta^T * theta + sigma^T * sigma) + tau^T * sigma.\n",
    "        - Constraints are derived from U, V, S, T.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0, device=X_train.device), torch.empty(0, device=X_train.device))\n",
    "    d = X_train.shape[1]\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "################################################\n",
    "#   2) 外层训练 (带Train/Val) + 梯度计算\n",
    "################################################\n",
    "    \n",
    "\n",
    "def compute_outer_gradients(\n",
    "    X_train, y_train,\n",
    "    X_val,   y_val,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg,\n",
    "    loss_type=\"mse\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute outer loss gradients w.r.t. hyperparameters U, V, S, T.\n",
    "\n",
    "    Solves the inner QP for beta, computes the chosen validation loss (MSE or MAE),\n",
    "    and calculates gradients of the outer loss via backpropagation.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization strength for inner QP beta.\n",
    "        loss_type (str): Type of loss function on validation set. Either \"mse\" or \"mae\".\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary with the following keys:\n",
    "            - \"beta_opt\": Optimal beta, shape (n_features,).\n",
    "            - \"loss_outer\": Validation loss (i.e. OUTER LOSS) (float, MSE or MAE).\n",
    "            - \"U_grad\", \"V_grad\", \"S_grad\", \"T_grad\": Gradients w.r.t. U, V, S, T.\n",
    "\n",
    "    Notes:\n",
    "        - Outer loss is chosen based on 'loss_type':\n",
    "            * \"mse\": (1/n_val) * sum((y_val - X_val @ beta_opt)^2)\n",
    "            * \"mae\": (1/n_val) * sum(|y_val - X_val @ beta_opt|)\n",
    "        - If an unknown loss_type is provided, raises ValueError.\n",
    "        - Uses PyTorch autograd; any None gradients replaced with zeros.\n",
    "        - Assumes all tensors on same device.\n",
    "    \"\"\"\n",
    "    # 1) Solve inner QP\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "\n",
    "    # 2) Compute outer loss on validation set\n",
    "    n_val = X_val.shape[0]\n",
    "    y_val_pred = X_val @ beta_opt\n",
    "\n",
    "    if loss_type == \"mse\":\n",
    "        # Mean Squared Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).pow(2).sum()\n",
    "    elif loss_type == \"mae\":\n",
    "        # Mean Absolute Error\n",
    "        loss_outer = (1.0 / n_val) * (y_val - y_val_pred).abs().sum()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported loss_type '{loss_type}'. Choose 'mse' or 'mae'.\")\n",
    "\n",
    "    # 3) Backprop to get gradients w.r.t. U, V, S, T\n",
    "    loss_outer.backward()\n",
    "\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 4) Zero out grads to avoid accumulation\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"loss_outer\": loss_outer.item(),\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad\n",
    "    }\n",
    "\n",
    "def train_hyperparams(X_train, y_train,\n",
    "                      X_val,   y_val,\n",
    "                      U, V, S, T, tau,\n",
    "                      lambda_reg,\n",
    "                      lr=1e-2,\n",
    "                      num_hyperparam_iterations=50,\n",
    "                      loss_type=\"mse\"):\n",
    "    \"\"\"\n",
    "    Train hyperparameters U, V, S, T via gradient descent on outer MSE loss.\n",
    "\n",
    "    Performs multiple steps of gradient descent to optimize U, V, S, T based on the outer\n",
    "    MSE loss, computed using beta from an inner QP solver.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "        X_val (torch.Tensor): Validation features, shape (n_val_samples, n_features).\n",
    "        y_val (torch.Tensor): Validation targets, shape (n_val_samples,).\n",
    "        U (torch.Tensor): Linear constraint coefficients, shape (L,), requires grad.\n",
    "        V (torch.Tensor): Linear constraint biases, shape (L,), requires grad.\n",
    "        S (torch.Tensor): Additional constraint coefficients, shape (H,), requires grad.\n",
    "        T (torch.Tensor): Additional constraint biases, shape (H,), requires grad.\n",
    "        tau (torch.Tensor): Slack variable penalties, shape (H,).\n",
    "        lambda_reg (float): Regularization parameter for inner QP beta.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_hyperparam_iterations (int, optional): Number of iterations for hyperparameter by GD. Defaults to 50.\n",
    "        loss_type (str, optional): Type of loss function on validation set. Defaults to \"mse\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, loss_history)\n",
    "            - U, V, S, T (torch.Tensor): Updated hyperparameters.\n",
    "            - loss_history (list): MSE loss per step.\n",
    "            - loss_history_val (list): MSE loss on validation set per step.\n",
    "\n",
    "    Notes:\n",
    "        - Uses compute_outer_gradients for gradient computation.\n",
    "        - Prints MSE every 10 steps.\n",
    "        - Assumes all tensors are on the same device.\n",
    "    \"\"\"\n",
    "    loss_outer_history = []\n",
    "\n",
    "    inner_range = trange(num_hyperparam_iterations, desc='Hyperparam Updates', leave=True)\n",
    "    \n",
    "    for step in inner_range:\n",
    "        results = compute_outer_gradients(X_train, y_train,\n",
    "                                          X_val,   y_val,\n",
    "                                          U, V, S, T, tau,\n",
    "                                          lambda_reg,\n",
    "                                          loss_type)\n",
    "\n",
    "        loss_val = results[\"loss_outer\"]\n",
    "        U_grad, V_grad = results[\"U_grad\"], results[\"V_grad\"]\n",
    "        S_grad, T_grad = results[\"S_grad\"], results[\"T_grad\"]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            U -= lr * U_grad\n",
    "            V -= lr * V_grad\n",
    "            S -= lr * S_grad\n",
    "            T -= lr * T_grad\n",
    "        \n",
    "        # 继续需要梯度\n",
    "        U.requires_grad_(True)\n",
    "        V.requires_grad_(True)\n",
    "        S.requires_grad_(True)\n",
    "        T.requires_grad_(True)\n",
    "        \n",
    "        loss_outer_history.append(loss_val)\n",
    "        captial_loss_type = loss_type.upper()\n",
    "        inner_range.set_postfix(val_loss = f\" {loss_val:.6f} ({captial_loss_type})\")\n",
    "        # if (step+1) % 10 == 0:\n",
    "        #     print(f\"[outer step {step+1}/{outer_steps}] Val MSE loss = {loss_val:.6f}\")\n",
    "    \n",
    "    return U, V, S, T, loss_outer_history\n",
    "\n",
    "################################################\n",
    "#    3) 辅助: 评估/打印\n",
    "################################################\n",
    "\n",
    "def evaluate_and_print(X, y, beta_est, beta_true, label=\"\"):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X, y) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients, calculates MSE and MAE\n",
    "    on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,), optional.\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X @ beta_est\n",
    "        mse = ((y_pred - y)**2).mean().item()\n",
    "        mae = (y_pred - y).abs().mean().item()\n",
    "        print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "\n",
    "\n",
    "################################################\n",
    "#    4)  核心: 生成数据 & 运行实验\n",
    "################################################\n",
    "\n",
    "def generate_data(n, d, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Generate synthetic data (X, y, beta_true) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    true coefficients beta_true from a uniform distribution, and targets y are computed\n",
    "    as a linear combination of X and beta_true plus noise from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        n (int): Number of samples.\n",
    "        d (int): Number of features.\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal', etc.). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y, beta_true)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n, d).\n",
    "            - y (torch.Tensor): Target vector, shape (n,).\n",
    "            - beta_true (torch.Tensor): True coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - beta_true is sampled from Uniform(0, 10).\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    # Generate true beta\n",
    "    beta_true = torch.rand(d, device=device) * 10\n",
    "    X = torch.randn(n, d, device=device)\n",
    "    \n",
    "    # Generate noise based on specified distribution\n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((n,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y, beta_true\n",
    "\n",
    "def log_msg(message, verbose = True):\n",
    "    if verbose:\n",
    "        print(message)\n",
    "\n",
    "def run_experiment(total_sample_size=200, feature_dimension=5,\n",
    "                   L=2, H=2,\n",
    "                   lambda_reg=0.1,\n",
    "                   num_hyperparam_iterations=10,\n",
    "                   lr=1e-2,\n",
    "                   num_global_updates=20,\n",
    "                   num_training_samples=150,\n",
    "                   seed=42,\n",
    "                   distribution='laplace',\n",
    "                   scale=1.0,\n",
    "                   device=None,\n",
    "                   loss_type=\"mse\",\n",
    "                   verbose=True,\n",
    "                   visualize=True):\n",
    "    \"\"\"\n",
    "    Optimize hyperparameters for a regression experiment with specified noise distribution.\n",
    "\n",
    "    Generates data, splits it into train/val sets, and runs multiple iterations of hyperparameter\n",
    "    optimization using gradient descent. Evaluates performance on both sets each iteration.\n",
    "\n",
    "    Args:\n",
    "        total_sample_size (int, optional): Total samples. Defaults to 200.\n",
    "        feature_dimension (int, optional): Features. Defaults to 5.\n",
    "        L (int, optional): Size of U, V. Defaults to 2.\n",
    "        H (int, optional): Size of S, T. Defaults to 2.\n",
    "        lambda_reg (float, optional): Regularization for inner QP. Defaults to 0.1.\n",
    "        outer_steps (int, optional): Steps per iteration. Defaults to 10.\n",
    "        lr (float, optional): Learning rate. Defaults to 1e-2.\n",
    "        num_global_updates (int, optional): Outer iterations. Defaults to 20.\n",
    "        num_training_samples (int, optional): Training samples. Defaults to 150.\n",
    "        seed (int, optional): Random seed. Defaults to 42.\n",
    "        distribution (str, optional): Noise type ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Noise scale. Defaults to 1.0.\n",
    "        device (str or torch.device, optional): Device (auto-detects CUDA if None). Defaults to None.\n",
    "        loss_type (str, optional): Type of loss function ('mse' or 'mae'). Defaults to \"mse\".\n",
    "        verbose (bool, optional): Whether to print progress. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (U, V, S, T, beta_opt)\n",
    "            - U, V (torch.Tensor): Optimized constraints, shape (L,).\n",
    "            - S, T (torch.Tensor): Optimized constraints, shape (H,).\n",
    "            - beta_opt (torch.Tensor): Final coefficients, shape (d,).\n",
    "\n",
    "    Notes:\n",
    "        - Uses generate_data for data creation.\n",
    "        - Optimizes U, V, S, T via train_hyperparams.\n",
    "        - Prints train/val performance per iteration.\n",
    "        - Assumes solve_inner_qpth and evaluate_and_print are defined.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    log_msg(f\"[*] Using device: {device}\", verbose)\n",
    "    \n",
    "    # 1) Generate data\n",
    "    log_msg(f\"\\n==== Generating {distribution.capitalize()} Noise Data ====\", verbose)\n",
    "    X, y, beta_true = generate_data(total_sample_size, feature_dimension, distribution=distribution, scale=scale, seed=seed, device=device)\n",
    "    log_msg(f\"[*] True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "    # 2) Train-Val Split\n",
    "    log_msg(f\"[*] Splitting data into train/val sets ({num_training_samples}/{total_sample_size-num_training_samples})\", verbose)\n",
    "    X_train, y_train = X[:num_training_samples], y[:num_training_samples]\n",
    "    X_val,   y_val   = X[num_training_samples:], y[num_training_samples:]\n",
    "    \n",
    "    # 3) Initialize hyperparameters\n",
    "    U = torch.randn(L, device=device, requires_grad=True)\n",
    "    V = torch.randn(L, device=device, requires_grad=True)\n",
    "    S = torch.randn(H, device=device, requires_grad=True)\n",
    "    T = torch.randn(H, device=device, requires_grad=True)\n",
    "    tau = torch.ones(H, device=device, requires_grad=False)  # Usually fixed\n",
    "    \n",
    "    \n",
    "    # 4) Multi-round for-loop: Perform train_hyperparams in each round\n",
    "\n",
    "    all_val_losses = []\n",
    "\n",
    "    \n",
    "    # outer_range = trange(num_global_updates, desc=\"Global Iterations (qpth + GD)\") \n",
    "\n",
    "    for it in range(num_global_updates):\n",
    "        log_msg(f\"\\n--- {distribution.capitalize()} Iteration {it+1}/{num_global_updates} ---\", verbose)\n",
    "        U, V, S, T, val_loss_hist = train_hyperparams(\n",
    "            X_train, y_train,\n",
    "            X_val,   y_val,\n",
    "            U, V, S, T, tau,\n",
    "            lambda_reg = lambda_reg,\n",
    "            lr = lr,\n",
    "            num_hyperparam_iterations = num_hyperparam_iterations,\n",
    "            loss_type = loss_type\n",
    "        )\n",
    "        # Solve for final beta_opt after each round\n",
    "        beta_opt = solve_inner_qpth(U, V, S, T, tau, X_train, y_train, lambda_reg)\n",
    "        log_msg(f\"> Temp beta: {beta_opt.detach().cpu().numpy()}\", \n",
    "        verbose)\n",
    "        log_msg(f\"> True beta: {beta_true.detach().cpu().numpy()}\", verbose)\n",
    "        log_msg(f\"> Beta Difference: {beta_true.detach().cpu().numpy()-beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "\n",
    "        all_val_losses.append(val_loss_hist)\n",
    "\n",
    "\n",
    "    \n",
    "    params = {\n",
    "        \"U\": U.detach().clone(),\n",
    "        \"V\": V.detach().clone(),\n",
    "        \"S\": S.detach().clone(),\n",
    "        \"T\": T.detach().clone(),\n",
    "        \"beta_opt\": beta_opt.detach().clone(),\n",
    "        \"num_H\":H,\n",
    "        \"num_L\":L,\n",
    "        \"lambda_reg\":lambda_reg,\n",
    "        \"tau\":tau,\n",
    "        \"data_distribution\":distribution,\n",
    "        \"scale\":scale,\n",
    "        \"metric\":loss_type\n",
    "    }\n",
    "\n",
    "    data = {\n",
    "        \"X_train\": X_train,\n",
    "        \"y_train\": y_train,\n",
    "        \"X_val\": X_val,\n",
    "        \"y_val\": y_val,\n",
    "        \"beta_true\": beta_true\n",
    "    }\n",
    "\n",
    "\n",
    "    log_msg(f\"[*] Final beta_opt: {beta_opt.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final U: {U.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final V: {V.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final S: {S.detach().cpu().numpy()}\", verbose)\n",
    "    log_msg(f\"[*] Final T: {T.detach().cpu().numpy()}\", verbose)\n",
    "    if verbose:\n",
    "        evaluate_and_print(X_train, y_train, beta_opt, beta_true, label=f\"[*] Train Autoloss ({distribution})\")\n",
    "        evaluate_and_print(X_val, y_val, beta_opt, beta_true, label=f\"[*] Val Autoloss ({distribution})\")   \n",
    "\n",
    "    val_losses_flat   = [val for iteration_losses in all_val_losses   for val in iteration_losses]\n",
    "    \n",
    "    if visualize:\n",
    "        plt.figure(figsize=(8,6))\n",
    "        # plt.plot(train_losses_flat, label=\"Train Loss\")\n",
    "        plt.plot(val_losses_flat, label=\"Val Loss\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(f\"{loss_type.upper()} Loss\")\n",
    "        plt.title(f\"Train vs Val Loss ({distribution}, {loss_type})\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return params, data, all_val_losses\n",
    "\n",
    "################################################\n",
    "#  运行实验\n",
    "################################################\n",
    "\n",
    "# 运行前请确认下列参数是否符合实验需求\n",
    "\n",
    "TOTAL_SAMPLE_SIZE = 200\n",
    "FEATURE_DIMENSION = 5\n",
    "L = 2\n",
    "H = 2\n",
    "LAMBDA_REG = 0.1\n",
    "NUM_HYPERPARAM_ITERATIONS = 2\n",
    "LR = 1e-2\n",
    "NUM_GLOBAL_UPDATES = 2\n",
    "NUM_TRAINING_SAMPLES = 150\n",
    "SEED = 42\n",
    "DISTRIBUTION = 'laplace'\n",
    "SCALE = 1.0\n",
    "DEVICE = None\n",
    "LOSS_TYPE = \"mse\"\n",
    "VERBOSE = True\n",
    "VISUALIZE = True\n",
    "\n",
    "# 运行实验\n",
    "autoloss_result, data, val_losses_list = run_experiment(total_sample_size=TOTAL_SAMPLE_SIZE,   \n",
    "                    feature_dimension=FEATURE_DIMENSION,\n",
    "                    L=L, H=H,\n",
    "                    lambda_reg=LAMBDA_REG,\n",
    "                    num_hyperparam_iterations=NUM_HYPERPARAM_ITERATIONS,\n",
    "                    lr=LR,\n",
    "                    num_global_updates=NUM_GLOBAL_UPDATES,\n",
    "                    num_training_samples=NUM_TRAINING_SAMPLES,\n",
    "                    seed=SEED,\n",
    "                    distribution=DISTRIBUTION,\n",
    "                    scale=SCALE,\n",
    "                    device=DEVICE,\n",
    "                    loss_type=LOSS_TYPE,\n",
    "                    verbose=VERBOSE,\n",
    "                    visualize=VISUALIZE)\n",
    "\n",
    "# 保存实验结果\n",
    "with open('autoloss_result.pkl', 'wb') as f:\n",
    "    pickle.dump(autoloss_result, f)\n",
    "    f.close\n",
    "with open('autoloss_data.pkl', 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "    f.close\n",
    "print(\"[*] Result and data saved to 'autoloss_result.pkl'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#    5)  测试: 读取数据 & 测试\n",
    "################################################\n",
    "\n",
    "# OLS 训练\n",
    "def train_ols(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Train OLS on the given training data.\n",
    "\n",
    "    Args:\n",
    "        X_train (torch.Tensor): Training features, shape (n_train_samples, n_features).\n",
    "        y_train (torch.Tensor): Training targets, shape (n_train_samples,).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Estimated beta coefficients, shape (n_features,).\n",
    "    \"\"\"\n",
    "    beta_ols = torch.linalg.lstsq(X_train, y_train).solution\n",
    "    return beta_ols\n",
    "\n",
    "#  生成测试数据\n",
    "def generate_test_data(num_test_sample, feature_dimension, beta_true, distribution='laplace', scale=1.0, seed=42, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Generate synthetic test data (X, y) with noise from a specified distribution.\n",
    "\n",
    "    Creates a dataset where features X are drawn from a standard normal distribution,\n",
    "    and targets y are computed as a linear combination of X and beta_true plus noise\n",
    "    from the specified distribution.\n",
    "\n",
    "    Args:\n",
    "        num_test_sample (int): Number of test samples.\n",
    "        feature_dimension (int): Number of features.\n",
    "        beta_true (torch.Tensor): True coefficients, shape (n_features,).\n",
    "        distribution (str, optional): Type of noise distribution ('laplace', 'normal'). Defaults to 'laplace'.\n",
    "        scale (float, optional): Scale parameter for the noise distribution. Defaults to 1.0.\n",
    "        seed (int, optional): Random seed for reproducibility. Defaults to 42.\n",
    "        device (str or torch.device, optional): Device to generate tensors on. Defaults to \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X, y)\n",
    "            - X (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "            - y (torch.Tensor): Target vector, shape (n_samples,).\n",
    "\n",
    "    Notes:\n",
    "        - X is sampled from N(0, 1).\n",
    "        - Noise (eps) is sampled from the specified distribution with given scale.\n",
    "        - Supported distributions: 'laplace', 'normal'. Others raise ValueError.\n",
    "        - All tensors are placed on the specified device.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    from torch.distributions import Laplace, Normal\n",
    "    \n",
    "    X = torch.randn(num_test_sample, feature_dimension, device=device)\n",
    "    \n",
    "    if distribution.lower() == 'laplace':\n",
    "        dist = Laplace(0.0, scale)\n",
    "    elif distribution.lower() == 'normal':\n",
    "        dist = Normal(0.0, scale)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported distribution: {distribution}. Supported options: 'laplace', 'normal'\")\n",
    "    \n",
    "    eps = dist.sample((num_test_sample,)).to(device)\n",
    "    y = X @ beta_true + eps\n",
    "    return X, y\n",
    "\n",
    "#  测试 \n",
    "\n",
    "def compute_test_Xbeta(X_test, y_test, beta_est, beta_true):\n",
    "    \"\"\"\n",
    "    Evaluate predictions using beta_est on (X_test, y_test) and print MSE/MAE metrics.\n",
    "\n",
    "    Computes predictions with estimated beta coefficients by Y_pred = X @ beta_est,\n",
    "    calculates MSE and MAE on the given data, and optionally compares beta_est to beta_true.\n",
    "\n",
    "    Args:\n",
    "        X_test (torch.Tensor): Feature matrix, shape (n_samples, n_features).\n",
    "        y_test (torch.Tensor): Target vector, shape (n_samples,).\n",
    "        beta_est (torch.Tensor): Estimated beta coefficients, shape (n_features,).\n",
    "        beta_true (torch.Tensor): True beta coefficients, shape (n_features,).\n",
    "        label (str, optional): Prefix for printed metrics. Defaults to \"\".\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (y_mse, y_mae, beta_mse, beta_mae)\n",
    "            - y_mse (float): MSE for predictions on y_test.\n",
    "            - y_mae (float): MAE for predictions on y_test.\n",
    "            - beta_mse (float): MSE for beta_est compared to beta_true.\n",
    "            - beta_mae (float): MAE for beta_est compared to beta_true.\n",
    "\n",
    "    Notes:\n",
    "        - Prints MSE and MAE for predictions, and Beta MSE/MAE if beta_true is provided.\n",
    "        - Computations are performed without gradient tracking.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        y_pred = X_test @ beta_est\n",
    "        y_mse = ((y_pred - y_test)**2).mean().item()\n",
    "        y_mae = (y_pred - y_test).abs().mean().item()\n",
    "        # print(f\"{label} MSE(y): {mse:.6f}, MAE(y): {mae:.6f}\")\n",
    "        if beta_true is not None:\n",
    "            beta_mse = ((beta_est - beta_true)**2).mean().item()\n",
    "            beta_mae = (beta_est - beta_true).abs().mean().item()\n",
    "            # print(f\"{label} MSE(beta): {beta_mse:.6f}, MAE(beta): {beta_mae:.6f}\")\n",
    "    return y_mse, y_mae, beta_mse, beta_mae\n",
    "\n",
    "# 读取训练/验证数据\n",
    "train_val_data = pickle.load(open('autoloss_data.pkl', 'rb'))\n",
    "X_train, y_train, X_val, y_val, beta_true = train_val_data[\"X_train\"], train_val_data[\"y_train\"], train_val_data[\"X_val\"], train_val_data[\"y_val\"], train_val_data[\"beta_true\"]\n",
    "X_full = torch.cat([X_train, X_val], dim=0)\n",
    "y_full = torch.cat([y_train, y_val], dim=0)\n",
    "\n",
    "# Train OLS\n",
    "beta_ols_train = train_ols(X_train, y_train)\n",
    "beta_ols_train_val = train_ols(X_full, y_full)\n",
    "\n",
    "# Test on OLS\n",
    "X_test, y_test = generate_test_data(50, FEATURE_DIMENSION, beta_true, distribution=DISTRIBUTION, scale=SCALE, seed=SEED, device=DEVICE)\n",
    "mse_ols_test, mae_ols_test, beta_mse_ols, beta_mae_ols = compute_test_Xbeta(X_test, y_test, beta_ols_train, beta_true)\n",
    "\n",
    "# Load AutoLoss Result\n",
    "autoloss_result = pickle.load(open('autoloss_result.pkl', 'rb'))\n",
    "U, V, S, T, beta_opt = autoloss_result[\"U\"], autoloss_result[\"V\"], autoloss_result[\"S\"], autoloss_result[\"T\"], autoloss_result[\"beta_opt\"]\n",
    "\n",
    "# Test on AutoLoss\n",
    "mse_autoloss_test, mae_autoloss_test, beta_mse_autoloss, beta_mae_autoloss = compute_test_Xbeta(X_test, y_test, beta_opt, beta_true)\n",
    "\n",
    "# Print Results\n",
    "print(\"===== Test Results =====\")\n",
    "pred_mse_better = \"OLS\" if mse_ols_test < mse_autoloss_test else \"AutoLoss\"\n",
    "pred_mae_better = \"OLS\" if mae_ols_test < mae_autoloss_test else \"AutoLoss\"\n",
    "beta_mse_better = \"OLS\" if beta_mse_ols < beta_mse_autoloss else \"AutoLoss\"\n",
    "beta_mae_better = \"OLS\" if beta_mae_ols < beta_mae_autoloss else \"AutoLoss\"\n",
    "print(f\"OLS Pred MSE: {mse_ols_test:.6f}, AutoLoss Pred MSE: {mse_autoloss_test:.6f}\", f\"({pred_mse_better} better)\")\n",
    "print(f\"OLS Pred MAE: {mae_ols_test:.6f}, AutoLoss Pred MAE: {mae_autoloss_test:.6f}\", f\"({pred_mae_better} better)\")\n",
    "print(f\"OLS Beta MSE: {beta_mse_ols:.6f}, AutoLoss Beta MSE: {beta_mse_autoloss:.6f}\", f\"({beta_mse_better} better)\")\n",
    "print(f\"OLS Beta MAE: {beta_mae_ols:.6f}, AutoLoss Beta MAE: {beta_mae_autoloss:.6f}\", f\"({beta_mae_better} better)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "#    6)  生成理论AutoLoss 并绘制\n",
    "################################################\n",
    "\n",
    "# 1) 计算理论AutoLoss (L=\\sum_{i=1}^n\\sum_{l=1}^L\\text{ReLU}[u_l(y_i-x_i^T\\beta) +v_l]+\\sum_{i=1}^n\\sum_{h=1}^H\\text{ReHU}_{\\gamma_h}[s_h(y_i-x_i^T\\beta )+t_h]) )\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def reHU_piecewise(x, gamma):\n",
    "    r\"\"\"\n",
    "    实现分段函数 ReHU_gamma(x):\n",
    "       = 0,                                       if x <= 0\n",
    "       = 0.5 * x^2,                               if 0 < x < gamma\n",
    "       = (x^2)/4 + (gamma*x)/2 - (gamma^2)/4,      if x >= gamma\n",
    "\n",
    "    x, gamma 均可为张量，通过 torch.where 分段拼接。\n",
    "    \"\"\"\n",
    "    cond0 = (x <= 0)\n",
    "    cond2 = (x >= gamma)\n",
    "    \n",
    "    val0 = torch.zeros_like(x)                          # x <= 0\n",
    "    val1 = 0.5 * x**2                                   # 0 < x < gamma\n",
    "    val2 = (x**2)/4 + 0.5*gamma*x - (gamma**2)/4        # x >= gamma\n",
    "    \n",
    "    out = torch.where(cond0, val0, val1)\n",
    "    out = torch.where(cond2, val2, out)\n",
    "    return out\n",
    "\n",
    "def single_autoloss(r, U, V, S, T, tau):\n",
    "    r\"\"\"\n",
    "    对单个标量残差 r（或一批 r 向量）计算:\n",
    "      L_single(r) = sum_{l=1}^L ReLU(u_l*r + v_l)  \n",
    "                  + sum_{h=1}^H ReHU_tau_h(s_h*r + t_h).\n",
    "\n",
    "    Args:\n",
    "        r (torch.Tensor): shape [N], 表示多个残差点\n",
    "        U, V: shape [L]\n",
    "        S, T, tau: shape [H]\n",
    "    Returns:\n",
    "        torch.Tensor: shape [N], 每个 r 下对应的损失值\n",
    "    \"\"\"\n",
    "    if not isinstance(r, torch.Tensor):\n",
    "        r = torch.tensor(r, dtype=U.dtype, device=U.device)\n",
    "    if r.dim() == 0:\n",
    "        r = r.view(1)  # 保证至少1D\n",
    "    \n",
    "    # 部分1: sum_l ReLU(u_l * r + v_l)\n",
    "    r_vals = r.unsqueeze(0)                 # => shape [1,N]\n",
    "    uv = U.unsqueeze(1)*r_vals + V.unsqueeze(1)  # => [L,N]\n",
    "    partL = torch.relu(uv).sum(dim=0)            # => [N]\n",
    "\n",
    "    # 部分2: sum_h ReHU_{tau_h}(s_h*r + t_h)\n",
    "    st = S.unsqueeze(1)*r_vals + T.unsqueeze(1)     # [H,N]\n",
    "    tau_b = tau.unsqueeze(1).expand_as(st)          # [H,N]\n",
    "    partH = reHU_piecewise(st, tau_b).sum(dim=0)    # [N]\n",
    "    \n",
    "    return partL + partH\n",
    "\n",
    "def plot_theoretical_autoloss(params, r_min=-5, r_max=5, num_points=200):\n",
    "    r\"\"\"\n",
    "    绘制“单点”理论 Autoloss 随残差 r 的分段曲线.\n",
    "\n",
    "    Args:\n",
    "        params (dict): 包含 U, V, S, T, tau 等键的字典\n",
    "        r_min, r_max (float): 绘制区间\n",
    "        num_points (int): 采样点数\n",
    "    \"\"\"\n",
    "    U = params[\"U\"]\n",
    "    V = params[\"V\"]\n",
    "    S = params[\"S\"]\n",
    "    T = params[\"T\"]\n",
    "    # 需要在 params 里也存一下 tau\n",
    "    # 假设 params[\"tau\"] = tau\n",
    "    if \"tau\" not in params:\n",
    "        raise KeyError(\"params中必须包含 'tau' 用来表示 ReHU 的门限.\")\n",
    "    tau = params[\"tau\"]\n",
    "\n",
    "    device = U.device\n",
    "    r_vals = torch.linspace(r_min, r_max, steps=num_points, device=device)\n",
    "    \n",
    "    L_vals = single_autoloss(r_vals, U, V, S, T, tau)\n",
    "    \n",
    "    # 转回 numpy 画图\n",
    "    r_np = r_vals.cpu().numpy()\n",
    "    L_np = L_vals.cpu().numpy()\n",
    "\n",
    "    data_distribution = params[\"data_distribution\"]\n",
    "    scale = params[\"scale\"]\n",
    "    metric = params[\"metric\"]\n",
    "    \n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.plot(r_np, L_np, label=\"Single-point Autoloss\")\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.xlabel(\"Residual r\")\n",
    "    plt.ylabel(\"Autoloss(r)\")\n",
    "    plt.title(f\"Theoretical Autoloss ({data_distribution}, {metric})\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 2) 绘制理论AutoLoss\n",
    "\n",
    "plot_theoretical_autoloss(autoloss_result, r_min=-5, r_max=5, num_points=200)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

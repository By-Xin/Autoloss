{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始超参数：\n",
      "U: tensor([-0.7963,  0.1172], requires_grad=True)\n",
      "V: tensor([ 0.8294, -0.1393], requires_grad=True)\n",
      "S: tensor([-1.9455, -0.1206], requires_grad=True)\n",
      "T: tensor([0.0673, 0.8415], requires_grad=True)\n",
      "\n",
      "超参数导数：\n",
      "U_grad: tensor([ 4.2148e-07, -9.8387e-07])\n",
      "V_grad: tensor([-1.5220e-07,  3.0716e-07])\n",
      "S_grad: tensor([4.3775e-03, 2.8695e-11])\n",
      "T_grad: tensor([ 2.1157e-02, -3.1344e-11])\n",
      "外层损失 (MSE): 0.8964\n",
      "\n",
      "优化后超参数（一步梯度下降）：\n",
      "U: tensor([-0.7963,  0.1172], requires_grad=True)\n",
      "V: tensor([ 0.8294, -0.1393], requires_grad=True)\n",
      "S: tensor([-1.9456, -0.1206], requires_grad=True)\n",
      "T: tensor([0.0671, 0.8415], requires_grad=True)\n",
      "\n",
      "重新计算的 beta: tensor([-1.6282e-08,  1.5359e-01, -2.4855e-08], grad_fn=<SqueezeBackward1>)\n",
      "原 beta_star: tensor([0.9384, 1.3597, 0.0404])\n",
      "约束满足情况 (beta >= 0): False\n",
      "\n",
      "预测值：tensor([ 0.0855, -0.0629,  0.0185,  0.2969,  0.0069,  0.0090, -0.2543, -0.1480,\n",
      "         0.2114, -0.0606,  0.4863,  0.0932,  0.0223, -0.3070, -0.0892, -0.0684,\n",
      "        -0.1343, -0.2777, -0.0868, -0.0420,  0.2291,  0.0437, -0.1231,  0.1612,\n",
      "        -0.0049, -0.0142,  0.0670, -0.0855, -0.1468,  0.2182,  0.3244, -0.1905,\n",
      "         0.1489, -0.3178,  0.0287,  0.0272, -0.1837,  0.1484,  0.0888,  0.1376,\n",
      "         0.3120, -0.0202,  0.0973, -0.0309, -0.1824,  0.2881,  0.0851,  0.1422,\n",
      "         0.2981,  0.0565, -0.1273,  0.0786, -0.1305,  0.2914,  0.1032,  0.1468,\n",
      "        -0.0490,  0.0843, -0.2920, -0.1314, -0.2289, -0.0628,  0.1662,  0.1052,\n",
      "        -0.0674,  0.0485,  0.1183, -0.2063,  0.2061,  0.0995, -0.0367, -0.0483,\n",
      "         0.1603,  0.0260, -0.0469, -0.0112, -0.0434,  0.1271,  0.1807,  0.2910,\n",
      "        -0.2359, -0.0925,  0.0809, -0.2534,  0.0876,  0.1394, -0.0895, -0.0158,\n",
      "        -0.2173,  0.0790, -0.1567,  0.0866,  0.0220,  0.1620, -0.0414,  0.0825,\n",
      "        -0.1824,  0.1577,  0.0100, -0.1725], grad_fn=<MvBackward0>)\n",
      "真实值：tensor([-6.2685e-01,  4.0025e-01, -1.0636e+00,  1.2954e+00,  3.5505e-01,\n",
      "         8.8127e-01,  3.3976e-01, -6.4206e-01, -7.7095e-01,  1.8668e+00,\n",
      "        -3.0415e-01, -1.1275e+00,  1.7460e-01, -1.1278e-01, -2.3421e-04,\n",
      "         1.2613e+00, -4.6698e-01, -5.0910e-01, -6.7152e-02, -3.8594e-01,\n",
      "         2.1675e-01, -2.3771e-01,  9.2724e-02, -1.2152e-01, -3.3318e-01,\n",
      "         1.3524e+00, -6.1204e-01, -1.4926e+00, -6.1036e-01,  7.0086e-01,\n",
      "        -9.8331e-01, -1.5240e+00,  1.9378e+00, -1.0024e-01, -6.3699e-02,\n",
      "         1.0985e+00, -5.4922e-01,  6.5147e-02, -4.5739e-01, -1.2076e+00,\n",
      "         1.7110e+00, -1.2336e-01,  1.5355e+00, -8.4323e-01,  4.1757e-01,\n",
      "        -9.1987e-01, -1.3002e+00,  7.3817e-02,  1.6454e+00,  4.0585e-01,\n",
      "        -6.9069e-01, -2.8518e-01,  8.5704e-01, -2.0520e-01,  1.9130e+00,\n",
      "         8.4141e-01, -3.2420e+00,  7.6115e-01,  1.4082e+00,  3.2925e-01,\n",
      "        -5.4758e-01, -1.5212e+00, -3.7597e-01, -5.3367e-01,  2.0446e-01,\n",
      "         9.9789e-01,  1.2309e+00,  1.7591e+00,  4.2338e-03, -1.2837e+00,\n",
      "         6.7920e-01, -2.5825e-01,  5.1429e-01, -1.2249e+00, -1.5078e-01,\n",
      "         5.7698e-01, -2.0366e-01,  6.0029e-01, -3.4101e-01,  3.2565e-01,\n",
      "         3.0229e-01, -2.3161e-01, -7.3172e-01, -8.4626e-02, -4.5726e-01,\n",
      "        -3.7469e-01, -5.5242e-01,  6.0755e-01,  1.5029e+00,  9.7680e-01,\n",
      "        -6.1373e-01,  7.0793e-01,  2.0836e+00,  4.9369e-01, -1.2307e+00,\n",
      "        -1.2250e+00, -7.0353e-01, -9.3296e-01,  6.1479e-02,  1.2527e+00])\n",
      "各项误差：tensor([-0.7124,  0.4632, -1.0821,  0.9984,  0.3481,  0.8722,  0.5940, -0.4941,\n",
      "        -0.9823,  1.9274, -0.7904, -1.2207,  0.1523,  0.1943,  0.0889,  1.3297,\n",
      "        -0.3327, -0.2314,  0.0196, -0.3440, -0.0124, -0.2814,  0.2159, -0.2827,\n",
      "        -0.3283,  1.3666, -0.6790, -1.4071, -0.4636,  0.4827, -1.3077, -1.3335,\n",
      "         1.7889,  0.2176, -0.0924,  1.0713, -0.3655, -0.0832, -0.5462, -1.3452,\n",
      "         1.3990, -0.1031,  1.4383, -0.8123,  0.6000, -1.2080, -1.3853, -0.0684,\n",
      "         1.3473,  0.3494, -0.5634, -0.3638,  0.9876, -0.4966,  1.8099,  0.6946,\n",
      "        -3.1930,  0.6769,  1.7002,  0.4607, -0.3187, -1.4584, -0.5421, -0.6389,\n",
      "         0.2719,  0.9494,  1.1126,  1.9655, -0.2019, -1.3832,  0.7159, -0.2099,\n",
      "         0.3540, -1.2509, -0.1038,  0.5881, -0.1602,  0.4732, -0.5217,  0.0346,\n",
      "         0.5382, -0.1391, -0.8126,  0.1688, -0.5448, -0.5141, -0.4629,  0.6233,\n",
      "         1.7202,  0.8978, -0.4570,  0.6214,  2.0616,  0.3317, -1.1893, -1.3075,\n",
      "        -0.5211, -1.0907,  0.0515,  1.4252], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "# 参数定义\n",
    "n, d = 100, 3  # 数据点数和模型参数维度\n",
    "L, H = 2, 2   # ReLU 和 ReHU 项数\n",
    "lambda_reg = 0.1  # Ridge 正则化参数\n",
    "\n",
    "# 生成示例数据\n",
    "X = torch.randn(n, d)  # 输入数据，形状 (n, d)\n",
    "y = torch.randn(n)     # 目标值，形状 (n,)\n",
    "\n",
    "# 固定 beta（假设已通过内层优化计算）\n",
    "beta_star = torch.abs(torch.randn(d))  # 确保 beta >= 0\n",
    "\n",
    "# 初始化超参数（u_l, v_l, s_h, t_h）\n",
    "U = torch.randn(L, requires_grad=True)  # u_l，形状 (L,)\n",
    "V = torch.randn(L, requires_grad=True)  # v_l，形状 (L,)\n",
    "S = torch.randn(H, requires_grad=True)  # s_h，形状 (H,)\n",
    "T = torch.randn(H, requires_grad=True)  # t_h，形状 (H,)\n",
    "tau = torch.ones(H, requires_grad=False)  # T_h 固定为 1，形状 (H,)\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg):\n",
    "    \"\"\"\n",
    "    重建内层 QP，用于计算 beta 对超参数的隐式导数\n",
    "    \"\"\"\n",
    "    # QP 参数\n",
    "    total_vars = d + L * n + H * n + H * n  # [beta, pi, theta, sigma]\n",
    "    Q = torch.zeros(total_vars)\n",
    "    Q[:d] = lambda_reg  # beta 的 Ridge 正则化\n",
    "    Q[d + L * n:d + L * n + H * n] = 1.0  # theta 的二次项\n",
    "    Q = torch.diag(Q).unsqueeze(0)\n",
    "    \n",
    "    p = torch.zeros(total_vars)\n",
    "    tau_expanded = tau.repeat(n)  # 形状 (H * n,)\n",
    "    p[d + L * n + H * n:] = tau_expanded  # sigma 的线性项\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 约束 G z <= h\n",
    "    G_rows = 2 * L * n + 2 * H * n + d  # 每样本 2L + 2H + d 个约束\n",
    "    G = torch.zeros(G_rows, total_vars)\n",
    "    h_values = torch.zeros(G_rows)  # 临时存储 h 的值\n",
    "    \n",
    "    # pi_li >= u_l (y_i - x_i^T beta) + v_l\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[idx, :d] = U[l] * X[i]  # u_l x_i^T beta，保持计算图\n",
    "            G[idx, d + l * n + i] = -1.0  # -pi_li\n",
    "            h_values[idx] = U[l] * y[i] + V[l]  # u_l y_i + v_l，保持计算图\n",
    "            idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[idx, d + l * n + i] = -1.0  # -pi_li <= 0\n",
    "            h_values[idx] = 0.0\n",
    "            idx += 1\n",
    "    \n",
    "    # theta_hi + sigma_hi >= s_h (y_i - x_i^T beta) + t_h\n",
    "    for i in range(n):\n",
    "        for h in range(H):\n",
    "            G[idx, :d] = S[h] * X[i]  # s_h x_i^T beta，保持计算图\n",
    "            G[idx, d + L * n + h * n + i] = -1.0  # -theta_hi\n",
    "            G[idx, d + L * n + H * n + h * n + i] = -1.0  # -sigma_hi\n",
    "            h_values[idx] = S[h] * y[i] + T[h]  # s_h y_i + t_h，保持计算图\n",
    "            idx += 1\n",
    "    \n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h in range(H):\n",
    "            G[idx, d + L * n + H * n + h * n + i] = -1.0  # -sigma_hi <= 0\n",
    "            h_values[idx] = 0.0\n",
    "            idx += 1\n",
    "    \n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[idx, j] = -1.0  # -beta_j <= 0\n",
    "        h_values[idx] = 0.0\n",
    "        idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_values.unsqueeze(0)  # 转换为 (1, G_rows) 形状\n",
    "    \n",
    "    # 确保 Q 是 SPD\n",
    "    Q += 1e-4 * torch.eye(total_vars).unsqueeze(0)\n",
    "    \n",
    "    # 使用 qpth 求解\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.Tensor(), torch.Tensor())\n",
    "    beta = z[:, :d].squeeze(0)  # 提取 beta\n",
    "    return beta\n",
    "\n",
    "def compute_outer_gradients(beta_star, X, y, U, V, S, T, tau, lambda_reg):\n",
    "    \"\"\"\n",
    "    固定 beta_star，计算外层损失对超参数的导数\n",
    "    返回：U, V, S, T 的梯度和外层损失值\n",
    "    \"\"\"\n",
    "    # 重建内层 QP，确保 beta 对超参数的依赖\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    \n",
    "    # 外层损失（MSE）\n",
    "    y_pred = X @ beta_opt\n",
    "    L_outer = (1/n) * (y - y_pred).pow(2).sum()\n",
    "    \n",
    "    # 计算外层损失的梯度（隐式微分）\n",
    "    L_outer.backward()\n",
    "    \n",
    "    # 提取超参数梯度\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零梯度（添加保护）\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return U_grad, V_grad, S_grad, T_grad, L_outer.item()\n",
    "\n",
    "# 简单实验：初始化超参数，计算导数，进行一步梯度下降\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始超参数\n",
    "    print(\"初始超参数：\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 计算外层损失对超参数的导数\n",
    "    U_grad, V_grad, S_grad, T_grad, outer_loss = compute_outer_gradients(\n",
    "        beta_star, X, y, U, V, S, T, tau, lambda_reg\n",
    "    )\n",
    "    \n",
    "    # 打印导数\n",
    "    print(\"\\n超参数导数：\")\n",
    "    print(f\"U_grad: {U_grad}\")\n",
    "    print(f\"V_grad: {V_grad}\")\n",
    "    print(f\"S_grad: {S_grad}\")\n",
    "    print(f\"T_grad: {T_grad}\")\n",
    "    print(f\"外层损失 (MSE): {outer_loss:.4f}\")\n",
    "    \n",
    "    # 进行一步梯度下降\n",
    "    learning_rate = 0.01\n",
    "    with torch.no_grad():\n",
    "        U_new = U - learning_rate * U_grad\n",
    "        V_new = V - learning_rate * V_grad\n",
    "        S_new = S - learning_rate * S_grad\n",
    "        T_new = T - learning_rate * T_grad\n",
    "    \n",
    "    # 更新超参数\n",
    "    U = U_new.clone().requires_grad_(True)\n",
    "    V = V_new.clone().requires_grad_(True)\n",
    "    S = S_new.clone().requires_grad_(True)\n",
    "    T = T_new.clone().requires_grad_(True)\n",
    "    \n",
    "    # 打印优化后的超参数\n",
    "    print(\"\\n优化后超参数（一步梯度下降）：\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 验证 beta 是否保持不变（重新计算 QP 解）\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    print(f\"\\n重新计算的 beta: {beta_opt}\")\n",
    "    print(f\"原 beta_star: {beta_star}\")\n",
    "    print(f\"约束满足情况 (beta >= 0): {torch.all(beta_opt >= 0)}\")\n",
    "\n",
    "    # 观察预测的结果和真实值\n",
    "    y_pred = X @ beta_opt\n",
    "    print(f\"\\n预测值：{y_pred}\")\n",
    "    print(f\"真实值：{y}\")\n",
    "    print(f\"各项误差：{(y - y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始超参数:\n",
      "U: tensor([-0.9723, -0.4389], requires_grad=True)\n",
      "V: tensor([ 0.6166, -0.1783], requires_grad=True)\n",
      "S: tensor([ 0.9274, -0.7048], requires_grad=True)\n",
      "T: tensor([1.9331, 1.2874], requires_grad=True)\n",
      "\n",
      "超参数导数:\n",
      "U_grad: tensor([2.3499e-06, 1.4791e-06])\n",
      "V_grad: tensor([ 1.0751e-06, -3.8228e-08])\n",
      "S_grad: tensor([0.0103, 0.0023])\n",
      "T_grad: tensor([-0.0042,  0.0002])\n",
      "外层损失(MSE): 1.1211\n",
      "\n",
      "优化后超参数（一步梯度下降）:\n",
      "U: tensor([-0.9723, -0.4389], requires_grad=True)\n",
      "V: tensor([ 0.6166, -0.1783], requires_grad=True)\n",
      "S: tensor([ 0.9273, -0.7048], requires_grad=True)\n",
      "T: tensor([1.9331, 1.2874], requires_grad=True)\n",
      "\n",
      "重新计算的 beta: tensor([3.2026e-07, 5.9602e-07, 3.7011e-02], grad_fn=<SqueezeBackward1>)\n",
      "原 beta_star: tensor([1.3249, 1.0254, 1.0872])\n",
      "约束满足情况 (beta >= 0): True\n",
      "\n",
      "预测值: tensor([-0.0157,  0.0306, -0.0022, -0.0161, -0.0750,  0.0298,  0.0312,  0.0044,\n",
      "         0.0346, -0.0289, -0.0788,  0.0546,  0.0109,  0.0192,  0.0818, -0.0532,\n",
      "        -0.0780,  0.0191, -0.0247,  0.0530,  0.0353,  0.0644, -0.0176, -0.0667,\n",
      "        -0.0401,  0.0087, -0.0676, -0.0316,  0.0190, -0.0106, -0.0103, -0.0391,\n",
      "        -0.0215, -0.0154,  0.0061,  0.0190,  0.0327, -0.0231, -0.0117, -0.0169,\n",
      "        -0.0344,  0.0271,  0.0046, -0.0252,  0.0094,  0.0244,  0.0461, -0.0399,\n",
      "        -0.0089, -0.0405, -0.0254, -0.0342, -0.0780,  0.0195, -0.0503,  0.0067,\n",
      "         0.0134,  0.0839, -0.0158,  0.0047,  0.0344, -0.0275,  0.0798, -0.0342,\n",
      "        -0.0033, -0.0196,  0.1011,  0.0379, -0.0375, -0.0107,  0.0238, -0.0056,\n",
      "         0.0023,  0.0304, -0.0032,  0.0113, -0.0231,  0.0533,  0.0736,  0.0523,\n",
      "         0.0161,  0.0837,  0.0285, -0.0180, -0.0158,  0.0199,  0.0318, -0.0379,\n",
      "         0.0048, -0.0434, -0.0921, -0.0324, -0.0391,  0.0009, -0.0522, -0.0303,\n",
      "         0.0352,  0.0113,  0.0128, -0.0538], grad_fn=<MvBackward0>)\n",
      "真实值: tensor([-1.0731,  0.4304, -1.2194, -0.7323, -0.5314, -0.0891,  1.2584,  0.4339,\n",
      "         1.5002, -0.7302, -1.6290, -0.0493,  0.1347,  0.4368,  0.0583,  0.8801,\n",
      "        -2.3512,  0.0132,  0.7325, -1.2154,  1.2643, -0.8617, -0.9163,  1.7147,\n",
      "         1.3003,  1.7833, -0.9229, -0.2583,  0.6249,  0.7627,  0.3925,  0.0424,\n",
      "         0.3998,  0.6187,  2.0206,  0.6623,  0.6540,  0.1810, -1.4418,  1.4822,\n",
      "        -0.6688,  0.5551, -0.0148,  1.0028,  1.3044, -0.8242,  0.2747, -1.8131,\n",
      "         0.8635,  0.5921,  1.4300, -0.5317,  0.3665, -0.8765, -0.0833, -0.5796,\n",
      "         0.2953, -0.6340,  1.4656,  0.4556, -1.6851,  0.2351, -0.0872, -1.1090,\n",
      "         0.3913,  0.9484, -0.2911,  0.8533,  0.9210, -1.9462, -0.3360,  0.2705,\n",
      "         2.2035,  0.2023,  1.7408,  0.1745, -0.1595, -0.0843, -1.1073, -0.2571,\n",
      "         1.5664,  1.0485,  0.1146, -0.9393,  1.4331,  2.7058,  1.7913,  1.4843,\n",
      "        -0.2773, -0.4401,  1.0914,  3.1600, -0.3162,  0.6654,  0.6291, -0.9051,\n",
      "         1.1847, -0.4966,  0.8460, -0.2807])\n",
      "各项误差: tensor([-1.0574,  0.3998, -1.2172, -0.7162, -0.4564, -0.1189,  1.2272,  0.4295,\n",
      "         1.4656, -0.7013, -1.5502, -0.1039,  0.1238,  0.4176, -0.0234,  0.9333,\n",
      "        -2.2732, -0.0059,  0.7573, -1.2684,  1.2290, -0.9261, -0.8987,  1.7814,\n",
      "         1.3404,  1.7746, -0.8553, -0.2268,  0.6059,  0.7733,  0.4028,  0.0815,\n",
      "         0.4212,  0.6341,  2.0145,  0.6432,  0.6213,  0.2041, -1.4301,  1.4991,\n",
      "        -0.6344,  0.5280, -0.0194,  1.0280,  1.2950, -0.8486,  0.2286, -1.7732,\n",
      "         0.8725,  0.6326,  1.4555, -0.4975,  0.4445, -0.8960, -0.0331, -0.5863,\n",
      "         0.2819, -0.7178,  1.4814,  0.4509, -1.7195,  0.2626, -0.1670, -1.0748,\n",
      "         0.3946,  0.9680, -0.3921,  0.8153,  0.9585, -1.9354, -0.3598,  0.2761,\n",
      "         2.2012,  0.1719,  1.7439,  0.1632, -0.1364, -0.1376, -1.1809, -0.3095,\n",
      "         1.5504,  0.9649,  0.0860, -0.9212,  1.4489,  2.6859,  1.7596,  1.5223,\n",
      "        -0.2822, -0.3967,  1.1835,  3.1924, -0.2771,  0.6645,  0.6813, -0.8748,\n",
      "         1.1495, -0.5079,  0.8332, -0.2269], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "def build_qp_matrices(U, V, S, T, tau, X, y, lambda_reg):\n",
    "    r\"\"\"\n",
    "    构建 Q, p, G, h 矩阵：\n",
    "    - 内层问题变量: z = [beta, pi, theta, sigma]\n",
    "    - 目标函数:  (1/2) * z^T Q z + p^T z\n",
    "    - 约束: G z <= h\n",
    "    \n",
    "    其中:\n",
    "      - beta 维度为 d\n",
    "      - pi 维度为 L * n\n",
    "      - theta 维度为 H * n\n",
    "      - sigma 维度为 H * n\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    L = U.shape[0]\n",
    "    H = S.shape[0]\n",
    "    \n",
    "    # total_vars = d + (L*n) + (H*n) + (H*n)\n",
    "    total_vars = d + L * n + 2 * H * n\n",
    "    \n",
    "    #-------------------------------------\n",
    "    # 1) 构建 Q (二次项)\n",
    "    #   Q 对角线各部分对应: [lambda_reg * I_d, 0, I_{H*n}, I_{H*n}],\n",
    "    #   pi_li 不含二次项，theta_hi 含 1.0, sigma_hi 含 1.0\n",
    "    #-------------------------------------\n",
    "    Q_diag = torch.zeros(total_vars, dtype=X.dtype, device=X.device)\n",
    "    # beta的正则化\n",
    "    Q_diag[:d] = lambda_reg\n",
    "    # theta 位置\n",
    "    Q_diag[d + L * n : d + L * n + H * n] = 1.0\n",
    "    # sigma 位置\n",
    "    Q_diag[d + L * n + H * n : d + L * n + 2 * H * n] = 1.0\n",
    "    \n",
    "    Q = torch.diag(Q_diag).unsqueeze(0)\n",
    "    \n",
    "    #-------------------------------------\n",
    "    # 2) 构建 p (线性项)\n",
    "    #   只在 sigma 上有 tau * sigma\n",
    "    #-------------------------------------\n",
    "    p = torch.zeros(total_vars, dtype=X.dtype, device=X.device)\n",
    "    p[d + L * n + H * n : ] = tau.repeat(n)\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    #-------------------------------------\n",
    "    # 3) 构建 G, h (不等式约束)\n",
    "    #   约束总数 = n*(2L + 2H) + d\n",
    "    #    -- pi_li >= U_l(y_i - x_i^T beta) + V_l\n",
    "    #    -- pi_li >= 0\n",
    "    #    -- theta_hi + sigma_hi >= S_h(y_i - x_i^T beta) + T_h\n",
    "    #    -- sigma_hi >= 0\n",
    "    #    -- beta_j >= 0\n",
    "    #-------------------------------------\n",
    "    G_rows = 2 * L * n + 2 * H * n + d\n",
    "    G = torch.zeros(G_rows, total_vars, dtype=X.dtype, device=X.device)\n",
    "    h_values = torch.zeros(G_rows, dtype=X.dtype, device=X.device)\n",
    "    \n",
    "    row_idx = 0\n",
    "    \n",
    "    # (a) pi_li >= U_l (y_i - x_i^T beta) + V_l\n",
    "    #     => -pi_li + U_l x_i^T beta <= U_l y_i + V_l\n",
    "    # 故 G[row_idx, :d] = U[l] * x_i\n",
    "    #    G[row_idx, d + l*n + i] = -1\n",
    "    #    h_values[row_idx] = U[l] * y[i] + V[l]\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, :d] = U[l] * X[i]\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_values[row_idx] = U[l] * y[i] + V[l]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # (b) pi_li >= 0 => -pi_li <= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[row_idx, d + l*n + i] = -1.0\n",
    "            h_values[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "    \n",
    "    # (c) theta_hi + sigma_hi >= S_h (y_i - x_i^T beta) + T_h\n",
    "    #     => -theta_hi - sigma_hi + S_h x_i^T beta <= S_h y_i + T_h\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            G[row_idx, :d] = S[h_] * X[i]\n",
    "            G[row_idx, d + L*n + h_*n + i] = -1.0   # theta\n",
    "            G[row_idx, d + L*n + H*n + h_*n + i] = -1.0  # sigma\n",
    "            h_values[row_idx] = S[h_] * y[i] + T[h_]\n",
    "            row_idx += 1\n",
    "    \n",
    "    # (d) sigma_hi >= 0 => -sigma_hi <= 0\n",
    "    for i in range(n):\n",
    "        for h_ in range(H):\n",
    "            idx_sigma = d + L*n + H*n + h_*n + i\n",
    "            G[row_idx, idx_sigma] = -1.0\n",
    "            h_values[row_idx] = 0.0\n",
    "            row_idx += 1\n",
    "    \n",
    "    # (e) beta_j >= 0 => -beta_j <= 0\n",
    "    for j in range(d):\n",
    "        G[row_idx, j] = -1.0\n",
    "        h_values[row_idx] = 0.0\n",
    "        row_idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_values.unsqueeze(0)\n",
    "    \n",
    "    #-------------------------------------\n",
    "    # 为 Q 增加扰动，避免数值不稳定\n",
    "    #-------------------------------------\n",
    "    eps = 1e-4\n",
    "    Q = Q + eps * torch.eye(total_vars, dtype=X.dtype, device=X.device).unsqueeze(0)\n",
    "    \n",
    "    return Q, p, G, h\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg):\n",
    "    r\"\"\"\n",
    "    给定超参数 (U, V, S, T, tau) 和数据 (X, y)，\n",
    "    构建并求解内层 QP。\n",
    "    返回: 内层最优解 beta (大小为 d)。\n",
    "    \"\"\"\n",
    "    # 构建 Q, p, G, h\n",
    "    Q, p, G, h = build_qp_matrices(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    \n",
    "    # 调用 QPFunction 做求解\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.empty(0), torch.empty(0))\n",
    "    \n",
    "    # 提取前 d 维为 beta\n",
    "    n, d = X.shape\n",
    "    beta_opt = z[:, :d].squeeze(0)\n",
    "    return beta_opt\n",
    "\n",
    "def compute_outer_gradients(\n",
    "    X, y,\n",
    "    U, V, S, T, tau,\n",
    "    lambda_reg\n",
    "):\n",
    "    r\"\"\"\n",
    "    计算外层目标 (此处为 MSE) 对超参数的梯度。\n",
    "    做法：\n",
    "    1) 基于给定超参数解内层 QP，得到 beta_opt\n",
    "    2) 用 beta_opt 计算外层损失 MSE\n",
    "    3) backward() 得到对 U, V, S, T 的梯度\n",
    "    \"\"\"\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    n = X.shape[0]\n",
    "    \n",
    "    # 外层损失: MSE\n",
    "    y_pred = X @ beta_opt\n",
    "    loss_outer = (1.0 / n) * (y - y_pred).pow(2).sum()\n",
    "    \n",
    "    # 反向传播\n",
    "    loss_outer.backward()\n",
    "    \n",
    "    # 收集梯度\n",
    "    U_grad = U.grad.detach().clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.detach().clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.detach().clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.detach().clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 梯度清零\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return {\n",
    "        \"U_grad\": U_grad,\n",
    "        \"V_grad\": V_grad,\n",
    "        \"S_grad\": S_grad,\n",
    "        \"T_grad\": T_grad,\n",
    "        \"loss\": loss_outer.item(),\n",
    "        \"beta_opt\": beta_opt.detach().clone()\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # 数据与超参数初始化\n",
    "    n, d = 100, 3\n",
    "    L, H = 2, 2\n",
    "    lambda_reg = 0.1\n",
    "    \n",
    "    X = torch.randn(n, d)\n",
    "    y = torch.randn(n)\n",
    "    \n",
    "    # 假设内层 beta_star 已知: (此处不太用到, 但留着也可)\n",
    "    beta_star = torch.abs(torch.randn(d))\n",
    "    \n",
    "    U = torch.randn(L, requires_grad=True)\n",
    "    V = torch.randn(L, requires_grad=True)\n",
    "    S = torch.randn(H, requires_grad=True)\n",
    "    T = torch.randn(H, requires_grad=True)\n",
    "    tau = torch.ones(H, requires_grad=False)  # 通常做固定\n",
    "    \n",
    "    print(\"初始超参数:\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 计算外层损失对超参数的导数\n",
    "    results = compute_outer_gradients(X, y, U, V, S, T, tau, lambda_reg)\n",
    "    U_grad = results[\"U_grad\"]\n",
    "    V_grad = results[\"V_grad\"]\n",
    "    S_grad = results[\"S_grad\"]\n",
    "    T_grad = results[\"T_grad\"]\n",
    "    outer_loss = results[\"loss\"]\n",
    "    \n",
    "    print(\"\\n超参数导数:\")\n",
    "    print(f\"U_grad: {U_grad}\")\n",
    "    print(f\"V_grad: {V_grad}\")\n",
    "    print(f\"S_grad: {S_grad}\")\n",
    "    print(f\"T_grad: {T_grad}\")\n",
    "    print(f\"外层损失(MSE): {outer_loss:.4f}\")\n",
    "    \n",
    "    # 做一步梯度下降\n",
    "    lr = 0.01\n",
    "    with torch.no_grad():\n",
    "        U -= lr * U_grad\n",
    "        V -= lr * V_grad\n",
    "        S -= lr * S_grad\n",
    "        T -= lr * T_grad\n",
    "    \n",
    "    # 更新需要 requires_grad\n",
    "    U.requires_grad_(True)\n",
    "    V.requires_grad_(True)\n",
    "    S.requires_grad_(True)\n",
    "    T.requires_grad_(True)\n",
    "    \n",
    "    print(\"\\n优化后超参数（一步梯度下降）:\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 重新验证内层解\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    print(f\"\\n重新计算的 beta: {beta_opt}\")\n",
    "    print(f\"原 beta_star: {beta_star}\")\n",
    "    print(f\"约束满足情况 (beta >= 0): {torch.all(beta_opt >= 0)}\")\n",
    "    \n",
    "    # 观察预测结果\n",
    "    y_pred = X @ beta_opt\n",
    "    print(f\"\\n预测值: {y_pred}\")\n",
    "    print(f\"真实值: {y}\")\n",
    "    print(f\"各项误差: {y - y_pred}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "win_DL_Q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

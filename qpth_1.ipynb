{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始超参数：\n",
      "U: tensor([-2.6461,  1.8845], requires_grad=True)\n",
      "V: tensor([-0.5992, -0.2085], requires_grad=True)\n",
      "S: tensor([-1.1804, -1.2868], requires_grad=True)\n",
      "T: tensor([-0.4024, -0.3870], requires_grad=True)\n",
      "\n",
      "超参数导数：\n",
      "U_grad: tensor([ 5.4043e-05, -7.2844e-05])\n",
      "V_grad: tensor([-1.0317e-05,  9.5653e-06])\n",
      "S_grad: tensor([-0.0459, -0.0548])\n",
      "T_grad: tensor([0.0507, 0.0550])\n",
      "外层损失 (MSE): 1.3661\n",
      "\n",
      "优化后超参数（一步梯度下降）：\n",
      "U: tensor([-2.6461,  1.8845], requires_grad=True)\n",
      "V: tensor([-0.5992, -0.2085], requires_grad=True)\n",
      "S: tensor([-1.1799, -1.2862], requires_grad=True)\n",
      "T: tensor([-0.4029, -0.3876], requires_grad=True)\n",
      "\n",
      "重新计算的 beta: tensor([5.1909e-09, 9.3970e-09, 4.4165e-01], grad_fn=<SqueezeBackward1>)\n",
      "原 beta_star: tensor([1.1445, 0.4805, 1.3974])\n",
      "约束满足情况 (beta >= 0): True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from qpth.qp import QPFunction\n",
    "\n",
    "# 参数定义\n",
    "n, d = 10, 3  # 数据点数和模型参数维度\n",
    "L, H = 2, 2   # ReLU 和 ReHU 项数\n",
    "lambda_reg = 0.1  # Ridge 正则化参数\n",
    "\n",
    "# 生成示例数据\n",
    "X = torch.randn(n, d)  # 输入数据，形状 (n, d)\n",
    "y = torch.randn(n)     # 目标值，形状 (n,)\n",
    "\n",
    "# 固定 beta（假设已通过内层优化计算）\n",
    "beta_star = torch.abs(torch.randn(d))  # 确保 beta >= 0\n",
    "\n",
    "# 初始化超参数（u_l, v_l, s_h, t_h）\n",
    "U = torch.randn(L, requires_grad=True)  # u_l，形状 (L,)\n",
    "V = torch.randn(L, requires_grad=True)  # v_l，形状 (L,)\n",
    "S = torch.randn(H, requires_grad=True)  # s_h，形状 (H,)\n",
    "T = torch.randn(H, requires_grad=True)  # t_h，形状 (H,)\n",
    "tau = torch.ones(H, requires_grad=False)  # T_h 固定为 1，形状 (H,)\n",
    "\n",
    "def solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg):\n",
    "    \"\"\"\n",
    "    重建内层 QP，用于计算 beta 对超参数的隐式导数\n",
    "    \"\"\"\n",
    "    # QP 参数\n",
    "    total_vars = d + L * n + H * n + H * n  # [beta, pi, theta, sigma]\n",
    "    Q = torch.zeros(total_vars)\n",
    "    Q[:d] = lambda_reg  # beta 的 Ridge 正则化\n",
    "    Q[d + L * n:d + L * n + H * n] = 1.0  # theta 的二次项\n",
    "    Q = torch.diag(Q).unsqueeze(0)\n",
    "    \n",
    "    p = torch.zeros(total_vars)\n",
    "    tau_expanded = tau.repeat(n)  # 形状 (H * n,)\n",
    "    p[d + L * n + H * n:] = tau_expanded  # sigma 的线性项\n",
    "    p = p.unsqueeze(0)\n",
    "    \n",
    "    # 约束 G z <= h\n",
    "    G_rows = 2 * L * n + 2 * H * n + d  # 每样本 2L + 2H + d 个约束\n",
    "    G = torch.zeros(G_rows, total_vars)\n",
    "    h_values = torch.zeros(G_rows)  # 临时存储 h 的值\n",
    "    \n",
    "    # pi_li >= u_l (y_i - x_i^T beta) + v_l\n",
    "    idx = 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[idx, :d] = U[l] * X[i]  # u_l x_i^T beta，保持计算图\n",
    "            G[idx, d + l * n + i] = -1.0  # -pi_li\n",
    "            h_values[idx] = U[l] * y[i] + V[l]  # u_l y_i + v_l，保持计算图\n",
    "            idx += 1\n",
    "    \n",
    "    # pi_li >= 0\n",
    "    for i in range(n):\n",
    "        for l in range(L):\n",
    "            G[idx, d + l * n + i] = -1.0  # -pi_li <= 0\n",
    "            h_values[idx] = 0.0\n",
    "            idx += 1\n",
    "    \n",
    "    # theta_hi + sigma_hi >= s_h (y_i - x_i^T beta) + t_h\n",
    "    for i in range(n):\n",
    "        for h in range(H):\n",
    "            G[idx, :d] = S[h] * X[i]  # s_h x_i^T beta，保持计算图\n",
    "            G[idx, d + L * n + h * n + i] = -1.0  # -theta_hi\n",
    "            G[idx, d + L * n + H * n + h * n + i] = -1.0  # -sigma_hi\n",
    "            h_values[idx] = S[h] * y[i] + T[h]  # s_h y_i + t_h，保持计算图\n",
    "            idx += 1\n",
    "    \n",
    "    # sigma_hi >= 0\n",
    "    for i in range(n):\n",
    "        for h in range(H):\n",
    "            G[idx, d + L * n + H * n + h * n + i] = -1.0  # -sigma_hi <= 0\n",
    "            h_values[idx] = 0.0\n",
    "            idx += 1\n",
    "    \n",
    "    # beta_j >= 0\n",
    "    for j in range(d):\n",
    "        G[idx, j] = -1.0  # -beta_j <= 0\n",
    "        h_values[idx] = 0.0\n",
    "        idx += 1\n",
    "    \n",
    "    G = G.unsqueeze(0)\n",
    "    h = h_values.unsqueeze(0)  # 转换为 (1, G_rows) 形状\n",
    "    \n",
    "    # 确保 Q 是 SPD\n",
    "    Q += 1e-4 * torch.eye(total_vars).unsqueeze(0)\n",
    "    \n",
    "    # 使用 qpth 求解\n",
    "    z = QPFunction(verbose=False)(Q, p, G, h, torch.Tensor(), torch.Tensor())\n",
    "    beta = z[:, :d].squeeze(0)  # 提取 beta\n",
    "    return beta\n",
    "\n",
    "def compute_outer_gradients(beta_star, X, y, U, V, S, T, tau, lambda_reg):\n",
    "    \"\"\"\n",
    "    固定 beta_star，计算外层损失对超参数的导数\n",
    "    返回：U, V, S, T 的梯度和外层损失值\n",
    "    \"\"\"\n",
    "    # 重建内层 QP，确保 beta 对超参数的依赖\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    \n",
    "    # 外层损失（MSE）\n",
    "    y_pred = X @ beta_opt\n",
    "    L_outer = (1/n) * (y - y_pred).pow(2).sum()\n",
    "    \n",
    "    # 计算外层损失的梯度（隐式微分）\n",
    "    L_outer.backward()\n",
    "    \n",
    "    # 提取超参数梯度\n",
    "    U_grad = U.grad.clone() if U.grad is not None else torch.zeros_like(U)\n",
    "    V_grad = V.grad.clone() if V.grad is not None else torch.zeros_like(V)\n",
    "    S_grad = S.grad.clone() if S.grad is not None else torch.zeros_like(S)\n",
    "    T_grad = T.grad.clone() if T.grad is not None else torch.zeros_like(T)\n",
    "    \n",
    "    # 清零梯度（添加保护）\n",
    "    if U.grad is not None:\n",
    "        U.grad.zero_()\n",
    "    if V.grad is not None:\n",
    "        V.grad.zero_()\n",
    "    if S.grad is not None:\n",
    "        S.grad.zero_()\n",
    "    if T.grad is not None:\n",
    "        T.grad.zero_()\n",
    "    \n",
    "    return U_grad, V_grad, S_grad, T_grad, L_outer.item()\n",
    "\n",
    "# 简单实验：初始化超参数，计算导数，进行一步梯度下降\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始超参数\n",
    "    print(\"初始超参数：\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 计算外层损失对超参数的导数\n",
    "    U_grad, V_grad, S_grad, T_grad, outer_loss = compute_outer_gradients(\n",
    "        beta_star, X, y, U, V, S, T, tau, lambda_reg\n",
    "    )\n",
    "    \n",
    "    # 打印导数\n",
    "    print(\"\\n超参数导数：\")\n",
    "    print(f\"U_grad: {U_grad}\")\n",
    "    print(f\"V_grad: {V_grad}\")\n",
    "    print(f\"S_grad: {S_grad}\")\n",
    "    print(f\"T_grad: {T_grad}\")\n",
    "    print(f\"外层损失 (MSE): {outer_loss:.4f}\")\n",
    "    \n",
    "    # 进行一步梯度下降\n",
    "    learning_rate = 0.01\n",
    "    with torch.no_grad():\n",
    "        U_new = U - learning_rate * U_grad\n",
    "        V_new = V - learning_rate * V_grad\n",
    "        S_new = S - learning_rate * S_grad\n",
    "        T_new = T - learning_rate * T_grad\n",
    "    \n",
    "    # 更新超参数\n",
    "    U = U_new.clone().requires_grad_(True)\n",
    "    V = V_new.clone().requires_grad_(True)\n",
    "    S = S_new.clone().requires_grad_(True)\n",
    "    T = T_new.clone().requires_grad_(True)\n",
    "    \n",
    "    # 打印优化后的超参数\n",
    "    print(\"\\n优化后超参数（一步梯度下降）：\")\n",
    "    print(f\"U: {U}\")\n",
    "    print(f\"V: {V}\")\n",
    "    print(f\"S: {S}\")\n",
    "    print(f\"T: {T}\")\n",
    "    \n",
    "    # 验证 beta 是否保持不变（重新计算 QP 解）\n",
    "    beta_opt = solve_inner_qpth(U, V, S, T, tau, X, y, lambda_reg)\n",
    "    print(f\"\\n重新计算的 beta: {beta_opt}\")\n",
    "    print(f\"原 beta_star: {beta_star}\")\n",
    "    print(f\"约束满足情况 (beta >= 0): {torch.all(beta_opt >= 0)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "optnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
